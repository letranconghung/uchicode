{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba83e56c",
      "metadata": {
        "id": "ba83e56c"
      },
      "source": [
        "# Homework 8\n",
        "In this homework, you will explore neural networks, writing code to complete forward and backward passes for a fully-connected network. Having done this, you will train these networks with the goal of classifying images of apparel into the type (from the Fashion MNIST dataset). You will finally upload your predictions on the test set we give you to [this kaggle competition](https://www.kaggle.com/t/e7ec8dd1f8e64aa2997d82edd5e8f6eb) (more details later in the homework).\n",
        "\n",
        "In the (optional) extension section at the end, you will see how to use [PyTorch](https://pytorch.org/) to implement, train, and evaluate a convolutional neural network.\n",
        "\n",
        "There are a number of programming **tasks** and **quiz questions** in this homework.\n",
        "- For **tasks**, you will need to either **add code between comments \"`#### TASK N CODE`\"** to complete them or **modify code between those comments**. **DO NOT delete the comments \"#### TASK N CODE\". This is for graders' reference and you might not get full points if you tamper with these comments.**\n",
        "- For **quiz questions**, you will need to answer in a few sentences between the given lines.\n",
        "- For **optional tasks**, you are **NOT required to turn them in**. However, we encourage you to complete them as they are good practice.\n",
        "- For **challenge-optional tasks**, you are **NOT required to turn them in**. However, you will receive extra credit for completing the challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32d1c89",
      "metadata": {
        "id": "c32d1c89"
      },
      "source": [
        "-----\n",
        "\n",
        "We will first import libraries.\n",
        "- We provide you with a few relevant functions in the `utils` module. The `linclass` module has a bunch of linear classifiers that we also provided you in the previous homework.\n",
        "- In this assignment, we will program neural networks with different network operations represented as classes. Thus, we will need a common \"abstract\" class and several abstract methods; `abc` module provides these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2471ad09-513f-4c81-adb2-edfa7e722178",
      "metadata": {
        "id": "2471ad09-513f-4c81-adb2-edfa7e722178"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import linclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5a59c5d9-a81b-4db8-bb2b-141529fe3eb2",
      "metadata": {
        "id": "5a59c5d9-a81b-4db8-bb2b-141529fe3eb2"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9ac85b",
      "metadata": {
        "id": "4e9ac85b"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "As discussed in lecture, neural networks are Directed Acyclic Graphs (DAGs) with input and output nodes, where each node is associated with an \"activation\" function, and where the edges have weights. Neural networks can be represented as DAGs in code using a \"computational graph\". In fact, many libraries such as PyTorch and Tensorflow choose to do so.\n",
        "\n",
        "We will follow a code structure similar to PyTorch, though we will limit the scope of this homework to a path in a DAG. A path in a DAG corresponds to a Sequential neural network, called as such because it comprises of a sequence of layers (collection of units of nodes) where the output of one layer is the input of the next layer in the sequence.\n",
        "\n",
        "We start by defining a `Module` that defines a node or a collection of nodes. A `Module` is an abstract view of the fundamental element of a neural network. It defines several attributes and methods, some of which must be implemented by the derived classes.\n",
        "- `train` is a flag to denote whether the module is in training or evaluation mode. We will see how this is important later. This flag is controlled by `train_mode()` and `eval_mode()` methods.\n",
        "- `forward()` and `backward()` methods do forward and backward passes of the module (recall the discussion from recitation).\n",
        "    - Inside `forward()`, attributes `_input` and `_output` must be assigned. These correspond to the input of the module (`x`) and the output (after whatever operations the module does in the forward pass).\n",
        "    - Inside `backward()`, attributes `_grad_output` and `grad_input` must be assigned. These correspond to the gradient of the loss w.r.t. module's outputs and module's inputs respectively. To do the backward pass, attributes `_input` and `_output` containing the values of the forward pass might come handy.\n",
        "- `get_params()` and `set_params()` are getter and setter functions for _all the **trainable** parameters_ of the module.\n",
        "- `get_grad_params()` method returns the gradient of the loss w.r.t. the **trainable** parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e1358e0f",
      "metadata": {
        "id": "e1358e0f"
      },
      "outputs": [],
      "source": [
        "class Module(abc.ABC):\n",
        "    '''\n",
        "    A module defines a sub-graph G' = (V, E) in the DAG G that represents a\n",
        "    neural network. Therefore G' is a subset of G. An instantiation of a module\n",
        "    thus represents a realization of a subgraph, comprising of a set of nodes and\n",
        "    their connections.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        # Train mode or eval mode\n",
        "        self.train = True\n",
        "\n",
        "        # Forward pass cache\n",
        "        self._input = None\n",
        "        self._output = None\n",
        "\n",
        "        # Backward pass cache\n",
        "        self._grad_output = None\n",
        "        self._grad_input = None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Computes the forward pass z = f(x) where f is the function represented by\n",
        "        the module, x is the input, and z is the output of the forward pass.\n",
        "\n",
        "        Assigns to attributes self._input and self._output.\n",
        "\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim) where m is the number of data\n",
        "                points and in_dim is the number of input features.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_forward_attrs(self):\n",
        "        '''Sanity check after a forward pass.'''\n",
        "        assert self._input is not None\n",
        "        assert self._output is not None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        '''\n",
        "        Computes the gradient of loss w.r.t. cached input and trainable parameters.\n",
        "\n",
        "        Assigns to attributes self._grad_output and self._grad_input.\n",
        "        The gradients w.r.t. trainable parameters must also be cached so that\n",
        "        they can be returned by self.get_grad_params().\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of loss w.r.t. output z of the module, dL/dz.\n",
        "                shape (m, out_dim) where m is the number of data points and\n",
        "                out_dim is the number of output features.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        '''Sanity check after a backward pass.'''\n",
        "        assert self._grad_output is not None\n",
        "        assert self._grad_input is not None\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        '''\n",
        "        Returns the trainable parameters of the module. If there are no trainable\n",
        "        parameters, returns None.\n",
        "\n",
        "        Returns:\n",
        "            arr: (jagged) array of trainable parameters, where the entries are\n",
        "                differently-sized numpy arrays themselves.\n",
        "        '''\n",
        "        return None\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        '''\n",
        "        Sets the trainable parameters to params. If there are no trainable parameters\n",
        "        to set, raises a RuntimeError.\n",
        "\n",
        "        Args:\n",
        "            params: (jagged) array of trainable parameters, in the same order\n",
        "                as obtained from self.get_params(). The identity operation is satisfied:\n",
        "                ```\n",
        "                x = self.get_params()\n",
        "                self.set_params(x)\n",
        "                x == self.get_params()\n",
        "                ```\n",
        "        '''\n",
        "        raise RuntimeError('No trainable parameters to set!')\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        '''\n",
        "        Returns the gradients of the loss w.r.t. trainable parameters of the module.\n",
        "        If there are no trainable parameters, returns None.\n",
        "\n",
        "        Returns:\n",
        "            arr: (jagged) array of gradients of trainable parameters,\n",
        "                where the entries are differently-sized numpy arrays themselves.\n",
        "        '''\n",
        "        return None\n",
        "\n",
        "    def train_mode(self):\n",
        "        '''\n",
        "        Switches on the training mode. Useful e.g. in Dropout, where the nodes must be\n",
        "        dropped only during training, not evaluation.\n",
        "        '''\n",
        "        self.train = True\n",
        "\n",
        "    def eval_mode(self):\n",
        "        '''\n",
        "        Switches on the evaluation mode. Useful e.g. in Dropout, where the nodes must be\n",
        "        dropped only during training, not evaluation.\n",
        "        '''\n",
        "        self.train = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c80c47-da9d-4fee-aa1e-ecaa0e77ff9d",
      "metadata": {
        "id": "66c80c47-da9d-4fee-aa1e-ecaa0e77ff9d"
      },
      "source": [
        "### [Task 1] Complete Forward-Backward Prop for a Sequential Network\n",
        "\n",
        "Next, we will define a `Sequential` architecture, called as such because it allows for implementing a sequence of modules. Each module is called a layer, so that the preceding layer connects to the next layer. Sequential architectures are also called \"feed-forward\" networks.\n",
        "\n",
        "In this task, you will finish the `backward()` method. Remember to cache `_grad_output` attribute in the method. Also, store the results in `_grad_input`.\n",
        "\n",
        "We give you the forward pass as reference: iterate through the layers, doing a forward pass on the output of the preceding layer (accessed using `_output`) to get the output of each layer. The output of the sequential network is the final output.\n",
        "\n",
        "Similarly, in the backward pass, you should iterate through the layers in a reverse manner, doing a backward pass on the gradients w.r.t. inputs of the succeeding layer (accessed using `_grad_input`) to get the gradients w.r.t. inputs of each layer. The gradients w.r.t. inputs of the sequential network should be the final gradients you calculate in this reverse iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2a43e012-e0d3-4666-8692-276b4c834953",
      "metadata": {
        "id": "2a43e012-e0d3-4666-8692-276b4c834953"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    '''\n",
        "    A sequence of modules, representing a DAG path.\n",
        "    '''\n",
        "    def __init__(self, layers: List[Module]):\n",
        "        '''\n",
        "        Args:\n",
        "            layers: A list of modules to initialize the sequential network.\n",
        "        '''\n",
        "        super(Sequential, self).__init__()\n",
        "        self.layers = layers\n",
        "\n",
        "    def add(self, layer: Module):\n",
        "        '''\n",
        "        Adds the layer at the end of the sequential network.\n",
        "        '''\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Computes a forward pass sequentially on the network layers.\n",
        "        '''\n",
        "        self._input = x\n",
        "        n_layers = len(self.layers)\n",
        "        for i in range(n_layers):\n",
        "            _output_prev = self._input if i == 0 else self.layers[i-1]._output\n",
        "            self.layers[i].forward(_output_prev)\n",
        "        self._output = self._input if n_layers == 0 else self.layers[-1]._output\n",
        "\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        '''\n",
        "        Backpropagates the gradient w.r.t. output of the sequential network,\n",
        "        computing gradients w.r.t. input and trainable parameters of the network.\n",
        "        '''\n",
        "        #### TASK 1 CODE\n",
        "        self._grad_output = grad_output\n",
        "        n_layers = len(self.layers)\n",
        "        go = grad_output\n",
        "        for i in range(n_layers - 1, -1, -1):\n",
        "            self.layers[i].backward(go)\n",
        "            go = self.layers[i]._grad_input\n",
        "        self._grad_input = go\n",
        "        #### TASK 1 CODE\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            p = layer.get_params()\n",
        "            if p is not None:\n",
        "                params.append(p)\n",
        "\n",
        "        # Wrap parameters in an array. Just np.array(params) won't work due to broadcasting\n",
        "        # conflicts: https://stackoverflow.com/a/49119983. So initialize array and then fill.\n",
        "        arr = np.empty(len(params), dtype=np.ndarray)\n",
        "        arr[:] = params\n",
        "        return arr\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        # Since params has trainable parameters listed in the same order as\n",
        "        # get_params() would have returned, follow the same iteration, and call\n",
        "        # layer.set_params() on params[i] where i is the ith layer with any trainable\n",
        "        # parameters\n",
        "        i = 0\n",
        "        for layer in self.layers:\n",
        "            p = layer.get_params()\n",
        "            if p is not None:\n",
        "                layer.set_params(params[i])\n",
        "                i += 1\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        grad_params = []\n",
        "        for layer in self.layers:\n",
        "            g = layer.get_grad_params()\n",
        "            if g is not None:\n",
        "                grad_params.append(g)\n",
        "        arr = np.empty(len(grad_params), dtype=np.ndarray)\n",
        "        arr[:] = grad_params\n",
        "        return arr\n",
        "\n",
        "    def train_mode(self):\n",
        "        # Switch on training in all layers\n",
        "        super().train_mode()\n",
        "        for layer in self.layers:\n",
        "            layer.train_mode()\n",
        "\n",
        "    def eval_mode(self):\n",
        "        # Switch on eval in all layers\n",
        "        super().eval_mode()\n",
        "        for layer in self.layers:\n",
        "            layer.eval_mode()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6a88ae",
      "metadata": {
        "id": "6e6a88ae"
      },
      "source": [
        "### [Task 2] Complete Forward-Backward Prop for a Linear Layer\n",
        "\n",
        "A linear layer defines an affine transformation on inputs $x \\in \\mathbb{R}^{m \\times d_{in}}$, that is $z = x W + b$ where $W \\in \\mathbb{R}^{d_{in} \\times d_{out}}, b \\in \\mathbb{R}^{d_{out}}$ are trainable parameters. This affine transformation defines a \"fully-connected module\" where each output feature $z[:, j]$ is connected to input feature $x[:, i]$ with an edge-weight $W[i, j]$.\n",
        "\n",
        "We provide you with code that initializes the weight and bias parameters, and their corresponding attributes for storing gradients.\n",
        "\n",
        "You will complete the `backward()` method. We provide you with the forward pass as reference.\n",
        "\n",
        "The discussion from the recitation might be helpful here.\n",
        "\n",
        "**Note on Initialization**: We initialize i.i.d. $W[i, j] \\sim \\mathcal{N}\\left( 0, \\sqrt{\\frac{2}{d_{in}}} \\right)$ and $b_j = 0$ for all $i, j$. The scaled variance for the weights is important for faster convergence, but this is outside the scope of the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a2285d4c",
      "metadata": {
        "id": "a2285d4c"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    '''\n",
        "    Linear transformation on the inputs, z = xW + b.\n",
        "\n",
        "    Corresponds to all nodes in the preceding layer connected to all nodes in the\n",
        "    current layer.\n",
        "    '''\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        '''\n",
        "        Args:\n",
        "            in_dim: Number of input dimensions (number of incoming connections\n",
        "                in the network).\n",
        "            out_dim: Number of output dimensions (number of outgoing connections\n",
        "                in the network).\n",
        "        '''\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # Initialize trainable parameters\n",
        "        self.weight = np.random.normal(0, np.sqrt(2/in_dim), (in_dim, out_dim))\n",
        "        self.bias =  np.zeros(out_dim)\n",
        "\n",
        "        # Initialize gradients w.r.t. trainable parameters\n",
        "        self._grad_weight = None\n",
        "        self._grad_bias = None\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim)\n",
        "        '''\n",
        "        assert x.shape[1] == self.weight.shape[0]\n",
        "        \n",
        "        self._input = x\n",
        "        self._output = x @ self.weight + self.bias\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Computes gradient w.r.t. trainable parameters and input and returns the gradient\n",
        "        w.r.t. input.\n",
        "\n",
        "        Important: gradients are accumulated for trainable parameters, i.e. added to the existing\n",
        "        values.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient w.r.t. output dL/dz. shape (m, out_dim)\n",
        "\n",
        "        Returns:\n",
        "            grad_input: shape (m, in_dim)\n",
        "        '''\n",
        "        assert grad_output.shape[1] == self.weight.shape[1]\n",
        "\n",
        "        #### TASK 2 CODE\n",
        "        self._grad_output = grad_output\n",
        "        self._grad_weight = self._input.T @ self._grad_output\n",
        "        self._grad_bias = grad_output.sum(axis = 0)\n",
        "        self._grad_input = self._grad_output @ self.weight.T\n",
        "        #### TASK 2 CODE\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        super()._check_backward_attrs()\n",
        "        assert self._grad_weight is not None\n",
        "        assert self._grad_bias is not None\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        params = np.empty(2, dtype=np.ndarray)\n",
        "        params[0] = self.weight\n",
        "        params[1] = self.bias\n",
        "        return params\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        assert len(params) == 2\n",
        "        self.weight = params[0]\n",
        "        self.bias = params[1]\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        grad_params = np.empty(2, dtype=np.ndarray)\n",
        "        grad_params[0] = self._grad_weight\n",
        "        grad_params[1] = self._grad_bias\n",
        "        return grad_params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6478f7d4",
      "metadata": {
        "id": "6478f7d4"
      },
      "source": [
        "### [Task 3] Complete Forward-Backward Prop for ReLU Activation\n",
        "\n",
        "The `Module` interface allows us to use every element of in the network as a node in the graph, even the activation function applied to a node. The `ReLU` activation function, $z = \\max(0, x)$ for inputs $x$, does not have any trainable parameters, however.\n",
        "\n",
        "You will complete the `backward()` method below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "77f30ff0",
      "metadata": {
        "id": "77f30ff0"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    '''\n",
        "    ReLU activation, not trainable. z = max(x, 0) for each input value x.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim)\n",
        "        '''\n",
        "        self._input = x\n",
        "        self._output = np.maximum(0., self._input)\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Since there are no trainable parameters, only the gradient w.r.t. input is computed.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient w.r.t. output dL/dz. Any shape\n",
        "        '''\n",
        "        assert grad_output.shape == self._input.shape\n",
        "\n",
        "        #### TASK 3 CODE\n",
        "        self._grad_output = grad_output\n",
        "        self._grad_input = self._grad_output * (self._input > 0)\n",
        "        #### TASK 3 CODE\n",
        "        self._check_backward_attrs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2db3c0",
      "metadata": {
        "id": "be2db3c0"
      },
      "source": [
        "### Regularization using Dropout\n",
        "\n",
        "We define the final module we will use in this homework: Dropout. Dropout is a commonly used technique for regularization to try to \"limit dependence\" of the network's prediction capabilities on any specific trainable parameters.\n",
        "\n",
        "Dropout accomplishes by switching off nodes, i.e. \"dropping\" them, with some probability during training. Nodes are only dropped during training and not evaluation as Dropout is a regularization technique. Using Dropout during evaluation would cause us to not use the full trained network. This is where the `train` attribute defined in the `Module` abstract class comes in handy. Before training the model, we will call `model.train_mode()` to set `model.train = True`. The implementation `train_mode()` in `Sequential` ensures that the `train` attribute of each layer is set to `True`. Before using the model for evaluation, we will call `model.eval_mode()` to set `model.train = False` and all `train` attributes of the comprising layers to `False`. This way Dropout is not active during evaluation.\n",
        "\n",
        "The non-dropped values are rescaled so that the expectation of the non-dropped node is the same as when dropout wasn't applied.\n",
        "\n",
        "Read through the code so you understand how Dropout is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "136cee5b",
      "metadata": {
        "id": "136cee5b"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    '''\n",
        "    A dropout layer.\n",
        "    '''\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            p: (default 0.5) Probability of dropping each node (prob. of setting each value to 0).\n",
        "                If p is 0, then no nodes are dropped, i.e. we get the identity layer.\n",
        "        '''\n",
        "        assert 0 <= p <= 1\n",
        "        super(Dropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        self._input = x\n",
        "        self._output = self._input\n",
        "        if self.train and (not np.isclose(self.p, 0)):\n",
        "            # In training mode and drop probability is positive\n",
        "\n",
        "            # Create a mask to apply to the input using Bernoulli(1-p) RV\n",
        "            self.mask = np.random.binomial(1, 1 - self.p, x.shape).astype(float)\n",
        "\n",
        "            # Scale the mask so that the expected value of prediction during\n",
        "            # testing is same as x, and not (1-p)x\n",
        "            self.mask /= 1 - self.p\n",
        "\n",
        "            self._output *= self.mask\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        self._grad_output = grad_output\n",
        "        self._grad_input = self._grad_output\n",
        "        if self.train and (not np.isclose(self.p, 0)):\n",
        "            # In training mode and drop probability is positive, _grad_input is masked\n",
        "            self._grad_input *= self.mask\n",
        "        self._check_backward_attrs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3067360-4d42-4516-b1a8-806fbe18727f",
      "metadata": {
        "id": "e3067360-4d42-4516-b1a8-806fbe18727f"
      },
      "source": [
        "### [Task 4] Complete MultiLogisticLoss\n",
        "\n",
        "A loss function is applied on the predicted probabilities of each label $r \\in \\mathbb{R}^{m \\times k}$ and the true labels $y \\in \\{0, \\dots, k\\}^{m}$. Following the code pattern of `Module`, we define the abstract class `Loss`.\n",
        "- The `forward()` method computes the loss value using responses $r$ and true labels $y$. It assigns to attributes `_input = r`, `_input_target = y`, and `_output`.\n",
        "- The `backward()` method computes the gradient of the loss w.r.t. the inputs, i.e. the responses `r`. It assigns to attribute `_grad_input`.\n",
        "\n",
        "In this task, you will complete `MultiLogisticLoss`. The implementation is almost the same as you did in the previous homework.\n",
        "\n",
        "We provide you an implementation of softmax in `utils.softmax`---you might find this useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1a48ee89-c655-4c70-87fd-810a261107cc",
      "metadata": {
        "id": "1a48ee89-c655-4c70-87fd-810a261107cc"
      },
      "outputs": [],
      "source": [
        "class Loss(abc.ABC):\n",
        "    '''Defines a loss function.'''\n",
        "    def __init__(self, k: int):\n",
        "        '''\n",
        "        Args:\n",
        "            k: Number of labels.\n",
        "        '''\n",
        "        self.k = k\n",
        "        self._input = None\n",
        "        self._input_target = None\n",
        "        self._output = None\n",
        "        self._grad_input = None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, r: np.ndarray, y: np.ndarray):\n",
        "        '''\n",
        "        Computes the loss value using responses r and true labels y.\n",
        "\n",
        "        Sets the attributes self._input, self._input_target, and self._output.\n",
        "\n",
        "        Args:\n",
        "            r: Responses of a classifier. shape (m, k) where m is the number of data\n",
        "                points.\n",
        "            y: True labels. shape (m). For all i, 0 <= y_i < k\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_forward_attrs(self):\n",
        "        assert self._input is not None\n",
        "        assert self._input_target is not None\n",
        "        assert self._output is not None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self):\n",
        "        '''\n",
        "        Computes the gradient of the loss value w.r.t. cached responses.\n",
        "\n",
        "        Sets the attribute self._grad_input.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        assert self._grad_input is not None\n",
        "\n",
        "\n",
        "class MultiLogisticLoss(Loss):\n",
        "    def __init__(self, k: int):\n",
        "        super(MultiLogisticLoss, self).__init__(k)\n",
        "\n",
        "    def forward(self, r: np.ndarray, y: np.ndarray):\n",
        "        '''\n",
        "        Computes the multiclass logistic loss, using the softmax operation to\n",
        "        convert responses r to normalized probabilities.\n",
        "        '''\n",
        "        assert r.shape[0] == y.shape[0]\n",
        "        assert r.shape[1] == self.k\n",
        "\n",
        "        self._input = r\n",
        "        self._input_target = y\n",
        "\n",
        "        stable_r = self._input - np.max(self._input, axis=1)[:, np.newaxis]\n",
        "        nll = np.log(np.sum(np.exp(stable_r), axis=1)) - \\\n",
        "            np.take_along_axis(stable_r, self._input_target[:, np.newaxis], axis=1).flatten()\n",
        "        self._output = np.mean(nll)\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self):\n",
        "        #### TASK 4 CODE\n",
        "        self._grad_input = utils.softmax(self._input)\n",
        "        self._grad_input -= np.eye(self.k)[self._input_target]\n",
        "        self._grad_input /= len(self._input)\n",
        "        #### TASK 4 CODE\n",
        "        self._check_backward_attrs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4ff6b0",
      "metadata": {
        "id": "fc4ff6b0",
        "tags": []
      },
      "source": [
        "### Testing your gradient calculation\n",
        "\n",
        "To help you ensure your code calculates the forward and backward passes correctly, we have provided a few tests below. These tests match your implementation against a numerical approximation of the gradient. You should get relative errors on the order of $10^{-8}$ or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1cf16719-87ec-409f-97c1-ed0b414cff8a",
      "metadata": {
        "id": "1cf16719-87ec-409f-97c1-ed0b414cff8a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi Logistic Loss Grad Test\n",
            "Relative err between your computation and numerical gradient: 9.176152e-09\n"
          ]
        }
      ],
      "source": [
        "def test_log_grad():\n",
        "    '''\n",
        "    Tests MultiLogisticLoss Gradient implementation. The relative error should be small.\n",
        "    '''\n",
        "    print('Multi Logistic Loss Grad Test')\n",
        "    loss = MultiLogisticLoss(k=10)\n",
        "    x = np.random.rand(3, 10)\n",
        "    y = np.array([4, 5, 6])\n",
        "    def test_f(x):\n",
        "        loss.forward(x, y)\n",
        "        return loss._output\n",
        "\n",
        "    # Backprop manually\n",
        "    test_f(x)\n",
        "    loss.backward()\n",
        "    grad_input = loss._grad_input\n",
        "\n",
        "    # Gradient approximation\n",
        "    grad_input_num = utils.numeric_grad(test_f, x, 1, 1e-6)\n",
        "\n",
        "    rel_err = utils.relative_error(grad_input, grad_input_num, 1e-8)\n",
        "    print(f'Relative err between your computation and numerical gradient: {rel_err:e}')\n",
        "\n",
        "test_log_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "371e4fde-7723-48cb-b3af-41ee5fac0e01",
      "metadata": {
        "id": "371e4fde-7723-48cb-b3af-41ee5fac0e01",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Linear...\n",
            "Module Grad Test\n",
            "Relative err between your computation and numerical gradient: 3.063965e-08\n",
            "Testing ReLU...\n",
            "Module Grad Test\n",
            "Relative err between your computation and numerical gradient: 5.139782e-10\n",
            "Testing Dropout...\n",
            "Module Grad Test\n",
            "Relative err between your computation and numerical gradient: 5.139782e-10\n",
            "Testing 2-layer model\n",
            "Module Grad Test\n",
            "Relative err between your computation and numerical gradient: 3.919368e-09\n"
          ]
        }
      ],
      "source": [
        "class LossForUnitTesting(Loss):\n",
        "    def __init__(self, k):\n",
        "        super(LossForUnitTesting, self).__init__(k)\n",
        "\n",
        "    def forward(self, x: np.ndarray, y: np.ndarray):\n",
        "        self._input = x\n",
        "        self._input_target = y\n",
        "        self._output = np.mean(np.sum(np.abs(self._input), axis=1))\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self):\n",
        "        self._grad_input = np.sign(self._input) / self._input.shape[0]\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "\n",
        "def test_module(model: Module):\n",
        "    '''\n",
        "    Tests Module Gradient implementation. The relative error should be small.\n",
        "    '''\n",
        "    print('Module Grad Test')\n",
        "\n",
        "    model.eval_mode()\n",
        "\n",
        "    loss = LossForUnitTesting(k=10)\n",
        "    x = np.random.rand(4, 10)\n",
        "    y = np.array([4, 5, 6, 8])\n",
        "    def test_f(x):\n",
        "        model.forward(x)\n",
        "        loss.forward(model._output, y)\n",
        "        return loss._output\n",
        "\n",
        "    # Backprop manually\n",
        "    test_f(x)\n",
        "    loss.backward()\n",
        "    model.backward(loss._grad_input)\n",
        "    grad_input = model._grad_input\n",
        "\n",
        "    # Gradient approximation\n",
        "    grad_input_num = utils.numeric_grad(test_f, x, 1, 1e-6)\n",
        "\n",
        "    rel_err = utils.relative_error(grad_input, grad_input_num, 1e-8)\n",
        "    print(f'Relative err between your computation and numerical gradient: {rel_err:e}')\n",
        "\n",
        "print('Testing Linear...')\n",
        "model = Linear(10, 20)\n",
        "test_module(model)\n",
        "\n",
        "print('Testing ReLU...')\n",
        "model = ReLU()\n",
        "test_module(model)\n",
        "\n",
        "print('Testing Dropout...')\n",
        "model = Dropout()\n",
        "test_module(model)\n",
        "\n",
        "print('Testing 2-layer model')\n",
        "model = Sequential([\n",
        "    Linear(10, 20),\n",
        "    ReLU(),\n",
        "    Linear(20, 10)\n",
        "])\n",
        "test_module(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2b53f5-757e-4222-893d-6a97feb31176",
      "metadata": {
        "id": "3e2b53f5-757e-4222-893d-6a97feb31176"
      },
      "source": [
        "### [Task 5] Building a Neural Network Classifier\n",
        "\n",
        "We can now define a classifier using the code structure we have followed in the previous homeworks. A neural network classifier consists of an initialized model and a loss, specified when instantiating `ERMNeuralNetClassifier`.\n",
        "\n",
        "We will be using SGD to train the classifier, using the same implementation from the previous homework. We have included the SGD implementation in `utils`.\n",
        "\n",
        "In this task, you will complete the `predict` and `train_grad` methods. The first is for logging purposes, and the second is for calculating the gradients w.r.t. trainable parameters. Like `train_obj`, `train_grad` takes in and sets the parameters of the model and does a forward pass to calculate the loss on data `X, y`  indexed by the batch indices. `train_grad` finally does a backward propagation and returns the gradients w.r.t. trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6afcdba5-c019-496d-9861-e025bd532d39",
      "metadata": {
        "id": "6afcdba5-c019-496d-9861-e025bd532d39"
      },
      "outputs": [],
      "source": [
        "class ERMNeuralNetClassifier(linclass.Classifier):\n",
        "    '''\n",
        "    Neural network trained by minimizing the empirical risk with SGD,\n",
        "    w.r.t. some loss function.\n",
        "    '''\n",
        "    def __init__(self, model: Module, loss: Loss, **kwargs):\n",
        "        '''\n",
        "        Args:\n",
        "            model: A neural network object with initialized parameters.\n",
        "            loss: A loss function.\n",
        "        '''\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.params0 = self.model.get_params()\n",
        "        self.params = None\n",
        "        self.loss = loss\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Returns predicted labels for data.\n",
        "\n",
        "        Args:\n",
        "            X: Data features. shape (m, d_in) where d_in is the number of\n",
        "                input features of self.model.\n",
        "\n",
        "        Returns:\n",
        "            shape (m)\n",
        "        '''\n",
        "        assert self.params is not None, \"Classifier hasn't been fit!\"\n",
        "\n",
        "        # Switch to evaluation mode to disable dropout\n",
        "        self.model.eval_mode()\n",
        "\n",
        "        #### TASK 5 CODE\n",
        "        self.model.forward(X)\n",
        "        ypred = np.argmax(self.model._output, axis = 1)\n",
        "        return ypred\n",
        "        #### TASK 5 CODE\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, **sgd_kwargs):\n",
        "        '''\n",
        "        Fits the classifier on dataset.\n",
        "\n",
        "        Args:\n",
        "            X: Data features. shape (m, d_in) where d_in is the number of input\n",
        "                features of self.model.\n",
        "            y: Data labels, 0 <= y_i < k. shape (m)\n",
        "        '''\n",
        "        assert X.shape[0] == y.shape[0]\n",
        "\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Define training objective\n",
        "        def train_obj(params: np.ndarray, batch: Optional[np.ndarray] = None) -> float:\n",
        "            '''\n",
        "            Calculates the training objective with parameters on a batch of training samples.\n",
        "\n",
        "            Args:\n",
        "                params: Trainable parameters, in the same format as self.model.get_params().\n",
        "                batch: (default None) Indices of samples to calculate objective on. If None,\n",
        "                    calculate objective on all samples.\n",
        "            '''\n",
        "            if batch is None:\n",
        "                # All data is in a batch\n",
        "                batch = slice(None)\n",
        "\n",
        "            self.model.set_params(params)\n",
        "\n",
        "            # Forward pass\n",
        "            self.model.forward(X[batch])\n",
        "            self.loss.forward(self.model._output, y[batch])\n",
        "\n",
        "            loss_val = self.loss._output\n",
        "            return loss_val\n",
        "\n",
        "        # Define training gradient\n",
        "        def train_grad(params: np.ndarray, batch: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "            '''\n",
        "            Returns the gradient of the training objective w.r.t. parameters,\n",
        "            calculated on a batch of training samples.\n",
        "\n",
        "            Args:\n",
        "                params: Trainable parameters, in the same format as self.model.get_params().\n",
        "                batch: (default None) Indices of samples to calculate objective on. If None,\n",
        "                    calculate objective on all samples.\n",
        "            '''\n",
        "            if batch is None:\n",
        "                # All data is in a batch\n",
        "                batch = slice(None)\n",
        "\n",
        "            self.model.set_params(params)\n",
        "            #### TASK 5 CODE\n",
        "            # Forward pass\n",
        "            self.model.forward(X[batch])\n",
        "            # Backward pass\n",
        "            self.loss.forward(self.model._output, y[batch])\n",
        "            self.loss.backward()\n",
        "            self.model.backward(self.loss._grad_input)\n",
        "            \n",
        "            grad_params = self.model.get_grad_params()\n",
        "            #### TASK 5 CODE\n",
        "            return grad_params\n",
        "\n",
        "        self.sgd_loggers = [\n",
        "            utils.SGDLogger('train_obj', train_obj, can_display=True, per_epoch=True),\n",
        "        ] + sgd_kwargs.pop('loggers', [])\n",
        "\n",
        "        # Switch to training mode to enable dropout, if present in the model\n",
        "        self.model.train_mode()\n",
        "\n",
        "        # Optimize using SGD\n",
        "        self.params = utils.SGD(\n",
        "            self.params0,\n",
        "            train_grad,\n",
        "            m,\n",
        "            loggers=self.sgd_loggers,\n",
        "            **sgd_kwargs\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20b3f74",
      "metadata": {
        "id": "e20b3f74"
      },
      "source": [
        "### Training the Neural Network Classifier\n",
        "\n",
        "We will now train a classifier using your implementation. We will use a toy example first. You should check that your training objective decreases with training, and that you get better than random training error (think what the error of random prediction is when the number of labels is 5).\n",
        "\n",
        "After this, we will train a classifier and evaluate it on a real-world dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eaee175b-38cf-41d3-a75f-a0db557328ad",
      "metadata": {
        "id": "eaee175b-38cf-41d3-a75f-a0db557328ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch: 0, train_obj: 1.773089387369215\n",
            "--- Epoch: 100, train_obj: 1.6264845126203757\n",
            "--- Epoch: 200, train_obj: 1.5823367309118779\n",
            "--- Epoch: 300, train_obj: 1.5478415524391145\n",
            "--- Epoch: 400, train_obj: 1.5176034728379904\n",
            "--- Epoch: 500, train_obj: 1.4935995581072277\n",
            "--- Epoch: 600, train_obj: 1.472814521869054\n",
            "--- Epoch: 700, train_obj: 1.4544168260579875\n",
            "--- Epoch: 800, train_obj: 1.43755934963062\n",
            "--- Epoch: 900, train_obj: 1.4217601665228434\n",
            "train_err: 0.600000\n"
          ]
        }
      ],
      "source": [
        "n_train = 50\n",
        "k = 5\n",
        "X_train = np.random.rand(n_train, 10)\n",
        "y_train = np.random.choice(k, n_train)\n",
        "\n",
        "model = Sequential([\n",
        "    Linear(10, 20),\n",
        "    ReLU(),\n",
        "    Linear(20, k)\n",
        "])\n",
        "loss = MultiLogisticLoss(k=k)\n",
        "clf = ERMNeuralNetClassifier(model, loss)\n",
        "\n",
        "clf.fit(X_train, y_train, eta=0.01, n_epochs=1000,\n",
        "        verbose=True, verbose_epoch_interval=100)\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "train_err = utils.empirical_err(y_train, y_train_pred)\n",
        "\n",
        "print(f'train_err: {train_err:5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d119e56",
      "metadata": {
        "id": "9d119e56"
      },
      "source": [
        "### [Task 6] Fashion MNIST dataset\n",
        "\n",
        "As mentioned, for this assigment, we will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. It is a slightly harder 10-class classification problem than the MNIST dataset you worked with last week but is set up in the same way (10 classes, 28x28 images).\n",
        "\n",
        "We provide you with `.npy` files of the data to use. The code below loads the training data and labels. Your task is to submit labels for the test set.\n",
        "\n",
        "We provide an example network for inspiration. You should build a better model, perhaps using deeper models or wider layers (just examples).\n",
        "\n",
        "#### **Endgame**\n",
        "\n",
        "Finally, upload the predicted test labels to [this kaggle competition](https://www.kaggle.com/t/e7ec8dd1f8e64aa2997d82edd5e8f6eb), where we will first evaluate it on a slice of the test set to show your standings. Kaggle allows resubmissions, but beware of submitting relentlessly to climb up in the public leaderboard---you'd be overfitting to the test set. After the deadline, we will test it on secret data, which the public leaderboard does not show you, to see how well your predictor generalizes. We recommend using validation or cross-validation for developing your model and then submitting to Kaggle.\n",
        "\n",
        "**Your grade will not depend on your performance relative to others in the class**. We simply want you to try your best on getting good performance. Any concepts you learnt in this class or code you developed in this class are fair game. You are also welcome to use methods not directly covered in this class, but **the work must be your own** -- i.e., you must implement and train the models yourself, even if the architecture is inspired by something you saw elsewhere.\n",
        "\n",
        "**Report your Kaggle username here so we can know where you are on the leaderboard!**\n",
        "#### **Your User Name: letranconghung, under name Hung L.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9a2fc8c3",
      "metadata": {
        "id": "9a2fc8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch: 0, train_obj: 0.7854294944265104\n",
            "--- Epoch: 1, train_obj: 0.6623864912903151\n",
            "--- Epoch: 2, train_obj: 0.6071661195308878\n",
            "--- Epoch: 3, train_obj: 0.5694127644985907\n",
            "--- Epoch: 4, train_obj: 0.5463399283574302\n",
            "--- Epoch: 5, train_obj: 0.5285976057118981\n",
            "--- Epoch: 6, train_obj: 0.5086578028712795\n",
            "--- Epoch: 7, train_obj: 0.5006866748628566\n",
            "--- Epoch: 8, train_obj: 0.4852357124796578\n",
            "--- Epoch: 9, train_obj: 0.4758477859411868\n",
            "--- Epoch: 10, train_obj: 0.46814120539534143\n",
            "--- Epoch: 11, train_obj: 0.46585802449710073\n",
            "--- Epoch: 12, train_obj: 0.4558283914960319\n",
            "--- Epoch: 13, train_obj: 0.4475244773133152\n",
            "--- Epoch: 14, train_obj: 0.44510178859304583\n",
            "--- Epoch: 15, train_obj: 0.4385863373353356\n",
            "--- Epoch: 16, train_obj: 0.4325293898891585\n",
            "--- Epoch: 17, train_obj: 0.42858014797898236\n",
            "--- Epoch: 18, train_obj: 0.427492642358077\n",
            "--- Epoch: 19, train_obj: 0.42553744881359384\n",
            "--- Epoch: 20, train_obj: 0.415288612835887\n",
            "--- Epoch: 21, train_obj: 0.41071249582683944\n",
            "--- Epoch: 22, train_obj: 0.4130694730711007\n",
            "--- Epoch: 23, train_obj: 0.40458843653321386\n",
            "--- Epoch: 24, train_obj: 0.4017255076920734\n",
            "--- Epoch: 25, train_obj: 0.39893961980383325\n",
            "--- Epoch: 26, train_obj: 0.39492053361213264\n",
            "--- Epoch: 27, train_obj: 0.39169495451662717\n",
            "--- Epoch: 28, train_obj: 0.3897047011726692\n",
            "--- Epoch: 29, train_obj: 0.3886386452147706\n",
            "--- Epoch: 30, train_obj: 0.38465202649496905\n",
            "--- Epoch: 31, train_obj: 0.3827964979495684\n",
            "--- Epoch: 32, train_obj: 0.37946388491838867\n",
            "--- Epoch: 33, train_obj: 0.37726042184054964\n",
            "--- Epoch: 34, train_obj: 0.3729199405415264\n",
            "--- Epoch: 35, train_obj: 0.3738670198230911\n",
            "--- Epoch: 36, train_obj: 0.36850474461583405\n",
            "--- Epoch: 37, train_obj: 0.3693026646977294\n",
            "--- Epoch: 38, train_obj: 0.36645227610754\n",
            "--- Epoch: 39, train_obj: 0.36477675590926445\n",
            "--- Epoch: 40, train_obj: 0.36165482041840247\n",
            "--- Epoch: 41, train_obj: 0.36061220415066136\n",
            "--- Epoch: 42, train_obj: 0.3567532416850717\n",
            "--- Epoch: 43, train_obj: 0.3557436151395176\n",
            "--- Epoch: 44, train_obj: 0.3536370065559156\n",
            "--- Epoch: 45, train_obj: 0.35091494421624936\n",
            "--- Epoch: 46, train_obj: 0.3509464065208958\n",
            "--- Epoch: 47, train_obj: 0.3510204804453936\n",
            "--- Epoch: 48, train_obj: 0.3497261911964162\n",
            "--- Epoch: 49, train_obj: 0.34642295080581176\n",
            "--- Epoch: 50, train_obj: 0.34299265524988853\n",
            "--- Epoch: 51, train_obj: 0.3440192699424072\n",
            "--- Epoch: 52, train_obj: 0.3429385538063892\n",
            "--- Epoch: 53, train_obj: 0.3387681009110185\n",
            "--- Epoch: 54, train_obj: 0.34076393206979577\n",
            "--- Epoch: 55, train_obj: 0.3358533964725426\n",
            "--- Epoch: 56, train_obj: 0.33540130187169986\n",
            "--- Epoch: 57, train_obj: 0.33313541927117174\n",
            "--- Epoch: 58, train_obj: 0.33193481563249433\n",
            "--- Epoch: 59, train_obj: 0.3298900930828038\n",
            "--- Epoch: 60, train_obj: 0.32925373014506065\n",
            "--- Epoch: 61, train_obj: 0.32758961563118727\n",
            "--- Epoch: 62, train_obj: 0.32795767488317584\n",
            "--- Epoch: 63, train_obj: 0.3242979003694537\n",
            "--- Epoch: 64, train_obj: 0.32473848648307785\n",
            "--- Epoch: 65, train_obj: 0.32260939027529395\n",
            "--- Epoch: 66, train_obj: 0.3201175396984856\n",
            "--- Epoch: 67, train_obj: 0.31975359668496883\n",
            "--- Epoch: 68, train_obj: 0.3188684723123572\n",
            "--- Epoch: 69, train_obj: 0.31956225226325086\n",
            "--- Epoch: 70, train_obj: 0.315317082896344\n",
            "--- Epoch: 71, train_obj: 0.31599682633532195\n",
            "--- Epoch: 72, train_obj: 0.31716551763694284\n",
            "--- Epoch: 73, train_obj: 0.3146565622566478\n",
            "--- Epoch: 74, train_obj: 0.3138699289583645\n",
            "--- Epoch: 75, train_obj: 0.3120575607632179\n",
            "--- Epoch: 76, train_obj: 0.310962392018505\n",
            "--- Epoch: 77, train_obj: 0.30976639208449924\n",
            "--- Epoch: 78, train_obj: 0.307080080773971\n",
            "--- Epoch: 79, train_obj: 0.3065310533388423\n",
            "--- Epoch: 80, train_obj: 0.3064237404585745\n",
            "--- Epoch: 81, train_obj: 0.30487773462680406\n",
            "--- Epoch: 82, train_obj: 0.3029317770267948\n",
            "--- Epoch: 83, train_obj: 0.3037942241892814\n",
            "--- Epoch: 84, train_obj: 0.30067992802951904\n",
            "--- Epoch: 85, train_obj: 0.29948779440545886\n",
            "--- Epoch: 86, train_obj: 0.2983281819241824\n",
            "--- Epoch: 87, train_obj: 0.2988108615469026\n",
            "--- Epoch: 88, train_obj: 0.29819847406348393\n",
            "--- Epoch: 89, train_obj: 0.29549577595278453\n",
            "--- Epoch: 90, train_obj: 0.2962913495732934\n",
            "--- Epoch: 91, train_obj: 0.29778241454753696\n",
            "--- Epoch: 92, train_obj: 0.29369926098728466\n",
            "--- Epoch: 93, train_obj: 0.2926763407942956\n",
            "--- Epoch: 94, train_obj: 0.29099961241673833\n",
            "--- Epoch: 95, train_obj: 0.2907001828209624\n",
            "--- Epoch: 96, train_obj: 0.28925663322213474\n",
            "--- Epoch: 97, train_obj: 0.29034244360361605\n",
            "--- Epoch: 98, train_obj: 0.2885897144981897\n",
            "--- Epoch: 99, train_obj: 0.28809363098637986\n",
            "--- Epoch: 100, train_obj: 0.2890119104530284\n",
            "--- Epoch: 101, train_obj: 0.28885143754229237\n",
            "--- Epoch: 102, train_obj: 0.2851300795773665\n",
            "--- Epoch: 103, train_obj: 0.28297814221772827\n",
            "--- Epoch: 104, train_obj: 0.2818914025263924\n",
            "--- Epoch: 105, train_obj: 0.283096450571033\n",
            "--- Epoch: 106, train_obj: 0.2824098952912441\n",
            "--- Epoch: 107, train_obj: 0.2779107710719619\n",
            "--- Epoch: 108, train_obj: 0.2800503669950038\n",
            "--- Epoch: 109, train_obj: 0.27897708403481364\n",
            "--- Epoch: 110, train_obj: 0.27646613227047395\n",
            "--- Epoch: 111, train_obj: 0.2762454954211571\n",
            "--- Epoch: 112, train_obj: 0.2768303236424097\n",
            "--- Epoch: 113, train_obj: 0.27576306483140023\n",
            "--- Epoch: 114, train_obj: 0.27375530139069637\n",
            "--- Epoch: 115, train_obj: 0.2725395497692403\n",
            "--- Epoch: 116, train_obj: 0.2731588207058524\n",
            "--- Epoch: 117, train_obj: 0.27258863847798226\n",
            "--- Epoch: 118, train_obj: 0.2714245122900914\n",
            "--- Epoch: 119, train_obj: 0.2735843869450478\n",
            "--- Epoch: 120, train_obj: 0.2708785435008161\n",
            "--- Epoch: 121, train_obj: 0.2693270943562471\n",
            "--- Epoch: 122, train_obj: 0.26794564616634403\n",
            "--- Epoch: 123, train_obj: 0.26881868401633374\n",
            "--- Epoch: 124, train_obj: 0.26764484352816326\n",
            "--- Epoch: 125, train_obj: 0.264887459571976\n",
            "--- Epoch: 126, train_obj: 0.263951840449784\n",
            "--- Epoch: 127, train_obj: 0.2642578121039748\n",
            "--- Epoch: 128, train_obj: 0.26388971298896696\n",
            "--- Epoch: 129, train_obj: 0.2628625737298686\n",
            "--- Epoch: 130, train_obj: 0.2608033525927901\n",
            "--- Epoch: 131, train_obj: 0.26052154261607774\n",
            "--- Epoch: 132, train_obj: 0.2603516034484195\n",
            "--- Epoch: 133, train_obj: 0.2601152993693728\n",
            "--- Epoch: 134, train_obj: 0.25814612370817863\n",
            "--- Epoch: 135, train_obj: 0.25686498403093017\n",
            "--- Epoch: 136, train_obj: 0.2591057902643433\n",
            "--- Epoch: 137, train_obj: 0.2561005779540752\n",
            "--- Epoch: 138, train_obj: 0.2554730055738513\n",
            "--- Epoch: 139, train_obj: 0.2554847807659044\n",
            "--- Epoch: 140, train_obj: 0.2592325366510283\n",
            "--- Epoch: 141, train_obj: 0.2553813634603427\n",
            "--- Epoch: 142, train_obj: 0.2541442002545472\n",
            "--- Epoch: 143, train_obj: 0.2525993701472774\n",
            "--- Epoch: 144, train_obj: 0.25152260196202664\n",
            "--- Epoch: 145, train_obj: 0.2517238963568127\n",
            "--- Epoch: 146, train_obj: 0.25106592757328156\n",
            "--- Epoch: 147, train_obj: 0.24899598061099285\n",
            "--- Epoch: 148, train_obj: 0.24932098726024401\n",
            "--- Epoch: 149, train_obj: 0.24876742530791127\n",
            "--- Epoch: 150, train_obj: 0.24833772286552994\n",
            "--- Epoch: 151, train_obj: 0.24731096353892887\n",
            "--- Epoch: 152, train_obj: 0.24892561091310153\n",
            "--- Epoch: 153, train_obj: 0.2442527813182094\n",
            "--- Epoch: 154, train_obj: 0.2463779264609362\n",
            "--- Epoch: 155, train_obj: 0.24494102980162072\n",
            "--- Epoch: 156, train_obj: 0.24494740581526941\n",
            "--- Epoch: 157, train_obj: 0.24209357465262352\n",
            "--- Epoch: 158, train_obj: 0.24467672149777173\n",
            "--- Epoch: 159, train_obj: 0.24217704208524587\n",
            "--- Epoch: 160, train_obj: 0.24064123592770345\n",
            "--- Epoch: 161, train_obj: 0.2391720495904403\n",
            "--- Epoch: 162, train_obj: 0.2394699471890678\n",
            "--- Epoch: 163, train_obj: 0.24008433036360355\n",
            "--- Epoch: 164, train_obj: 0.2376733672108859\n",
            "--- Epoch: 165, train_obj: 0.23736144926189262\n",
            "--- Epoch: 166, train_obj: 0.23776514678346053\n",
            "--- Epoch: 167, train_obj: 0.23697344817731592\n",
            "--- Epoch: 168, train_obj: 0.24039990789183252\n",
            "--- Epoch: 169, train_obj: 0.2351370633865288\n",
            "--- Epoch: 170, train_obj: 0.2342835741162052\n",
            "--- Epoch: 171, train_obj: 0.23587010363102168\n",
            "--- Epoch: 172, train_obj: 0.23556476308144034\n",
            "--- Epoch: 173, train_obj: 0.23239137832995765\n",
            "--- Epoch: 174, train_obj: 0.23225850182543642\n",
            "--- Epoch: 175, train_obj: 0.2330611346043543\n",
            "--- Epoch: 176, train_obj: 0.23096654877504177\n",
            "--- Epoch: 177, train_obj: 0.2315433609176688\n",
            "--- Epoch: 178, train_obj: 0.2300647274197893\n",
            "--- Epoch: 179, train_obj: 0.22906501896513054\n",
            "--- Epoch: 180, train_obj: 0.23283891786868993\n",
            "--- Epoch: 181, train_obj: 0.22899221882808432\n",
            "--- Epoch: 182, train_obj: 0.23018036426332705\n",
            "--- Epoch: 183, train_obj: 0.22887936259996036\n",
            "--- Epoch: 184, train_obj: 0.22700399607445584\n",
            "--- Epoch: 185, train_obj: 0.2265892454077815\n",
            "--- Epoch: 186, train_obj: 0.2248880259953911\n",
            "--- Epoch: 187, train_obj: 0.2274680615503665\n",
            "--- Epoch: 188, train_obj: 0.2244057366592493\n",
            "--- Epoch: 189, train_obj: 0.2240436341402495\n",
            "--- Epoch: 190, train_obj: 0.2231814592154444\n",
            "--- Epoch: 191, train_obj: 0.221907482424019\n",
            "--- Epoch: 192, train_obj: 0.22519287520434986\n",
            "--- Epoch: 193, train_obj: 0.22242881215214455\n",
            "--- Epoch: 194, train_obj: 0.22339508024501936\n",
            "--- Epoch: 195, train_obj: 0.22226362226285026\n",
            "--- Epoch: 196, train_obj: 0.21872438180901585\n",
            "--- Epoch: 197, train_obj: 0.2185848709963052\n",
            "--- Epoch: 198, train_obj: 0.2189854001957458\n",
            "--- Epoch: 199, train_obj: 0.21744292675659563\n",
            "--- Epoch: 200, train_obj: 0.21710998744379878\n",
            "--- Epoch: 201, train_obj: 0.21727127650124486\n",
            "--- Epoch: 202, train_obj: 0.21590984267293856\n",
            "--- Epoch: 203, train_obj: 0.21620105644587242\n",
            "--- Epoch: 204, train_obj: 0.21571321640329458\n",
            "--- Epoch: 205, train_obj: 0.21446944380536256\n",
            "--- Epoch: 206, train_obj: 0.21500664499492977\n",
            "--- Epoch: 207, train_obj: 0.2194751132878827\n",
            "--- Epoch: 208, train_obj: 0.21298379346247892\n",
            "--- Epoch: 209, train_obj: 0.21745955295837188\n",
            "--- Epoch: 210, train_obj: 0.21432096113649937\n",
            "--- Epoch: 211, train_obj: 0.211550648143973\n",
            "--- Epoch: 212, train_obj: 0.21431665572307038\n",
            "--- Epoch: 213, train_obj: 0.21206477136926818\n",
            "--- Epoch: 214, train_obj: 0.2084869398845689\n",
            "--- Epoch: 215, train_obj: 0.21032954484082295\n",
            "--- Epoch: 216, train_obj: 0.20879629083964488\n",
            "--- Epoch: 217, train_obj: 0.209044519810492\n",
            "--- Epoch: 218, train_obj: 0.2093629971299346\n",
            "--- Epoch: 219, train_obj: 0.2079015371427304\n",
            "--- Epoch: 220, train_obj: 0.20669642990873896\n",
            "--- Epoch: 221, train_obj: 0.20883877983844373\n",
            "--- Epoch: 222, train_obj: 0.2065824159856234\n",
            "--- Epoch: 223, train_obj: 0.20491677526145294\n",
            "--- Epoch: 224, train_obj: 0.20794987224209563\n",
            "--- Epoch: 225, train_obj: 0.20427427994144762\n",
            "--- Epoch: 226, train_obj: 0.20472473994068185\n",
            "--- Epoch: 227, train_obj: 0.2034761139020829\n",
            "--- Epoch: 228, train_obj: 0.2031816749911251\n",
            "--- Epoch: 229, train_obj: 0.20259081552963512\n",
            "--- Epoch: 230, train_obj: 0.2013061810211805\n",
            "--- Epoch: 231, train_obj: 0.20276497975806118\n",
            "--- Epoch: 232, train_obj: 0.20441363492981957\n",
            "--- Epoch: 233, train_obj: 0.19929457452768756\n",
            "--- Epoch: 234, train_obj: 0.2021467612406088\n",
            "--- Epoch: 235, train_obj: 0.20066743613416763\n",
            "--- Epoch: 236, train_obj: 0.2003756128629304\n",
            "--- Epoch: 237, train_obj: 0.1982462819374839\n",
            "--- Epoch: 238, train_obj: 0.19784447013542908\n",
            "--- Epoch: 239, train_obj: 0.1984662873487653\n",
            "--- Epoch: 240, train_obj: 0.19660722952987536\n",
            "--- Epoch: 241, train_obj: 0.19700287336754563\n",
            "--- Epoch: 242, train_obj: 0.19709922202125998\n",
            "--- Epoch: 243, train_obj: 0.19640363139035372\n",
            "--- Epoch: 244, train_obj: 0.19527930462661583\n",
            "--- Epoch: 245, train_obj: 0.1939706539499322\n",
            "--- Epoch: 246, train_obj: 0.19390757758207475\n",
            "--- Epoch: 247, train_obj: 0.19271158072491737\n",
            "--- Epoch: 248, train_obj: 0.19364230766634705\n",
            "--- Epoch: 249, train_obj: 0.19236442484574506\n",
            "--- Epoch: 250, train_obj: 0.19965979974620318\n",
            "--- Epoch: 251, train_obj: 0.19583460945681933\n",
            "--- Epoch: 252, train_obj: 0.191867445677882\n",
            "--- Epoch: 253, train_obj: 0.19083437658381408\n",
            "--- Epoch: 254, train_obj: 0.19003602945999676\n",
            "--- Epoch: 255, train_obj: 0.19137557995803028\n",
            "--- Epoch: 256, train_obj: 0.19348173722303413\n",
            "--- Epoch: 257, train_obj: 0.19039538385967117\n",
            "--- Epoch: 258, train_obj: 0.18881496313270216\n",
            "--- Epoch: 259, train_obj: 0.18639716591588648\n",
            "--- Epoch: 260, train_obj: 0.19060438085283682\n",
            "--- Epoch: 261, train_obj: 0.19058627938037997\n",
            "--- Epoch: 262, train_obj: 0.18861199028732342\n",
            "--- Epoch: 263, train_obj: 0.18729793628838537\n",
            "--- Epoch: 264, train_obj: 0.18685645519417202\n",
            "--- Epoch: 265, train_obj: 0.18735518430788273\n",
            "--- Epoch: 266, train_obj: 0.18585257503846514\n",
            "--- Epoch: 267, train_obj: 0.18591402622679803\n",
            "--- Epoch: 268, train_obj: 0.1859091456969252\n",
            "--- Epoch: 269, train_obj: 0.18459998379401732\n",
            "--- Epoch: 270, train_obj: 0.1828976934784677\n",
            "--- Epoch: 271, train_obj: 0.18443646800201743\n",
            "--- Epoch: 272, train_obj: 0.18617566109824762\n",
            "--- Epoch: 273, train_obj: 0.18131396068922384\n",
            "--- Epoch: 274, train_obj: 0.18231141154294586\n",
            "--- Epoch: 275, train_obj: 0.1828759345128734\n",
            "--- Epoch: 276, train_obj: 0.18467609022634676\n",
            "--- Epoch: 277, train_obj: 0.18168150216092074\n",
            "--- Epoch: 278, train_obj: 0.17965355593023938\n",
            "--- Epoch: 279, train_obj: 0.17936743146900566\n",
            "--- Epoch: 280, train_obj: 0.18019371575334758\n",
            "--- Epoch: 281, train_obj: 0.1807507964337027\n",
            "--- Epoch: 282, train_obj: 0.1824427015445806\n",
            "--- Epoch: 283, train_obj: 0.1776899527564483\n",
            "--- Epoch: 284, train_obj: 0.17797794557306906\n",
            "--- Epoch: 285, train_obj: 0.17742188282997085\n",
            "--- Epoch: 286, train_obj: 0.17621115490879916\n",
            "--- Epoch: 287, train_obj: 0.1775295501400867\n",
            "--- Epoch: 288, train_obj: 0.17571436641197502\n",
            "--- Epoch: 289, train_obj: 0.17633993846442633\n",
            "--- Epoch: 290, train_obj: 0.17731951277699584\n",
            "--- Epoch: 291, train_obj: 0.17512007498580678\n",
            "--- Epoch: 292, train_obj: 0.17573872498949605\n",
            "--- Epoch: 293, train_obj: 0.17593978640009653\n",
            "--- Epoch: 294, train_obj: 0.17622969015107265\n",
            "--- Epoch: 295, train_obj: 0.1742365589437044\n",
            "--- Epoch: 296, train_obj: 0.1726473035210422\n",
            "--- Epoch: 297, train_obj: 0.17162959682066858\n",
            "--- Epoch: 298, train_obj: 0.17082208446854907\n",
            "--- Epoch: 299, train_obj: 0.17133151988673542\n",
            "--- Epoch: 300, train_obj: 0.1716814074889301\n",
            "--- Epoch: 301, train_obj: 0.17028683985774518\n",
            "--- Epoch: 302, train_obj: 0.17266990694567577\n",
            "--- Epoch: 303, train_obj: 0.17407516410203527\n",
            "--- Epoch: 304, train_obj: 0.17120535983425725\n",
            "--- Epoch: 305, train_obj: 0.16796262013496258\n",
            "--- Epoch: 306, train_obj: 0.16984163372820543\n",
            "--- Epoch: 307, train_obj: 0.16986140648812834\n",
            "--- Epoch: 308, train_obj: 0.1675903452812671\n",
            "--- Epoch: 309, train_obj: 0.16735231422299368\n",
            "--- Epoch: 310, train_obj: 0.1689812255835125\n",
            "--- Epoch: 311, train_obj: 0.16965355687452185\n",
            "--- Epoch: 312, train_obj: 0.16667346384148035\n",
            "--- Epoch: 313, train_obj: 0.16494696472819115\n",
            "--- Epoch: 314, train_obj: 0.1657753454349316\n",
            "--- Epoch: 315, train_obj: 0.1655506009431892\n",
            "--- Epoch: 316, train_obj: 0.16642499354110785\n",
            "--- Epoch: 317, train_obj: 0.16521494702018646\n",
            "--- Epoch: 318, train_obj: 0.1647277084168821\n",
            "--- Epoch: 319, train_obj: 0.16566383136137916\n",
            "--- Epoch: 320, train_obj: 0.16411745109344797\n",
            "--- Epoch: 321, train_obj: 0.16372770703762007\n",
            "--- Epoch: 322, train_obj: 0.16502024458302023\n",
            "--- Epoch: 323, train_obj: 0.16293869706384623\n",
            "--- Epoch: 324, train_obj: 0.1627186120974959\n",
            "--- Epoch: 325, train_obj: 0.16425128949876264\n",
            "--- Epoch: 326, train_obj: 0.16050390324828998\n",
            "--- Epoch: 327, train_obj: 0.1607294597412552\n",
            "--- Epoch: 328, train_obj: 0.16082735366090856\n",
            "--- Epoch: 329, train_obj: 0.16071834259428197\n",
            "--- Epoch: 330, train_obj: 0.1594172595021482\n",
            "--- Epoch: 331, train_obj: 0.16318007116751387\n",
            "--- Epoch: 332, train_obj: 0.16538437492836597\n",
            "--- Epoch: 333, train_obj: 0.1622688978887989\n",
            "--- Epoch: 334, train_obj: 0.15722426733739997\n",
            "--- Epoch: 335, train_obj: 0.15878729726879784\n",
            "--- Epoch: 336, train_obj: 0.15798559548606414\n",
            "--- Epoch: 337, train_obj: 0.15656242361940126\n",
            "--- Epoch: 338, train_obj: 0.16035681291166656\n",
            "--- Epoch: 339, train_obj: 0.15983876129597144\n",
            "--- Epoch: 340, train_obj: 0.15889147756101063\n",
            "--- Epoch: 341, train_obj: 0.15683093576906112\n",
            "--- Epoch: 342, train_obj: 0.15439887018478632\n",
            "--- Epoch: 343, train_obj: 0.15444109009949322\n",
            "--- Epoch: 344, train_obj: 0.1536131238544896\n",
            "--- Epoch: 345, train_obj: 0.15494395090375546\n",
            "--- Epoch: 346, train_obj: 0.1546054660435357\n",
            "--- Epoch: 347, train_obj: 0.1532676159121302\n",
            "--- Epoch: 348, train_obj: 0.15323924934679514\n",
            "--- Epoch: 349, train_obj: 0.15246613163912257\n",
            "--- Epoch: 350, train_obj: 0.15286240844595977\n",
            "--- Epoch: 351, train_obj: 0.15198992406406686\n",
            "--- Epoch: 352, train_obj: 0.1526360820805345\n",
            "--- Epoch: 353, train_obj: 0.1516111439051714\n",
            "--- Epoch: 354, train_obj: 0.1516877282601018\n",
            "--- Epoch: 355, train_obj: 0.15357540713830328\n",
            "--- Epoch: 356, train_obj: 0.15154777197654387\n",
            "--- Epoch: 357, train_obj: 0.15242474241260223\n",
            "--- Epoch: 358, train_obj: 0.15046607727274264\n",
            "--- Epoch: 359, train_obj: 0.15195999202727406\n",
            "--- Epoch: 360, train_obj: 0.1500231524575383\n",
            "--- Epoch: 361, train_obj: 0.1486326782881668\n",
            "--- Epoch: 362, train_obj: 0.14977259016071928\n",
            "--- Epoch: 363, train_obj: 0.14946920552778403\n",
            "--- Epoch: 364, train_obj: 0.15051246773170102\n",
            "--- Epoch: 365, train_obj: 0.14733588337552547\n",
            "--- Epoch: 366, train_obj: 0.14880992926347467\n",
            "--- Epoch: 367, train_obj: 0.14997555648172864\n",
            "--- Epoch: 368, train_obj: 0.14828115136824419\n",
            "--- Epoch: 369, train_obj: 0.146732943713561\n",
            "--- Epoch: 370, train_obj: 0.1456526508464519\n",
            "--- Epoch: 371, train_obj: 0.1467065939657066\n",
            "--- Epoch: 372, train_obj: 0.1460534259749562\n",
            "--- Epoch: 373, train_obj: 0.14980685777577765\n",
            "--- Epoch: 374, train_obj: 0.146635529527065\n",
            "--- Epoch: 375, train_obj: 0.14419277715857795\n",
            "--- Epoch: 376, train_obj: 0.14431352357985044\n",
            "--- Epoch: 377, train_obj: 0.14361972255701932\n",
            "--- Epoch: 378, train_obj: 0.14897598217823524\n",
            "--- Epoch: 379, train_obj: 0.14301420337240853\n",
            "--- Epoch: 380, train_obj: 0.14355409721220666\n",
            "--- Epoch: 381, train_obj: 0.1441011127988846\n",
            "--- Epoch: 382, train_obj: 0.1430137713927827\n",
            "--- Epoch: 383, train_obj: 0.14467133662530096\n",
            "--- Epoch: 384, train_obj: 0.14118217768118427\n",
            "--- Epoch: 385, train_obj: 0.14354238736202835\n",
            "--- Epoch: 386, train_obj: 0.14165564003838618\n",
            "--- Epoch: 387, train_obj: 0.14284989278790966\n",
            "--- Epoch: 388, train_obj: 0.13971655256465515\n",
            "--- Epoch: 389, train_obj: 0.13956935975902945\n",
            "--- Epoch: 390, train_obj: 0.14209457388908078\n",
            "--- Epoch: 391, train_obj: 0.1427971209721097\n",
            "--- Epoch: 392, train_obj: 0.1398298315550493\n",
            "--- Epoch: 393, train_obj: 0.14115131874397804\n",
            "--- Epoch: 394, train_obj: 0.1403662843142391\n",
            "--- Epoch: 395, train_obj: 0.13859321174077222\n",
            "--- Epoch: 396, train_obj: 0.13892903081307273\n",
            "--- Epoch: 397, train_obj: 0.13617934715722876\n",
            "--- Epoch: 398, train_obj: 0.13621451719984967\n",
            "--- Epoch: 399, train_obj: 0.1358691208035183\n",
            "--- Epoch: 400, train_obj: 0.13762617664844454\n",
            "--- Epoch: 401, train_obj: 0.1367753943090623\n",
            "--- Epoch: 402, train_obj: 0.1370560302419345\n",
            "--- Epoch: 403, train_obj: 0.13568720074048385\n",
            "--- Epoch: 404, train_obj: 0.1351615254680335\n",
            "--- Epoch: 405, train_obj: 0.1365197570022723\n",
            "--- Epoch: 406, train_obj: 0.13485153119088347\n",
            "--- Epoch: 407, train_obj: 0.13937233177710634\n",
            "--- Epoch: 408, train_obj: 0.13598845378515945\n",
            "--- Epoch: 409, train_obj: 0.13304147199207325\n",
            "--- Epoch: 410, train_obj: 0.13264976711275026\n",
            "--- Epoch: 411, train_obj: 0.1325003292174343\n",
            "--- Epoch: 412, train_obj: 0.13549207707690084\n",
            "--- Epoch: 413, train_obj: 0.1332029781064977\n",
            "--- Epoch: 414, train_obj: 0.1321843314181249\n",
            "--- Epoch: 415, train_obj: 0.13147903799945396\n",
            "--- Epoch: 416, train_obj: 0.13413749563547442\n",
            "--- Epoch: 417, train_obj: 0.13061950103634157\n",
            "--- Epoch: 418, train_obj: 0.13250279804204612\n",
            "--- Epoch: 419, train_obj: 0.1300757133515287\n",
            "--- Epoch: 420, train_obj: 0.1309192982017951\n",
            "--- Epoch: 421, train_obj: 0.13036522917349072\n",
            "--- Epoch: 422, train_obj: 0.12953336655681658\n",
            "--- Epoch: 423, train_obj: 0.13123077812968142\n",
            "--- Epoch: 424, train_obj: 0.13044658338563284\n",
            "--- Epoch: 425, train_obj: 0.1324835693778012\n",
            "--- Epoch: 426, train_obj: 0.12968013545155885\n",
            "--- Epoch: 427, train_obj: 0.12832558629571578\n",
            "--- Epoch: 428, train_obj: 0.12797988561132806\n",
            "--- Epoch: 429, train_obj: 0.12850554291714839\n",
            "--- Epoch: 430, train_obj: 0.13257530529862044\n",
            "--- Epoch: 431, train_obj: 0.12824423323134224\n",
            "--- Epoch: 432, train_obj: 0.12845000341687957\n",
            "--- Epoch: 433, train_obj: 0.12682994719833707\n",
            "--- Epoch: 434, train_obj: 0.12785030860569216\n",
            "--- Epoch: 435, train_obj: 0.1294992677662146\n",
            "--- Epoch: 436, train_obj: 0.1281141534214986\n",
            "--- Epoch: 437, train_obj: 0.12704377639525483\n",
            "--- Epoch: 438, train_obj: 0.12676293497250338\n",
            "--- Epoch: 439, train_obj: 0.12519227219216456\n",
            "--- Epoch: 440, train_obj: 0.1254714698550583\n",
            "--- Epoch: 441, train_obj: 0.12666351820696445\n",
            "--- Epoch: 442, train_obj: 0.12418310429781709\n",
            "--- Epoch: 443, train_obj: 0.12898099966025017\n",
            "--- Epoch: 444, train_obj: 0.12417802249824578\n",
            "--- Epoch: 445, train_obj: 0.12504360967644754\n",
            "--- Epoch: 446, train_obj: 0.1249317447059456\n",
            "--- Epoch: 447, train_obj: 0.12353562025114509\n",
            "--- Epoch: 448, train_obj: 0.1246189274455683\n",
            "--- Epoch: 449, train_obj: 0.12318118290483748\n",
            "--- Epoch: 450, train_obj: 0.12266127429669965\n",
            "--- Epoch: 451, train_obj: 0.12145305511039582\n",
            "--- Epoch: 452, train_obj: 0.12520937911549454\n",
            "--- Epoch: 453, train_obj: 0.1219654871898269\n",
            "--- Epoch: 454, train_obj: 0.12053324326618148\n",
            "--- Epoch: 455, train_obj: 0.12212474067334017\n",
            "--- Epoch: 456, train_obj: 0.12095314571682976\n",
            "--- Epoch: 457, train_obj: 0.12032188673393557\n",
            "--- Epoch: 458, train_obj: 0.12024657821211608\n",
            "--- Epoch: 459, train_obj: 0.12131316194241203\n",
            "--- Epoch: 460, train_obj: 0.12055215635383203\n",
            "--- Epoch: 461, train_obj: 0.12339006325169805\n",
            "--- Epoch: 462, train_obj: 0.11999810630114974\n",
            "--- Epoch: 463, train_obj: 0.11920631943996768\n",
            "--- Epoch: 464, train_obj: 0.11897137593663927\n",
            "--- Epoch: 465, train_obj: 0.12069718331927856\n",
            "--- Epoch: 466, train_obj: 0.1183848673865452\n",
            "--- Epoch: 467, train_obj: 0.11948614911377245\n",
            "--- Epoch: 468, train_obj: 0.11790209327398203\n",
            "--- Epoch: 469, train_obj: 0.11678627508698607\n",
            "--- Epoch: 470, train_obj: 0.11576628838986903\n",
            "--- Epoch: 471, train_obj: 0.12164650508042964\n",
            "--- Epoch: 472, train_obj: 0.11647387521027257\n",
            "--- Epoch: 473, train_obj: 0.11714563865550293\n",
            "--- Epoch: 474, train_obj: 0.11674496155685272\n",
            "--- Epoch: 475, train_obj: 0.1157077764834516\n",
            "--- Epoch: 476, train_obj: 0.12174234735815444\n",
            "--- Epoch: 477, train_obj: 0.11779665747601072\n",
            "--- Epoch: 478, train_obj: 0.11590799907809168\n",
            "--- Epoch: 479, train_obj: 0.1155711151028254\n",
            "--- Epoch: 480, train_obj: 0.11719942476643475\n",
            "--- Epoch: 481, train_obj: 0.11695672398455823\n",
            "--- Epoch: 482, train_obj: 0.11790810725025691\n",
            "--- Epoch: 483, train_obj: 0.11413367794021091\n",
            "--- Epoch: 484, train_obj: 0.11726699572142023\n",
            "--- Epoch: 485, train_obj: 0.11769224295382978\n",
            "--- Epoch: 486, train_obj: 0.1146764553754678\n",
            "--- Epoch: 487, train_obj: 0.11652939184199439\n",
            "--- Epoch: 488, train_obj: 0.11433547761649898\n",
            "--- Epoch: 489, train_obj: 0.11152298818874631\n",
            "--- Epoch: 490, train_obj: 0.1135615377746819\n",
            "--- Epoch: 491, train_obj: 0.11372712101303221\n",
            "--- Epoch: 492, train_obj: 0.11409302855051578\n",
            "--- Epoch: 493, train_obj: 0.11207978401088167\n",
            "--- Epoch: 494, train_obj: 0.11198046468182264\n",
            "--- Epoch: 495, train_obj: 0.1114246071126694\n",
            "--- Epoch: 496, train_obj: 0.1100596269444237\n",
            "--- Epoch: 497, train_obj: 0.11109283059480952\n",
            "--- Epoch: 498, train_obj: 0.10984058511125758\n",
            "--- Epoch: 499, train_obj: 0.11018111661951069\n",
            "--- Epoch: 500, train_obj: 0.11034319854967\n",
            "--- Epoch: 501, train_obj: 0.11099871257593327\n",
            "--- Epoch: 502, train_obj: 0.10974994620965038\n",
            "--- Epoch: 503, train_obj: 0.10949865720908815\n",
            "--- Epoch: 504, train_obj: 0.11196210630876567\n",
            "--- Epoch: 505, train_obj: 0.10924335526388107\n",
            "--- Epoch: 506, train_obj: 0.11015447422087205\n",
            "--- Epoch: 507, train_obj: 0.109029970451155\n",
            "--- Epoch: 508, train_obj: 0.10868810078355431\n",
            "--- Epoch: 509, train_obj: 0.10808420281758983\n",
            "--- Epoch: 510, train_obj: 0.10865302923443097\n",
            "--- Epoch: 511, train_obj: 0.10724274172564256\n",
            "--- Epoch: 512, train_obj: 0.10882851906976584\n",
            "--- Epoch: 513, train_obj: 0.1073295274227928\n",
            "--- Epoch: 514, train_obj: 0.10591658411318917\n",
            "--- Epoch: 515, train_obj: 0.10800915495143738\n",
            "--- Epoch: 516, train_obj: 0.10834733196106462\n",
            "--- Epoch: 517, train_obj: 0.10595473672153226\n",
            "--- Epoch: 518, train_obj: 0.10586087268292961\n",
            "--- Epoch: 519, train_obj: 0.10611154963439529\n",
            "--- Epoch: 520, train_obj: 0.10929491225715429\n",
            "--- Epoch: 521, train_obj: 0.10573662718680528\n",
            "--- Epoch: 522, train_obj: 0.10584814227831002\n",
            "--- Epoch: 523, train_obj: 0.10579992885787029\n",
            "--- Epoch: 524, train_obj: 0.10491879636223503\n",
            "--- Epoch: 525, train_obj: 0.10575166183752341\n",
            "--- Epoch: 526, train_obj: 0.10500965693131176\n",
            "--- Epoch: 527, train_obj: 0.10983842530679379\n",
            "--- Epoch: 528, train_obj: 0.10410153681287547\n",
            "--- Epoch: 529, train_obj: 0.10783583213491882\n",
            "--- Epoch: 530, train_obj: 0.10417165859236524\n",
            "--- Epoch: 531, train_obj: 0.10409457882535078\n",
            "--- Epoch: 532, train_obj: 0.10446899234482804\n",
            "--- Epoch: 533, train_obj: 0.10548396057528636\n",
            "--- Epoch: 534, train_obj: 0.10169755837485059\n",
            "--- Epoch: 535, train_obj: 0.10946033857934098\n",
            "--- Epoch: 536, train_obj: 0.10589859643844496\n",
            "--- Epoch: 537, train_obj: 0.10201197495869832\n",
            "--- Epoch: 538, train_obj: 0.10040053504455686\n",
            "--- Epoch: 539, train_obj: 0.10180189858439963\n",
            "--- Epoch: 540, train_obj: 0.10127957487471884\n",
            "--- Epoch: 541, train_obj: 0.10160303033620015\n",
            "--- Epoch: 542, train_obj: 0.1020402462349941\n",
            "--- Epoch: 543, train_obj: 0.10178166409389816\n",
            "--- Epoch: 544, train_obj: 0.10105544425832753\n",
            "--- Epoch: 545, train_obj: 0.10014795560664654\n",
            "--- Epoch: 546, train_obj: 0.10130251771416035\n",
            "--- Epoch: 547, train_obj: 0.10233512534996551\n",
            "--- Epoch: 548, train_obj: 0.10048162035993345\n",
            "--- Epoch: 549, train_obj: 0.10154679021223136\n",
            "--- Epoch: 550, train_obj: 0.10290391551340056\n",
            "--- Epoch: 551, train_obj: 0.09935929777004252\n",
            "--- Epoch: 552, train_obj: 0.1013278172502154\n",
            "--- Epoch: 553, train_obj: 0.09913527311213394\n",
            "--- Epoch: 554, train_obj: 0.09955732573361388\n",
            "--- Epoch: 555, train_obj: 0.101785854542778\n",
            "--- Epoch: 556, train_obj: 0.09764885100492839\n",
            "--- Epoch: 557, train_obj: 0.10067675714849181\n",
            "--- Epoch: 558, train_obj: 0.09837853026976821\n",
            "--- Epoch: 559, train_obj: 0.09685552853403354\n",
            "--- Epoch: 560, train_obj: 0.09838239456213395\n",
            "--- Epoch: 561, train_obj: 0.09625395442960193\n",
            "--- Epoch: 562, train_obj: 0.10073658205829032\n",
            "--- Epoch: 563, train_obj: 0.09930345797802764\n",
            "--- Epoch: 564, train_obj: 0.09729109474247492\n",
            "--- Epoch: 565, train_obj: 0.09731098160695573\n",
            "--- Epoch: 566, train_obj: 0.09804709992621329\n",
            "--- Epoch: 567, train_obj: 0.09985501776196111\n",
            "--- Epoch: 568, train_obj: 0.09571315618752807\n",
            "--- Epoch: 569, train_obj: 0.09569030794321998\n",
            "--- Epoch: 570, train_obj: 0.09511944017939643\n",
            "--- Epoch: 571, train_obj: 0.09627682821971462\n",
            "--- Epoch: 572, train_obj: 0.09460891639128838\n",
            "--- Epoch: 573, train_obj: 0.0944523790231646\n",
            "--- Epoch: 574, train_obj: 0.09486118428053808\n",
            "--- Epoch: 575, train_obj: 0.09741409678804318\n",
            "--- Epoch: 576, train_obj: 0.09405195896614398\n",
            "--- Epoch: 577, train_obj: 0.0962484791380959\n",
            "--- Epoch: 578, train_obj: 0.09451705369974449\n",
            "--- Epoch: 579, train_obj: 0.09677168584840094\n",
            "--- Epoch: 580, train_obj: 0.0930795653981768\n",
            "--- Epoch: 581, train_obj: 0.09410814700988773\n",
            "--- Epoch: 582, train_obj: 0.09663276037786067\n",
            "--- Epoch: 583, train_obj: 0.09569872162661187\n",
            "--- Epoch: 584, train_obj: 0.09286243979325097\n",
            "--- Epoch: 585, train_obj: 0.09490365974094585\n",
            "--- Epoch: 586, train_obj: 0.09259320538211631\n",
            "--- Epoch: 587, train_obj: 0.09535655718930572\n",
            "--- Epoch: 588, train_obj: 0.09248294183592928\n",
            "--- Epoch: 589, train_obj: 0.09333215413351965\n",
            "--- Epoch: 590, train_obj: 0.0905585092356576\n",
            "--- Epoch: 591, train_obj: 0.09565034553026601\n",
            "--- Epoch: 592, train_obj: 0.0922628632354764\n",
            "--- Epoch: 593, train_obj: 0.09217542798060258\n",
            "--- Epoch: 594, train_obj: 0.09024109516847557\n",
            "--- Epoch: 595, train_obj: 0.0906795922043561\n",
            "--- Epoch: 596, train_obj: 0.09150241481314844\n",
            "--- Epoch: 597, train_obj: 0.09216495893316248\n",
            "--- Epoch: 598, train_obj: 0.08975809686741826\n",
            "--- Epoch: 599, train_obj: 0.08917106922746044\n",
            "train_err: 0.016479, val_err: 0.100833\n",
            "\n",
            "--- Epoch: 0, train_obj: 0.665332728868833\n",
            "--- Epoch: 1, train_obj: 0.5736422366249571\n",
            "--- Epoch: 2, train_obj: 0.5275687779637627\n",
            "--- Epoch: 3, train_obj: 0.5025546048160843\n",
            "--- Epoch: 4, train_obj: 0.4841687460152728\n",
            "--- Epoch: 5, train_obj: 0.4731789596247839\n",
            "--- Epoch: 6, train_obj: 0.45982280208101567\n",
            "--- Epoch: 7, train_obj: 0.44860820579422006\n",
            "--- Epoch: 8, train_obj: 0.4437811626149877\n",
            "--- Epoch: 9, train_obj: 0.4337112467937413\n",
            "--- Epoch: 10, train_obj: 0.4278382847763262\n",
            "--- Epoch: 11, train_obj: 0.42446058792665825\n",
            "--- Epoch: 12, train_obj: 0.41505303876832317\n",
            "--- Epoch: 13, train_obj: 0.4089543443660754\n",
            "--- Epoch: 14, train_obj: 0.41544627026756287\n",
            "--- Epoch: 15, train_obj: 0.3989989919496702\n",
            "--- Epoch: 16, train_obj: 0.3960035799323175\n",
            "--- Epoch: 17, train_obj: 0.39285358482145755\n",
            "--- Epoch: 18, train_obj: 0.3898731899193968\n",
            "--- Epoch: 19, train_obj: 0.38432761073486144\n",
            "--- Epoch: 20, train_obj: 0.3805302747179895\n",
            "--- Epoch: 21, train_obj: 0.3796540043515989\n",
            "--- Epoch: 22, train_obj: 0.37697894920307284\n",
            "--- Epoch: 23, train_obj: 0.37374285362061854\n",
            "--- Epoch: 24, train_obj: 0.3699523990423298\n",
            "--- Epoch: 25, train_obj: 0.3692275040515216\n",
            "--- Epoch: 26, train_obj: 0.36568955630640826\n",
            "--- Epoch: 27, train_obj: 0.36437993115966455\n",
            "--- Epoch: 28, train_obj: 0.3596727851187655\n",
            "--- Epoch: 29, train_obj: 0.3594218952926962\n",
            "--- Epoch: 30, train_obj: 0.3558209079745891\n",
            "--- Epoch: 31, train_obj: 0.35289002329904795\n",
            "--- Epoch: 32, train_obj: 0.3541785987669336\n",
            "--- Epoch: 33, train_obj: 0.3495387536360304\n",
            "--- Epoch: 34, train_obj: 0.347480123157856\n",
            "--- Epoch: 35, train_obj: 0.3481982264285663\n",
            "--- Epoch: 36, train_obj: 0.3424679300114134\n",
            "--- Epoch: 37, train_obj: 0.34278464590410046\n",
            "--- Epoch: 38, train_obj: 0.33815841213352216\n",
            "--- Epoch: 39, train_obj: 0.3371268401043347\n",
            "--- Epoch: 40, train_obj: 0.334570236209027\n",
            "--- Epoch: 41, train_obj: 0.3318827320918991\n",
            "--- Epoch: 42, train_obj: 0.3327947082348669\n",
            "--- Epoch: 43, train_obj: 0.32975342453251183\n",
            "--- Epoch: 44, train_obj: 0.3308506129579357\n",
            "--- Epoch: 45, train_obj: 0.3252904288931366\n",
            "--- Epoch: 46, train_obj: 0.32733995732513627\n",
            "--- Epoch: 47, train_obj: 0.32451015209900697\n",
            "--- Epoch: 48, train_obj: 0.3243980283001426\n",
            "--- Epoch: 49, train_obj: 0.31877850755328563\n",
            "--- Epoch: 50, train_obj: 0.31991327156813965\n",
            "--- Epoch: 51, train_obj: 0.3161778079222726\n",
            "--- Epoch: 52, train_obj: 0.31469858124786726\n",
            "--- Epoch: 53, train_obj: 0.31403230479427147\n",
            "--- Epoch: 54, train_obj: 0.31309430869906263\n",
            "--- Epoch: 55, train_obj: 0.3093954411445823\n",
            "--- Epoch: 56, train_obj: 0.31139573679282456\n",
            "--- Epoch: 57, train_obj: 0.3099164247558851\n",
            "--- Epoch: 58, train_obj: 0.30791880366230134\n",
            "--- Epoch: 59, train_obj: 0.3047177166394375\n",
            "--- Epoch: 60, train_obj: 0.30273453291128677\n",
            "--- Epoch: 61, train_obj: 0.3018526161217617\n",
            "--- Epoch: 62, train_obj: 0.30114517703736354\n",
            "--- Epoch: 63, train_obj: 0.3025333807498146\n",
            "--- Epoch: 64, train_obj: 0.30058384782974545\n",
            "--- Epoch: 65, train_obj: 0.29965408590714915\n",
            "--- Epoch: 66, train_obj: 0.29886363523782766\n",
            "--- Epoch: 67, train_obj: 0.296250730123976\n",
            "--- Epoch: 68, train_obj: 0.29294654098174855\n",
            "--- Epoch: 69, train_obj: 0.2920190359982723\n",
            "--- Epoch: 70, train_obj: 0.29003667640772013\n",
            "--- Epoch: 71, train_obj: 0.291344194856244\n",
            "--- Epoch: 72, train_obj: 0.2902572270236999\n",
            "--- Epoch: 73, train_obj: 0.2886495322052939\n",
            "--- Epoch: 74, train_obj: 0.28669213413104006\n",
            "--- Epoch: 75, train_obj: 0.28569217480170117\n",
            "--- Epoch: 76, train_obj: 0.28700729993166013\n",
            "--- Epoch: 77, train_obj: 0.28381309926365805\n",
            "--- Epoch: 78, train_obj: 0.28227536597364583\n",
            "--- Epoch: 79, train_obj: 0.2799514738346998\n",
            "--- Epoch: 80, train_obj: 0.2800043080352794\n",
            "--- Epoch: 81, train_obj: 0.2816234720065548\n",
            "--- Epoch: 82, train_obj: 0.2759266700874646\n",
            "--- Epoch: 83, train_obj: 0.27485685647010155\n",
            "--- Epoch: 84, train_obj: 0.2763896754086238\n",
            "--- Epoch: 85, train_obj: 0.27349592915741283\n",
            "--- Epoch: 86, train_obj: 0.27255011659929534\n",
            "--- Epoch: 87, train_obj: 0.272708585543571\n",
            "--- Epoch: 88, train_obj: 0.2709150467033513\n",
            "--- Epoch: 89, train_obj: 0.26945526992842916\n",
            "--- Epoch: 90, train_obj: 0.26754228277049247\n",
            "--- Epoch: 91, train_obj: 0.26867945552517125\n",
            "--- Epoch: 92, train_obj: 0.2659423410155295\n",
            "--- Epoch: 93, train_obj: 0.26424716325403974\n",
            "--- Epoch: 94, train_obj: 0.26447867464320884\n",
            "--- Epoch: 95, train_obj: 0.26414660245402344\n",
            "--- Epoch: 96, train_obj: 0.26448575997001705\n",
            "--- Epoch: 97, train_obj: 0.26150091032349515\n",
            "--- Epoch: 98, train_obj: 0.26169755144315493\n",
            "--- Epoch: 99, train_obj: 0.26019680757972935\n",
            "--- Epoch: 100, train_obj: 0.25862914531284864\n",
            "--- Epoch: 101, train_obj: 0.25696274061479346\n",
            "--- Epoch: 102, train_obj: 0.2569965736769427\n",
            "--- Epoch: 103, train_obj: 0.25886653770732343\n",
            "--- Epoch: 104, train_obj: 0.2608106355920867\n",
            "--- Epoch: 105, train_obj: 0.25592720066431\n",
            "--- Epoch: 106, train_obj: 0.25274930873779033\n",
            "--- Epoch: 107, train_obj: 0.2528615783111189\n",
            "--- Epoch: 108, train_obj: 0.25470050867738225\n",
            "--- Epoch: 109, train_obj: 0.2504888709061664\n",
            "--- Epoch: 110, train_obj: 0.2491342402897365\n",
            "--- Epoch: 111, train_obj: 0.25017177582703015\n",
            "--- Epoch: 112, train_obj: 0.2481349999677983\n",
            "--- Epoch: 113, train_obj: 0.24714687836682306\n",
            "--- Epoch: 114, train_obj: 0.24410871522264552\n",
            "--- Epoch: 115, train_obj: 0.24682004257980983\n",
            "--- Epoch: 116, train_obj: 0.24483585739942076\n",
            "--- Epoch: 117, train_obj: 0.24246522416116348\n",
            "--- Epoch: 118, train_obj: 0.2429057964453599\n",
            "--- Epoch: 119, train_obj: 0.24183437612286782\n",
            "--- Epoch: 120, train_obj: 0.24302088399300284\n",
            "--- Epoch: 121, train_obj: 0.24229639245934095\n",
            "--- Epoch: 122, train_obj: 0.23800949071769203\n",
            "--- Epoch: 123, train_obj: 0.2405264057407679\n",
            "--- Epoch: 124, train_obj: 0.2394555778765657\n",
            "--- Epoch: 125, train_obj: 0.23765038789795798\n",
            "--- Epoch: 126, train_obj: 0.2374424387429122\n",
            "--- Epoch: 127, train_obj: 0.237347462723533\n",
            "--- Epoch: 128, train_obj: 0.23348822210016695\n",
            "--- Epoch: 129, train_obj: 0.23573097485177213\n",
            "--- Epoch: 130, train_obj: 0.23347014769725652\n",
            "--- Epoch: 131, train_obj: 0.23480053612990534\n",
            "--- Epoch: 132, train_obj: 0.23144464146445073\n",
            "--- Epoch: 133, train_obj: 0.23079353371605604\n",
            "--- Epoch: 134, train_obj: 0.23011107237377268\n",
            "--- Epoch: 135, train_obj: 0.2292132895868125\n",
            "--- Epoch: 136, train_obj: 0.2266553792390445\n",
            "--- Epoch: 137, train_obj: 0.2271172241511064\n",
            "--- Epoch: 138, train_obj: 0.22562540512894347\n",
            "--- Epoch: 139, train_obj: 0.22811929717909152\n",
            "--- Epoch: 140, train_obj: 0.224247359855454\n",
            "--- Epoch: 141, train_obj: 0.22347419561609075\n",
            "--- Epoch: 142, train_obj: 0.22277684730471378\n",
            "--- Epoch: 143, train_obj: 0.22223446480357176\n",
            "--- Epoch: 144, train_obj: 0.22373601789259784\n",
            "--- Epoch: 145, train_obj: 0.2212369713981559\n",
            "--- Epoch: 146, train_obj: 0.21971958082918344\n",
            "--- Epoch: 147, train_obj: 0.22072909427478282\n",
            "--- Epoch: 148, train_obj: 0.2188200686149669\n",
            "--- Epoch: 149, train_obj: 0.21883193558309016\n",
            "--- Epoch: 150, train_obj: 0.2174538033612187\n",
            "--- Epoch: 151, train_obj: 0.2162671242541823\n",
            "--- Epoch: 152, train_obj: 0.21560114414867826\n",
            "--- Epoch: 153, train_obj: 0.21658126068227984\n",
            "--- Epoch: 154, train_obj: 0.2151579120634611\n",
            "--- Epoch: 155, train_obj: 0.2152492823991889\n",
            "--- Epoch: 156, train_obj: 0.21428698483626693\n",
            "--- Epoch: 157, train_obj: 0.21337969763347275\n",
            "--- Epoch: 158, train_obj: 0.2161078511824621\n",
            "--- Epoch: 159, train_obj: 0.21087694391629547\n",
            "--- Epoch: 160, train_obj: 0.21190772460928564\n",
            "--- Epoch: 161, train_obj: 0.20905761915159263\n",
            "--- Epoch: 162, train_obj: 0.2077525916291196\n",
            "--- Epoch: 163, train_obj: 0.20896711692613662\n",
            "--- Epoch: 164, train_obj: 0.2087104282571104\n",
            "--- Epoch: 165, train_obj: 0.20585341210983724\n",
            "--- Epoch: 166, train_obj: 0.20847960189928907\n",
            "--- Epoch: 167, train_obj: 0.2066149729062175\n",
            "--- Epoch: 168, train_obj: 0.2066117107232133\n",
            "--- Epoch: 169, train_obj: 0.2034596255510616\n",
            "--- Epoch: 170, train_obj: 0.2019203707385011\n",
            "--- Epoch: 171, train_obj: 0.20584327722036214\n",
            "--- Epoch: 172, train_obj: 0.2010981103341023\n",
            "--- Epoch: 173, train_obj: 0.20108547915384292\n",
            "--- Epoch: 174, train_obj: 0.200277629663265\n",
            "--- Epoch: 175, train_obj: 0.2003715686858661\n",
            "--- Epoch: 176, train_obj: 0.20037538425963367\n",
            "--- Epoch: 177, train_obj: 0.19894561045963177\n",
            "--- Epoch: 178, train_obj: 0.19859503051808666\n",
            "--- Epoch: 179, train_obj: 0.19954707443981182\n",
            "--- Epoch: 180, train_obj: 0.19502969306141482\n",
            "--- Epoch: 181, train_obj: 0.19687954636283453\n",
            "--- Epoch: 182, train_obj: 0.19348474157477583\n",
            "--- Epoch: 183, train_obj: 0.1975770446441091\n",
            "--- Epoch: 184, train_obj: 0.19368184138405659\n",
            "--- Epoch: 185, train_obj: 0.19541084246569052\n",
            "--- Epoch: 186, train_obj: 0.19134693646297324\n",
            "--- Epoch: 187, train_obj: 0.19184499591550905\n",
            "--- Epoch: 188, train_obj: 0.19043756758757158\n",
            "--- Epoch: 189, train_obj: 0.1924420127341463\n",
            "--- Epoch: 190, train_obj: 0.19017523002909262\n",
            "--- Epoch: 191, train_obj: 0.19032885540306585\n",
            "--- Epoch: 192, train_obj: 0.1925926775876304\n",
            "--- Epoch: 193, train_obj: 0.18936527429312816\n",
            "--- Epoch: 194, train_obj: 0.18707004971909738\n",
            "--- Epoch: 195, train_obj: 0.1885315044005395\n",
            "--- Epoch: 196, train_obj: 0.18719104148322713\n",
            "--- Epoch: 197, train_obj: 0.18633693633047868\n",
            "--- Epoch: 198, train_obj: 0.1867254864409057\n",
            "--- Epoch: 199, train_obj: 0.18477928852533698\n",
            "--- Epoch: 200, train_obj: 0.18571456482280094\n",
            "--- Epoch: 201, train_obj: 0.1820551907434361\n",
            "--- Epoch: 202, train_obj: 0.18224331752519815\n",
            "--- Epoch: 203, train_obj: 0.1836548265553465\n",
            "--- Epoch: 204, train_obj: 0.18144820534377845\n",
            "--- Epoch: 205, train_obj: 0.18211444892388318\n",
            "--- Epoch: 206, train_obj: 0.18395050308213634\n",
            "--- Epoch: 207, train_obj: 0.18228192436896662\n",
            "--- Epoch: 208, train_obj: 0.17958014389541488\n",
            "--- Epoch: 209, train_obj: 0.17884710939641965\n",
            "--- Epoch: 210, train_obj: 0.178926936312054\n",
            "--- Epoch: 211, train_obj: 0.17736991098352256\n",
            "--- Epoch: 212, train_obj: 0.1796678906489675\n",
            "--- Epoch: 213, train_obj: 0.17815341758297085\n",
            "--- Epoch: 214, train_obj: 0.17569773745258613\n",
            "--- Epoch: 215, train_obj: 0.1748491556330491\n",
            "--- Epoch: 216, train_obj: 0.17465001371011313\n",
            "--- Epoch: 217, train_obj: 0.1753771608699594\n",
            "--- Epoch: 218, train_obj: 0.17335294908448592\n",
            "--- Epoch: 219, train_obj: 0.17370557096117378\n",
            "--- Epoch: 220, train_obj: 0.17281856426079775\n",
            "--- Epoch: 221, train_obj: 0.17370254445479466\n",
            "--- Epoch: 222, train_obj: 0.17128590505814434\n",
            "--- Epoch: 223, train_obj: 0.18250291439797403\n",
            "--- Epoch: 224, train_obj: 0.17345279121988355\n",
            "--- Epoch: 225, train_obj: 0.17335566742026984\n",
            "--- Epoch: 226, train_obj: 0.1695758056711405\n",
            "--- Epoch: 227, train_obj: 0.1697294570141693\n",
            "--- Epoch: 228, train_obj: 0.1678174759949365\n",
            "--- Epoch: 229, train_obj: 0.16978048879526245\n",
            "--- Epoch: 230, train_obj: 0.16945164615576389\n",
            "--- Epoch: 231, train_obj: 0.16666793707213282\n",
            "--- Epoch: 232, train_obj: 0.16624980869184947\n",
            "--- Epoch: 233, train_obj: 0.16753774944205782\n",
            "--- Epoch: 234, train_obj: 0.16526483272134743\n",
            "--- Epoch: 235, train_obj: 0.16472630009122624\n",
            "--- Epoch: 236, train_obj: 0.166519812914396\n",
            "--- Epoch: 237, train_obj: 0.17062689297052036\n",
            "--- Epoch: 238, train_obj: 0.1632811853404921\n",
            "--- Epoch: 239, train_obj: 0.16218997486668943\n",
            "--- Epoch: 240, train_obj: 0.1618636807393379\n",
            "--- Epoch: 241, train_obj: 0.160365643909314\n",
            "--- Epoch: 242, train_obj: 0.16122862020825365\n",
            "--- Epoch: 243, train_obj: 0.1605444112125878\n",
            "--- Epoch: 244, train_obj: 0.16099448893450116\n",
            "--- Epoch: 245, train_obj: 0.16165951991632332\n",
            "--- Epoch: 246, train_obj: 0.15976188795999388\n",
            "--- Epoch: 247, train_obj: 0.157912894420916\n",
            "--- Epoch: 248, train_obj: 0.15797431076474694\n",
            "--- Epoch: 249, train_obj: 0.15731635629168034\n",
            "--- Epoch: 250, train_obj: 0.15622834685937387\n",
            "--- Epoch: 251, train_obj: 0.1557633319100451\n",
            "--- Epoch: 252, train_obj: 0.15700681620000148\n",
            "--- Epoch: 253, train_obj: 0.15846883328640887\n",
            "--- Epoch: 254, train_obj: 0.15523172463989354\n",
            "--- Epoch: 255, train_obj: 0.16146315657152385\n",
            "--- Epoch: 256, train_obj: 0.15397755866968107\n",
            "--- Epoch: 257, train_obj: 0.1518427584314852\n",
            "--- Epoch: 258, train_obj: 0.15292833074093282\n",
            "--- Epoch: 259, train_obj: 0.15397392640833796\n",
            "--- Epoch: 260, train_obj: 0.15159074926768593\n",
            "--- Epoch: 261, train_obj: 0.15398371920953535\n",
            "--- Epoch: 262, train_obj: 0.14992816895923516\n",
            "--- Epoch: 263, train_obj: 0.14965079083929742\n",
            "--- Epoch: 264, train_obj: 0.1515768745572413\n",
            "--- Epoch: 265, train_obj: 0.1520219550582577\n",
            "--- Epoch: 266, train_obj: 0.14908196929918152\n",
            "--- Epoch: 267, train_obj: 0.14820690907091832\n",
            "--- Epoch: 268, train_obj: 0.14925401132258528\n",
            "--- Epoch: 269, train_obj: 0.15132679022121398\n",
            "--- Epoch: 270, train_obj: 0.1479798232920419\n",
            "--- Epoch: 271, train_obj: 0.14644785593609594\n",
            "--- Epoch: 272, train_obj: 0.14895302335844704\n",
            "--- Epoch: 273, train_obj: 0.14680291103696375\n",
            "--- Epoch: 274, train_obj: 0.14527216708750165\n",
            "--- Epoch: 275, train_obj: 0.14533693347334356\n",
            "--- Epoch: 276, train_obj: 0.14627992620917699\n",
            "--- Epoch: 277, train_obj: 0.14499233792815452\n",
            "--- Epoch: 278, train_obj: 0.14617181903341755\n",
            "--- Epoch: 279, train_obj: 0.14344649034771897\n",
            "--- Epoch: 280, train_obj: 0.1433379746089059\n",
            "--- Epoch: 281, train_obj: 0.1434613573186229\n",
            "--- Epoch: 282, train_obj: 0.142457420173674\n",
            "--- Epoch: 283, train_obj: 0.14279095390442237\n",
            "--- Epoch: 284, train_obj: 0.13997242479125852\n",
            "--- Epoch: 285, train_obj: 0.1447049876851825\n",
            "--- Epoch: 286, train_obj: 0.13994746450318168\n",
            "--- Epoch: 287, train_obj: 0.1385934943279288\n",
            "--- Epoch: 288, train_obj: 0.14355292876387107\n",
            "--- Epoch: 289, train_obj: 0.13770931058629182\n",
            "--- Epoch: 290, train_obj: 0.14082971381120735\n",
            "--- Epoch: 291, train_obj: 0.13637815040291737\n",
            "--- Epoch: 292, train_obj: 0.13824989702645557\n",
            "--- Epoch: 293, train_obj: 0.1382011446788444\n",
            "--- Epoch: 294, train_obj: 0.13752062556961686\n",
            "--- Epoch: 295, train_obj: 0.1383265440922379\n",
            "--- Epoch: 296, train_obj: 0.13615576779115746\n",
            "--- Epoch: 297, train_obj: 0.13421164947959766\n",
            "--- Epoch: 298, train_obj: 0.13922257175174949\n",
            "--- Epoch: 299, train_obj: 0.13775923330909187\n",
            "--- Epoch: 300, train_obj: 0.1353723586432256\n",
            "--- Epoch: 301, train_obj: 0.13322419049243148\n",
            "--- Epoch: 302, train_obj: 0.13319129070837216\n",
            "--- Epoch: 303, train_obj: 0.1325368232496448\n",
            "--- Epoch: 304, train_obj: 0.13402567256225104\n",
            "--- Epoch: 305, train_obj: 0.13252861448319872\n",
            "--- Epoch: 306, train_obj: 0.1311707341656507\n",
            "--- Epoch: 307, train_obj: 0.13104200661719773\n",
            "--- Epoch: 308, train_obj: 0.13113666625188755\n",
            "--- Epoch: 309, train_obj: 0.1291379205229743\n",
            "--- Epoch: 310, train_obj: 0.13034614239081826\n",
            "--- Epoch: 311, train_obj: 0.14018793576235755\n",
            "--- Epoch: 312, train_obj: 0.12979514285569319\n",
            "--- Epoch: 313, train_obj: 0.12905878791990752\n",
            "--- Epoch: 314, train_obj: 0.12819789153450167\n",
            "--- Epoch: 315, train_obj: 0.12853534858265334\n",
            "--- Epoch: 316, train_obj: 0.12892153944442505\n",
            "--- Epoch: 317, train_obj: 0.12846007992876196\n",
            "--- Epoch: 318, train_obj: 0.12534242364938045\n",
            "--- Epoch: 319, train_obj: 0.12661838542800205\n",
            "--- Epoch: 320, train_obj: 0.1239729749526765\n",
            "--- Epoch: 321, train_obj: 0.1262531293664907\n",
            "--- Epoch: 322, train_obj: 0.12580965073297729\n",
            "--- Epoch: 323, train_obj: 0.12440338690656984\n",
            "--- Epoch: 324, train_obj: 0.12630163455032645\n",
            "--- Epoch: 325, train_obj: 0.12568627362156043\n",
            "--- Epoch: 326, train_obj: 0.1250322753926505\n",
            "--- Epoch: 327, train_obj: 0.12410475874823748\n",
            "--- Epoch: 328, train_obj: 0.1232887832761615\n",
            "--- Epoch: 329, train_obj: 0.12251495978526863\n",
            "--- Epoch: 330, train_obj: 0.12334737424259887\n",
            "--- Epoch: 331, train_obj: 0.12350990746121804\n",
            "--- Epoch: 332, train_obj: 0.12327438886318491\n",
            "--- Epoch: 333, train_obj: 0.12423261622143372\n",
            "--- Epoch: 334, train_obj: 0.12341530378698612\n",
            "--- Epoch: 335, train_obj: 0.12186277646200627\n",
            "--- Epoch: 336, train_obj: 0.11921884513790275\n",
            "--- Epoch: 337, train_obj: 0.12177438552723331\n",
            "--- Epoch: 338, train_obj: 0.11920522957236494\n",
            "--- Epoch: 339, train_obj: 0.11829532492585622\n",
            "--- Epoch: 340, train_obj: 0.11863166540921376\n",
            "--- Epoch: 341, train_obj: 0.11918627364301691\n",
            "--- Epoch: 342, train_obj: 0.11939577103388062\n",
            "--- Epoch: 343, train_obj: 0.11674894717293495\n",
            "--- Epoch: 344, train_obj: 0.1197097013435301\n",
            "--- Epoch: 345, train_obj: 0.11707024232461187\n",
            "--- Epoch: 346, train_obj: 0.11927787192053947\n",
            "--- Epoch: 347, train_obj: 0.1170617305763814\n",
            "--- Epoch: 348, train_obj: 0.11398756779631188\n",
            "--- Epoch: 349, train_obj: 0.11620936104157156\n",
            "--- Epoch: 350, train_obj: 0.11629990691001045\n",
            "--- Epoch: 351, train_obj: 0.1170479260545923\n",
            "--- Epoch: 352, train_obj: 0.114941827593263\n",
            "--- Epoch: 353, train_obj: 0.11824593233101689\n",
            "--- Epoch: 354, train_obj: 0.11406472203984694\n",
            "--- Epoch: 355, train_obj: 0.11672856411625454\n",
            "--- Epoch: 356, train_obj: 0.11240733342699072\n",
            "--- Epoch: 357, train_obj: 0.11322592751649004\n",
            "--- Epoch: 358, train_obj: 0.11106795090821225\n",
            "--- Epoch: 359, train_obj: 0.1121118728782806\n",
            "--- Epoch: 360, train_obj: 0.11079208133818637\n",
            "--- Epoch: 361, train_obj: 0.11051918586081781\n",
            "--- Epoch: 362, train_obj: 0.11331544368252738\n",
            "--- Epoch: 363, train_obj: 0.11218583642269116\n",
            "--- Epoch: 364, train_obj: 0.11343060137952439\n",
            "--- Epoch: 365, train_obj: 0.11024408608542104\n",
            "--- Epoch: 366, train_obj: 0.10853856458658186\n",
            "--- Epoch: 367, train_obj: 0.10805956355909155\n",
            "--- Epoch: 368, train_obj: 0.10956545443299169\n",
            "--- Epoch: 369, train_obj: 0.10866323989400174\n",
            "--- Epoch: 370, train_obj: 0.10853784172061626\n",
            "--- Epoch: 371, train_obj: 0.11032954787576746\n",
            "--- Epoch: 372, train_obj: 0.10741366467836999\n",
            "--- Epoch: 373, train_obj: 0.10805199877134229\n",
            "--- Epoch: 374, train_obj: 0.10647690335753436\n",
            "--- Epoch: 375, train_obj: 0.10895038730844116\n",
            "--- Epoch: 376, train_obj: 0.1076730427807287\n",
            "--- Epoch: 377, train_obj: 0.10602057079705327\n",
            "--- Epoch: 378, train_obj: 0.10608461233859784\n",
            "--- Epoch: 379, train_obj: 0.10485156499219957\n",
            "--- Epoch: 380, train_obj: 0.10525947364726312\n",
            "--- Epoch: 381, train_obj: 0.10443634732464319\n",
            "--- Epoch: 382, train_obj: 0.10519656341474784\n",
            "--- Epoch: 383, train_obj: 0.1036667057698204\n",
            "--- Epoch: 384, train_obj: 0.10268239850816938\n",
            "--- Epoch: 385, train_obj: 0.1030145736376082\n",
            "--- Epoch: 386, train_obj: 0.10223026133069053\n",
            "--- Epoch: 387, train_obj: 0.10370723177587154\n",
            "--- Epoch: 388, train_obj: 0.10199439205971345\n",
            "--- Epoch: 389, train_obj: 0.10288255263158097\n",
            "--- Epoch: 390, train_obj: 0.10262530858548793\n",
            "--- Epoch: 391, train_obj: 0.1005367380235723\n",
            "--- Epoch: 392, train_obj: 0.1038464737772229\n",
            "--- Epoch: 393, train_obj: 0.10177424690240658\n",
            "--- Epoch: 394, train_obj: 0.09999578910047047\n",
            "--- Epoch: 395, train_obj: 0.10069224015892424\n",
            "--- Epoch: 396, train_obj: 0.10288252710689151\n",
            "--- Epoch: 397, train_obj: 0.09952521035454524\n",
            "--- Epoch: 398, train_obj: 0.09992667223867097\n",
            "--- Epoch: 399, train_obj: 0.09977860589294679\n",
            "--- Epoch: 400, train_obj: 0.09764991470994397\n",
            "--- Epoch: 401, train_obj: 0.0978386220494323\n",
            "--- Epoch: 402, train_obj: 0.10890170908803236\n",
            "--- Epoch: 403, train_obj: 0.09677061770245227\n",
            "--- Epoch: 404, train_obj: 0.09815668188278737\n",
            "--- Epoch: 405, train_obj: 0.09869569576480101\n",
            "--- Epoch: 406, train_obj: 0.09681221215221897\n",
            "--- Epoch: 407, train_obj: 0.09769899172137834\n",
            "--- Epoch: 408, train_obj: 0.09541998851167045\n",
            "--- Epoch: 409, train_obj: 0.095363658045376\n",
            "--- Epoch: 410, train_obj: 0.09483236150520566\n",
            "--- Epoch: 411, train_obj: 0.09474879519797234\n",
            "--- Epoch: 412, train_obj: 0.09658280314576634\n",
            "--- Epoch: 413, train_obj: 0.09616566916124634\n",
            "--- Epoch: 414, train_obj: 0.09576273698067916\n",
            "--- Epoch: 415, train_obj: 0.09525030429465217\n",
            "--- Epoch: 416, train_obj: 0.09474541028915053\n",
            "--- Epoch: 417, train_obj: 0.09351741455438857\n",
            "--- Epoch: 418, train_obj: 0.09995784806339619\n",
            "--- Epoch: 419, train_obj: 0.09285565385522186\n",
            "--- Epoch: 420, train_obj: 0.09302215494748911\n",
            "--- Epoch: 421, train_obj: 0.09166256404793158\n",
            "--- Epoch: 422, train_obj: 0.09122419268972176\n",
            "--- Epoch: 423, train_obj: 0.09320758716785353\n",
            "--- Epoch: 424, train_obj: 0.09549770201530962\n",
            "--- Epoch: 425, train_obj: 0.09115605050956803\n",
            "--- Epoch: 426, train_obj: 0.09493242497570094\n",
            "--- Epoch: 427, train_obj: 0.0910932758516937\n",
            "--- Epoch: 428, train_obj: 0.09334106422218014\n",
            "--- Epoch: 429, train_obj: 0.09200868353045981\n",
            "--- Epoch: 430, train_obj: 0.09013573596354295\n",
            "--- Epoch: 431, train_obj: 0.08954705012978477\n",
            "--- Epoch: 432, train_obj: 0.0898388302961835\n",
            "--- Epoch: 433, train_obj: 0.09011758287043321\n",
            "--- Epoch: 434, train_obj: 0.09012572473714263\n",
            "--- Epoch: 435, train_obj: 0.09480028006127718\n",
            "--- Epoch: 436, train_obj: 0.08937717598788864\n",
            "--- Epoch: 437, train_obj: 0.08958264187543828\n",
            "--- Epoch: 438, train_obj: 0.08773246323969407\n",
            "--- Epoch: 439, train_obj: 0.0904655547727509\n",
            "--- Epoch: 440, train_obj: 0.08765480143322509\n",
            "--- Epoch: 441, train_obj: 0.08994499930150598\n",
            "--- Epoch: 442, train_obj: 0.08803542665796268\n",
            "--- Epoch: 443, train_obj: 0.08647139143305098\n",
            "--- Epoch: 444, train_obj: 0.08844580858458469\n",
            "--- Epoch: 445, train_obj: 0.08550439206381888\n",
            "--- Epoch: 446, train_obj: 0.08670055350643306\n",
            "--- Epoch: 447, train_obj: 0.08396952319847566\n",
            "--- Epoch: 448, train_obj: 0.08499607866392099\n",
            "--- Epoch: 449, train_obj: 0.0862152017686098\n",
            "--- Epoch: 450, train_obj: 0.08632985769050752\n",
            "--- Epoch: 451, train_obj: 0.09098870885115695\n",
            "--- Epoch: 452, train_obj: 0.0838737702109022\n",
            "--- Epoch: 453, train_obj: 0.08379044642536669\n",
            "--- Epoch: 454, train_obj: 0.08373147216407915\n",
            "--- Epoch: 455, train_obj: 0.08467901983471902\n",
            "--- Epoch: 456, train_obj: 0.09395984855638258\n",
            "--- Epoch: 457, train_obj: 0.08253875267769228\n",
            "--- Epoch: 458, train_obj: 0.08236287958864984\n",
            "--- Epoch: 459, train_obj: 0.08168853977127845\n",
            "--- Epoch: 460, train_obj: 0.08604233334986536\n",
            "--- Epoch: 461, train_obj: 0.08622923390951079\n",
            "--- Epoch: 462, train_obj: 0.08306139628297275\n",
            "--- Epoch: 463, train_obj: 0.08162264660152171\n",
            "--- Epoch: 464, train_obj: 0.08365068117990372\n",
            "--- Epoch: 465, train_obj: 0.08114750465437019\n",
            "--- Epoch: 466, train_obj: 0.08071250802578439\n",
            "--- Epoch: 467, train_obj: 0.08091368884944453\n",
            "--- Epoch: 468, train_obj: 0.08085968842598178\n",
            "--- Epoch: 469, train_obj: 0.08272015095077219\n",
            "--- Epoch: 470, train_obj: 0.08064291715761501\n",
            "--- Epoch: 471, train_obj: 0.08273497272421886\n",
            "--- Epoch: 472, train_obj: 0.07983091760749784\n",
            "--- Epoch: 473, train_obj: 0.07907613201055706\n",
            "--- Epoch: 474, train_obj: 0.07842338067947187\n",
            "--- Epoch: 475, train_obj: 0.08148601567412425\n",
            "--- Epoch: 476, train_obj: 0.07825769047723408\n",
            "--- Epoch: 477, train_obj: 0.07940999051539739\n",
            "--- Epoch: 478, train_obj: 0.08476434777130816\n",
            "--- Epoch: 479, train_obj: 0.07871866491440231\n",
            "--- Epoch: 480, train_obj: 0.07780682640194232\n",
            "--- Epoch: 481, train_obj: 0.07728181017870372\n",
            "--- Epoch: 482, train_obj: 0.07844362775010176\n",
            "--- Epoch: 483, train_obj: 0.0767994351567466\n",
            "--- Epoch: 484, train_obj: 0.07691311193566697\n",
            "--- Epoch: 485, train_obj: 0.07800188719014552\n",
            "--- Epoch: 486, train_obj: 0.07616581480756399\n",
            "--- Epoch: 487, train_obj: 0.07764039602626598\n",
            "--- Epoch: 488, train_obj: 0.07562621865068632\n",
            "--- Epoch: 489, train_obj: 0.07751688606530306\n",
            "--- Epoch: 490, train_obj: 0.07762016827836139\n",
            "--- Epoch: 491, train_obj: 0.07679000963371273\n",
            "--- Epoch: 492, train_obj: 0.07498960725264528\n",
            "--- Epoch: 493, train_obj: 0.07535059128363508\n",
            "--- Epoch: 494, train_obj: 0.07636544323324411\n",
            "--- Epoch: 495, train_obj: 0.07712961707733625\n",
            "--- Epoch: 496, train_obj: 0.07467862578013369\n",
            "--- Epoch: 497, train_obj: 0.0747428720377119\n",
            "--- Epoch: 498, train_obj: 0.07409806178597668\n",
            "--- Epoch: 499, train_obj: 0.07347325914642765\n",
            "--- Epoch: 500, train_obj: 0.07408453711137854\n",
            "--- Epoch: 501, train_obj: 0.07560070921108629\n",
            "--- Epoch: 502, train_obj: 0.07631151305659481\n",
            "--- Epoch: 503, train_obj: 0.07479510375383473\n",
            "--- Epoch: 504, train_obj: 0.07482006372537103\n",
            "--- Epoch: 505, train_obj: 0.07406135805439445\n",
            "--- Epoch: 506, train_obj: 0.07393728138658065\n",
            "--- Epoch: 507, train_obj: 0.07385735814269186\n",
            "--- Epoch: 508, train_obj: 0.07437648687355278\n",
            "--- Epoch: 509, train_obj: 0.07103065562097713\n",
            "--- Epoch: 510, train_obj: 0.07253302003432598\n",
            "--- Epoch: 511, train_obj: 0.07290030991307185\n",
            "--- Epoch: 512, train_obj: 0.07252454085204849\n",
            "--- Epoch: 513, train_obj: 0.07407720273378954\n",
            "--- Epoch: 514, train_obj: 0.07066508144964771\n",
            "--- Epoch: 515, train_obj: 0.07126803842965677\n",
            "--- Epoch: 516, train_obj: 0.07223596367315667\n",
            "--- Epoch: 517, train_obj: 0.07072873943531867\n",
            "--- Epoch: 518, train_obj: 0.0703055508252091\n",
            "--- Epoch: 519, train_obj: 0.07156940799899224\n",
            "--- Epoch: 520, train_obj: 0.07449464633585702\n",
            "--- Epoch: 521, train_obj: 0.06936342307101266\n",
            "--- Epoch: 522, train_obj: 0.06907086423760658\n",
            "--- Epoch: 523, train_obj: 0.067762117174033\n",
            "--- Epoch: 524, train_obj: 0.0699511900741997\n",
            "--- Epoch: 525, train_obj: 0.06914799013519018\n",
            "--- Epoch: 526, train_obj: 0.0701348420820807\n",
            "--- Epoch: 527, train_obj: 0.07024232440987542\n",
            "--- Epoch: 528, train_obj: 0.07140318254945935\n",
            "--- Epoch: 529, train_obj: 0.0686566500735369\n",
            "--- Epoch: 530, train_obj: 0.07067412790399932\n",
            "--- Epoch: 531, train_obj: 0.06876386042265115\n",
            "--- Epoch: 532, train_obj: 0.06892527175485706\n",
            "--- Epoch: 533, train_obj: 0.06888709038308781\n",
            "--- Epoch: 534, train_obj: 0.06927477269900637\n",
            "--- Epoch: 535, train_obj: 0.06585629017406272\n",
            "--- Epoch: 536, train_obj: 0.06697208272713022\n",
            "--- Epoch: 537, train_obj: 0.06635535322592538\n",
            "--- Epoch: 538, train_obj: 0.06670142146945932\n",
            "--- Epoch: 539, train_obj: 0.06710370212230608\n",
            "--- Epoch: 540, train_obj: 0.06631368673914652\n",
            "--- Epoch: 541, train_obj: 0.06808460146942227\n",
            "--- Epoch: 542, train_obj: 0.06473933895582847\n",
            "--- Epoch: 543, train_obj: 0.06682893908939577\n",
            "--- Epoch: 544, train_obj: 0.06680384718164292\n",
            "--- Epoch: 545, train_obj: 0.06707760846776317\n",
            "--- Epoch: 546, train_obj: 0.06540623801335073\n",
            "--- Epoch: 547, train_obj: 0.06530470253189555\n",
            "--- Epoch: 548, train_obj: 0.06436371757783747\n",
            "--- Epoch: 549, train_obj: 0.06647871974709986\n",
            "--- Epoch: 550, train_obj: 0.06444817689782949\n",
            "--- Epoch: 551, train_obj: 0.06351455449700515\n",
            "--- Epoch: 552, train_obj: 0.06410685673831336\n",
            "--- Epoch: 553, train_obj: 0.06503529541233795\n",
            "--- Epoch: 554, train_obj: 0.06565609264104152\n",
            "--- Epoch: 555, train_obj: 0.06457436773517268\n",
            "--- Epoch: 556, train_obj: 0.06300098155584473\n",
            "--- Epoch: 557, train_obj: 0.0624738412659272\n",
            "--- Epoch: 558, train_obj: 0.06387824952690985\n",
            "--- Epoch: 559, train_obj: 0.06276018097493752\n",
            "--- Epoch: 560, train_obj: 0.062430268289296396\n",
            "--- Epoch: 561, train_obj: 0.06212315500411737\n",
            "--- Epoch: 562, train_obj: 0.062208394422055786\n",
            "--- Epoch: 563, train_obj: 0.06231493903790052\n",
            "--- Epoch: 564, train_obj: 0.0627809906054477\n",
            "--- Epoch: 565, train_obj: 0.06335962984581306\n",
            "--- Epoch: 566, train_obj: 0.06154311646636386\n",
            "--- Epoch: 567, train_obj: 0.061706767395611754\n",
            "--- Epoch: 568, train_obj: 0.06156331678064517\n",
            "--- Epoch: 569, train_obj: 0.06511935713293038\n",
            "--- Epoch: 570, train_obj: 0.06253708677296328\n",
            "--- Epoch: 571, train_obj: 0.06093757361600309\n",
            "--- Epoch: 572, train_obj: 0.05990877484612912\n",
            "--- Epoch: 573, train_obj: 0.0595452188060823\n",
            "--- Epoch: 574, train_obj: 0.06072989833855884\n",
            "--- Epoch: 575, train_obj: 0.06218292743265786\n",
            "--- Epoch: 576, train_obj: 0.06425848764884129\n",
            "--- Epoch: 577, train_obj: 0.06034429535088657\n",
            "--- Epoch: 578, train_obj: 0.05915069851409983\n",
            "--- Epoch: 579, train_obj: 0.06314894017763488\n",
            "--- Epoch: 580, train_obj: 0.06035291462742605\n",
            "--- Epoch: 581, train_obj: 0.0585975669334916\n",
            "--- Epoch: 582, train_obj: 0.05722702311261086\n",
            "--- Epoch: 583, train_obj: 0.05907583318805924\n",
            "--- Epoch: 584, train_obj: 0.05848188440880163\n",
            "--- Epoch: 585, train_obj: 0.058525800216898155\n",
            "--- Epoch: 586, train_obj: 0.058071721349867884\n",
            "--- Epoch: 587, train_obj: 0.058205926877059784\n",
            "--- Epoch: 588, train_obj: 0.05962075608384185\n",
            "--- Epoch: 589, train_obj: 0.057861421763620476\n",
            "--- Epoch: 590, train_obj: 0.0584474900931015\n",
            "--- Epoch: 591, train_obj: 0.057608125820350235\n",
            "--- Epoch: 592, train_obj: 0.05865252850481025\n",
            "--- Epoch: 593, train_obj: 0.057747495592135455\n",
            "--- Epoch: 594, train_obj: 0.058023933257819724\n",
            "--- Epoch: 595, train_obj: 0.05887860913863219\n",
            "--- Epoch: 596, train_obj: 0.06052024022935915\n",
            "--- Epoch: 597, train_obj: 0.056467796567462016\n",
            "--- Epoch: 598, train_obj: 0.05617243739372509\n",
            "--- Epoch: 599, train_obj: 0.05675423303655606\n",
            "train_err: 0.007021, val_err: 0.095833\n",
            "\n",
            "--- Epoch: 0, train_obj: 0.6776126922355888\n",
            "--- Epoch: 1, train_obj: 0.5811482321088594\n",
            "--- Epoch: 2, train_obj: 0.5320631619547357\n",
            "--- Epoch: 3, train_obj: 0.5043443634866406\n",
            "--- Epoch: 4, train_obj: 0.4884207626451165\n",
            "--- Epoch: 5, train_obj: 0.4661123028366195\n",
            "--- Epoch: 6, train_obj: 0.4492681247894555\n",
            "--- Epoch: 7, train_obj: 0.44361939501591763\n",
            "--- Epoch: 8, train_obj: 0.43089889661569447\n",
            "--- Epoch: 9, train_obj: 0.42126991362595934\n",
            "--- Epoch: 10, train_obj: 0.41472743583954724\n",
            "--- Epoch: 11, train_obj: 0.40533065513815497\n",
            "--- Epoch: 12, train_obj: 0.40137106364728037\n",
            "--- Epoch: 13, train_obj: 0.39445204263437467\n",
            "--- Epoch: 14, train_obj: 0.38586244001260683\n",
            "--- Epoch: 15, train_obj: 0.3828047305873406\n",
            "--- Epoch: 16, train_obj: 0.3816633438782896\n",
            "--- Epoch: 17, train_obj: 0.37147417816699607\n",
            "--- Epoch: 18, train_obj: 0.366806259954844\n",
            "--- Epoch: 19, train_obj: 0.36658230637914624\n",
            "--- Epoch: 20, train_obj: 0.3592553299674133\n",
            "--- Epoch: 21, train_obj: 0.35499007288591755\n",
            "--- Epoch: 22, train_obj: 0.3559410230348235\n",
            "--- Epoch: 23, train_obj: 0.3477018600026361\n",
            "--- Epoch: 24, train_obj: 0.3462523944237993\n",
            "--- Epoch: 25, train_obj: 0.34258303991028327\n",
            "--- Epoch: 26, train_obj: 0.33905643884046055\n",
            "--- Epoch: 27, train_obj: 0.33617362391936284\n",
            "--- Epoch: 28, train_obj: 0.3356672702829622\n",
            "--- Epoch: 29, train_obj: 0.3317776546854363\n",
            "--- Epoch: 30, train_obj: 0.3275300185674324\n",
            "--- Epoch: 31, train_obj: 0.32655332224465233\n",
            "--- Epoch: 32, train_obj: 0.319841418589626\n",
            "--- Epoch: 33, train_obj: 0.31924700068990713\n",
            "--- Epoch: 34, train_obj: 0.31698631449262044\n",
            "--- Epoch: 35, train_obj: 0.3144686343945382\n",
            "--- Epoch: 36, train_obj: 0.3104735394602253\n",
            "--- Epoch: 37, train_obj: 0.3080652021678512\n",
            "--- Epoch: 38, train_obj: 0.30805412491247036\n",
            "--- Epoch: 39, train_obj: 0.30528725853320143\n",
            "--- Epoch: 40, train_obj: 0.30227256508397654\n",
            "--- Epoch: 41, train_obj: 0.2973944567474422\n",
            "--- Epoch: 42, train_obj: 0.3002304035816422\n",
            "--- Epoch: 43, train_obj: 0.2982775421151634\n",
            "--- Epoch: 44, train_obj: 0.2922210472888407\n",
            "--- Epoch: 45, train_obj: 0.29327503786402653\n",
            "--- Epoch: 46, train_obj: 0.2893166884655243\n",
            "--- Epoch: 47, train_obj: 0.2892051523524097\n",
            "--- Epoch: 48, train_obj: 0.2901176770255663\n",
            "--- Epoch: 49, train_obj: 0.2858520031237303\n",
            "--- Epoch: 50, train_obj: 0.2779625858616147\n",
            "--- Epoch: 51, train_obj: 0.2785806763931377\n",
            "--- Epoch: 52, train_obj: 0.27655554498029716\n",
            "--- Epoch: 53, train_obj: 0.27479718348775295\n",
            "--- Epoch: 54, train_obj: 0.2764247103512332\n",
            "--- Epoch: 55, train_obj: 0.2714705409830411\n",
            "--- Epoch: 56, train_obj: 0.27152085085696065\n",
            "--- Epoch: 57, train_obj: 0.26841240005285494\n",
            "--- Epoch: 58, train_obj: 0.2694586554478658\n",
            "--- Epoch: 59, train_obj: 0.2635495328127847\n",
            "--- Epoch: 60, train_obj: 0.2629223104214728\n",
            "--- Epoch: 61, train_obj: 0.26083958686887193\n",
            "--- Epoch: 62, train_obj: 0.26034245555945107\n",
            "--- Epoch: 63, train_obj: 0.25597111037088593\n",
            "--- Epoch: 64, train_obj: 0.2539224775418384\n",
            "--- Epoch: 65, train_obj: 0.2542444369092736\n",
            "--- Epoch: 66, train_obj: 0.2520179192564898\n",
            "--- Epoch: 67, train_obj: 0.25066601486832435\n",
            "--- Epoch: 68, train_obj: 0.24837375744771362\n",
            "--- Epoch: 69, train_obj: 0.24793251594106716\n",
            "--- Epoch: 70, train_obj: 0.2460438357519688\n",
            "--- Epoch: 71, train_obj: 0.2443947607127224\n",
            "--- Epoch: 72, train_obj: 0.24244540071830786\n",
            "--- Epoch: 73, train_obj: 0.24032679201704293\n",
            "--- Epoch: 74, train_obj: 0.24213133937601988\n",
            "--- Epoch: 75, train_obj: 0.24160564206890392\n",
            "--- Epoch: 76, train_obj: 0.23698528817012304\n",
            "--- Epoch: 77, train_obj: 0.23593633768364186\n",
            "--- Epoch: 78, train_obj: 0.2376401843500993\n",
            "--- Epoch: 79, train_obj: 0.23313168717385474\n",
            "--- Epoch: 80, train_obj: 0.23057640278688618\n",
            "--- Epoch: 81, train_obj: 0.22922294468825763\n",
            "--- Epoch: 82, train_obj: 0.2282019876246203\n",
            "--- Epoch: 83, train_obj: 0.22887753693684648\n",
            "--- Epoch: 84, train_obj: 0.22398285828193842\n",
            "--- Epoch: 85, train_obj: 0.2231044338859226\n",
            "--- Epoch: 86, train_obj: 0.2226807525078921\n",
            "--- Epoch: 87, train_obj: 0.22436939610517878\n",
            "--- Epoch: 88, train_obj: 0.22349306526287813\n",
            "--- Epoch: 89, train_obj: 0.22197202480562836\n",
            "--- Epoch: 90, train_obj: 0.21594466036512866\n",
            "--- Epoch: 91, train_obj: 0.2179032233897788\n",
            "--- Epoch: 92, train_obj: 0.21328552363352085\n",
            "--- Epoch: 93, train_obj: 0.21128590842787734\n",
            "--- Epoch: 94, train_obj: 0.2093710422740615\n",
            "--- Epoch: 95, train_obj: 0.211858932934207\n",
            "--- Epoch: 96, train_obj: 0.2115866281834971\n",
            "--- Epoch: 97, train_obj: 0.2061761703185285\n",
            "--- Epoch: 98, train_obj: 0.20824381037479406\n",
            "--- Epoch: 99, train_obj: 0.20504831965356765\n",
            "--- Epoch: 100, train_obj: 0.20183372995290552\n",
            "--- Epoch: 101, train_obj: 0.2022293775775954\n",
            "--- Epoch: 102, train_obj: 0.20272218417505716\n",
            "--- Epoch: 103, train_obj: 0.19951161246963114\n",
            "--- Epoch: 104, train_obj: 0.19758774893186823\n",
            "--- Epoch: 105, train_obj: 0.19635727740155695\n",
            "--- Epoch: 106, train_obj: 0.19658264843021694\n",
            "--- Epoch: 107, train_obj: 0.19325909783473286\n",
            "--- Epoch: 108, train_obj: 0.19938836426981071\n",
            "--- Epoch: 109, train_obj: 0.19158032064453584\n",
            "--- Epoch: 110, train_obj: 0.19322235531027157\n",
            "--- Epoch: 111, train_obj: 0.19002737767867753\n",
            "--- Epoch: 112, train_obj: 0.1880095005407333\n",
            "--- Epoch: 113, train_obj: 0.18651960738983608\n",
            "--- Epoch: 114, train_obj: 0.1844711794476068\n",
            "--- Epoch: 115, train_obj: 0.1860374169209721\n",
            "--- Epoch: 116, train_obj: 0.18230810452435806\n",
            "--- Epoch: 117, train_obj: 0.18427046106208606\n",
            "--- Epoch: 118, train_obj: 0.18152094389742118\n",
            "--- Epoch: 119, train_obj: 0.18164046531745665\n",
            "--- Epoch: 120, train_obj: 0.17758731665758187\n",
            "--- Epoch: 121, train_obj: 0.17759234362706086\n",
            "--- Epoch: 122, train_obj: 0.17617170700455184\n",
            "--- Epoch: 123, train_obj: 0.1768733435763279\n",
            "--- Epoch: 124, train_obj: 0.1742983567076363\n",
            "--- Epoch: 125, train_obj: 0.17111240703582825\n",
            "--- Epoch: 126, train_obj: 0.17218903944644234\n",
            "--- Epoch: 127, train_obj: 0.1726028774199966\n",
            "--- Epoch: 128, train_obj: 0.17331274231950075\n",
            "--- Epoch: 129, train_obj: 0.18291618808011473\n",
            "--- Epoch: 130, train_obj: 0.168812532192619\n",
            "--- Epoch: 131, train_obj: 0.16580249150117018\n",
            "--- Epoch: 132, train_obj: 0.16564251948580208\n",
            "--- Epoch: 133, train_obj: 0.16765061371822498\n",
            "--- Epoch: 134, train_obj: 0.165569224655744\n",
            "--- Epoch: 135, train_obj: 0.16234085096666936\n",
            "--- Epoch: 136, train_obj: 0.16129051259434668\n",
            "--- Epoch: 137, train_obj: 0.15901190381645977\n",
            "--- Epoch: 138, train_obj: 0.15979514214860957\n",
            "--- Epoch: 139, train_obj: 0.1591328508277338\n",
            "--- Epoch: 140, train_obj: 0.15584854038726464\n",
            "--- Epoch: 141, train_obj: 0.156756275152727\n",
            "--- Epoch: 142, train_obj: 0.15466802652083309\n",
            "--- Epoch: 143, train_obj: 0.15420733467721617\n",
            "--- Epoch: 144, train_obj: 0.1530430432796733\n",
            "--- Epoch: 145, train_obj: 0.15155213998090297\n",
            "--- Epoch: 146, train_obj: 0.1579270262695696\n",
            "--- Epoch: 147, train_obj: 0.15264989750958555\n",
            "--- Epoch: 148, train_obj: 0.1527580480964336\n",
            "--- Epoch: 149, train_obj: 0.14616561820612273\n",
            "--- Epoch: 150, train_obj: 0.14680670718573754\n",
            "--- Epoch: 151, train_obj: 0.14603977647816085\n",
            "--- Epoch: 152, train_obj: 0.14247859770010163\n",
            "--- Epoch: 153, train_obj: 0.14527638505438586\n",
            "--- Epoch: 154, train_obj: 0.14926618361625718\n",
            "--- Epoch: 155, train_obj: 0.14414428853834935\n",
            "--- Epoch: 156, train_obj: 0.15574647944246553\n",
            "--- Epoch: 157, train_obj: 0.13853697750923324\n",
            "--- Epoch: 158, train_obj: 0.1397881783332797\n",
            "--- Epoch: 159, train_obj: 0.14612450545739492\n",
            "--- Epoch: 160, train_obj: 0.1365805530990286\n",
            "--- Epoch: 161, train_obj: 0.13544400668395112\n",
            "--- Epoch: 162, train_obj: 0.1407116201258335\n",
            "--- Epoch: 163, train_obj: 0.13346771457507733\n",
            "--- Epoch: 164, train_obj: 0.13864765242858462\n",
            "--- Epoch: 165, train_obj: 0.13428943743563979\n",
            "--- Epoch: 166, train_obj: 0.13008530603616075\n",
            "--- Epoch: 167, train_obj: 0.13588888033752566\n",
            "--- Epoch: 168, train_obj: 0.13251864024238016\n",
            "--- Epoch: 169, train_obj: 0.1292920328623835\n",
            "--- Epoch: 170, train_obj: 0.12847431799994552\n",
            "--- Epoch: 171, train_obj: 0.12765983344742707\n",
            "--- Epoch: 172, train_obj: 0.13149881780536105\n",
            "--- Epoch: 173, train_obj: 0.12753511640360352\n",
            "--- Epoch: 174, train_obj: 0.1267906778792842\n",
            "--- Epoch: 175, train_obj: 0.12646100383719736\n",
            "--- Epoch: 176, train_obj: 0.12281377476196827\n",
            "--- Epoch: 177, train_obj: 0.12166249443703028\n",
            "--- Epoch: 178, train_obj: 0.12210194614814897\n",
            "--- Epoch: 179, train_obj: 0.12385200798512454\n",
            "--- Epoch: 180, train_obj: 0.12113693449660928\n",
            "--- Epoch: 181, train_obj: 0.11845099959727108\n",
            "--- Epoch: 182, train_obj: 0.12036442460754056\n",
            "--- Epoch: 183, train_obj: 0.11525509382819217\n",
            "--- Epoch: 184, train_obj: 0.1177123606580542\n",
            "--- Epoch: 185, train_obj: 0.12049836514254014\n",
            "--- Epoch: 186, train_obj: 0.11981824112334989\n",
            "--- Epoch: 187, train_obj: 0.11472039302076575\n",
            "--- Epoch: 188, train_obj: 0.11439298455990755\n",
            "--- Epoch: 189, train_obj: 0.11171182948946803\n",
            "--- Epoch: 190, train_obj: 0.11395574327565798\n",
            "--- Epoch: 191, train_obj: 0.11346262282482944\n",
            "--- Epoch: 192, train_obj: 0.10980913805294938\n",
            "--- Epoch: 193, train_obj: 0.11008303409720208\n",
            "--- Epoch: 194, train_obj: 0.11205658927126427\n",
            "--- Epoch: 195, train_obj: 0.11250219758046859\n",
            "--- Epoch: 196, train_obj: 0.10562296391802566\n",
            "--- Epoch: 197, train_obj: 0.10947265893186522\n",
            "--- Epoch: 198, train_obj: 0.1074876219117113\n",
            "--- Epoch: 199, train_obj: 0.10336786164686176\n",
            "--- Epoch: 200, train_obj: 0.10252288907011389\n",
            "--- Epoch: 201, train_obj: 0.1085315199519503\n",
            "--- Epoch: 202, train_obj: 0.10384065218878243\n",
            "--- Epoch: 203, train_obj: 0.11475218457721936\n",
            "--- Epoch: 204, train_obj: 0.10535956942368738\n",
            "--- Epoch: 205, train_obj: 0.10030795831286919\n",
            "--- Epoch: 206, train_obj: 0.09882922210621556\n",
            "--- Epoch: 207, train_obj: 0.10149030184079674\n",
            "--- Epoch: 208, train_obj: 0.0993966041501843\n",
            "--- Epoch: 209, train_obj: 0.0989454397845362\n",
            "--- Epoch: 210, train_obj: 0.09785223594207586\n",
            "--- Epoch: 211, train_obj: 0.09947695276277835\n",
            "--- Epoch: 212, train_obj: 0.09785483677610836\n",
            "--- Epoch: 213, train_obj: 0.09683724638227076\n",
            "--- Epoch: 214, train_obj: 0.09831720233231965\n",
            "--- Epoch: 215, train_obj: 0.09317404872024501\n",
            "--- Epoch: 216, train_obj: 0.09509258121975146\n",
            "--- Epoch: 217, train_obj: 0.09007250789477124\n",
            "--- Epoch: 218, train_obj: 0.09109849359136729\n",
            "--- Epoch: 219, train_obj: 0.09066499801263456\n",
            "--- Epoch: 220, train_obj: 0.09175187750116295\n",
            "--- Epoch: 221, train_obj: 0.09157752643695019\n",
            "--- Epoch: 222, train_obj: 0.09182505634761802\n",
            "--- Epoch: 223, train_obj: 0.09657141267181728\n",
            "--- Epoch: 224, train_obj: 0.08565949498111462\n",
            "--- Epoch: 225, train_obj: 0.08763413336754951\n",
            "--- Epoch: 226, train_obj: 0.08555912835402321\n",
            "--- Epoch: 227, train_obj: 0.08662986781852021\n",
            "--- Epoch: 228, train_obj: 0.08280671658781832\n",
            "--- Epoch: 229, train_obj: 0.0865658168668031\n",
            "--- Epoch: 230, train_obj: 0.09055514334887929\n",
            "--- Epoch: 231, train_obj: 0.0813667925054042\n",
            "--- Epoch: 232, train_obj: 0.08234032174406415\n",
            "--- Epoch: 233, train_obj: 0.08167116336241531\n",
            "--- Epoch: 234, train_obj: 0.0829803635483566\n",
            "--- Epoch: 235, train_obj: 0.07831525611994779\n",
            "--- Epoch: 236, train_obj: 0.07990431890437605\n",
            "--- Epoch: 237, train_obj: 0.08624616802835197\n",
            "--- Epoch: 238, train_obj: 0.07715258797558894\n",
            "--- Epoch: 239, train_obj: 0.09529530403392486\n",
            "--- Epoch: 240, train_obj: 0.0784900934576535\n",
            "--- Epoch: 241, train_obj: 0.07938577692944063\n",
            "--- Epoch: 242, train_obj: 0.07759428039824377\n",
            "--- Epoch: 243, train_obj: 0.07848648213886662\n",
            "--- Epoch: 244, train_obj: 0.07630349222821513\n",
            "--- Epoch: 245, train_obj: 0.07519740644658028\n",
            "--- Epoch: 246, train_obj: 0.07479726329025324\n",
            "--- Epoch: 247, train_obj: 0.07493775999998417\n",
            "--- Epoch: 248, train_obj: 0.07234503885436833\n",
            "--- Epoch: 249, train_obj: 0.07030520146865908\n",
            "--- Epoch: 250, train_obj: 0.07203572274603257\n",
            "--- Epoch: 251, train_obj: 0.07247323783768948\n",
            "--- Epoch: 252, train_obj: 0.07137922862043468\n",
            "--- Epoch: 253, train_obj: 0.07197233434198452\n",
            "--- Epoch: 254, train_obj: 0.07706696326722154\n",
            "--- Epoch: 255, train_obj: 0.06796861203149927\n",
            "--- Epoch: 256, train_obj: 0.06870934953440061\n",
            "--- Epoch: 257, train_obj: 0.06836914602324529\n",
            "--- Epoch: 258, train_obj: 0.06713909006790056\n",
            "--- Epoch: 259, train_obj: 0.07344006305174429\n",
            "--- Epoch: 260, train_obj: 0.07146004620741135\n",
            "--- Epoch: 261, train_obj: 0.06792987088643518\n",
            "--- Epoch: 262, train_obj: 0.0709048516517161\n",
            "--- Epoch: 263, train_obj: 0.06536119404534171\n",
            "--- Epoch: 264, train_obj: 0.069999034976369\n",
            "--- Epoch: 265, train_obj: 0.07283553560362373\n",
            "--- Epoch: 266, train_obj: 0.06529629020641825\n",
            "--- Epoch: 267, train_obj: 0.06657581483642805\n",
            "--- Epoch: 268, train_obj: 0.06370859844652084\n",
            "--- Epoch: 269, train_obj: 0.0640516299525574\n",
            "--- Epoch: 270, train_obj: 0.06219331936207529\n",
            "--- Epoch: 271, train_obj: 0.06292153407008498\n",
            "--- Epoch: 272, train_obj: 0.07606749827385653\n",
            "--- Epoch: 273, train_obj: 0.061459537128289915\n",
            "--- Epoch: 274, train_obj: 0.06220071501753467\n",
            "--- Epoch: 275, train_obj: 0.06921471167590477\n",
            "--- Epoch: 276, train_obj: 0.05981661732949997\n",
            "--- Epoch: 277, train_obj: 0.06114272828527804\n",
            "--- Epoch: 278, train_obj: 0.0620848291404336\n",
            "--- Epoch: 279, train_obj: 0.060627290858552396\n",
            "--- Epoch: 280, train_obj: 0.057659729029287995\n",
            "--- Epoch: 281, train_obj: 0.057024078475232345\n",
            "--- Epoch: 282, train_obj: 0.055970889542125236\n",
            "--- Epoch: 283, train_obj: 0.05659766760906824\n",
            "--- Epoch: 284, train_obj: 0.058787169616508456\n",
            "--- Epoch: 285, train_obj: 0.05474256270027348\n",
            "--- Epoch: 286, train_obj: 0.06063997290767852\n",
            "--- Epoch: 287, train_obj: 0.06101598657973481\n",
            "--- Epoch: 288, train_obj: 0.05676523801595903\n",
            "--- Epoch: 289, train_obj: 0.05861974772898974\n",
            "--- Epoch: 290, train_obj: 0.05758212293389998\n",
            "--- Epoch: 291, train_obj: 0.059566726766357554\n",
            "--- Epoch: 292, train_obj: 0.053898150551193964\n",
            "--- Epoch: 293, train_obj: 0.05604452307417853\n",
            "--- Epoch: 294, train_obj: 0.055562411793731246\n",
            "--- Epoch: 295, train_obj: 0.05639378220295019\n",
            "--- Epoch: 296, train_obj: 0.05249605198387464\n",
            "--- Epoch: 297, train_obj: 0.051885943331528024\n",
            "--- Epoch: 298, train_obj: 0.050192928190255066\n",
            "--- Epoch: 299, train_obj: 0.052359996262680536\n",
            "--- Epoch: 300, train_obj: 0.05007997807489033\n",
            "--- Epoch: 301, train_obj: 0.05069096176609101\n",
            "--- Epoch: 302, train_obj: 0.05145327166975173\n",
            "--- Epoch: 303, train_obj: 0.04840602495406545\n",
            "--- Epoch: 304, train_obj: 0.0484244724602993\n",
            "--- Epoch: 305, train_obj: 0.047884943530728356\n",
            "--- Epoch: 306, train_obj: 0.05202427824325571\n",
            "--- Epoch: 307, train_obj: 0.049468179279397013\n",
            "--- Epoch: 308, train_obj: 0.0488629467981683\n",
            "--- Epoch: 309, train_obj: 0.0495114925160553\n",
            "--- Epoch: 310, train_obj: 0.05181192245227945\n",
            "--- Epoch: 311, train_obj: 0.04845051539014637\n",
            "--- Epoch: 312, train_obj: 0.04653844877095328\n",
            "--- Epoch: 313, train_obj: 0.04799637883435639\n",
            "--- Epoch: 314, train_obj: 0.05914047704435638\n",
            "--- Epoch: 315, train_obj: 0.04549292617782221\n",
            "--- Epoch: 316, train_obj: 0.045160712461327614\n",
            "--- Epoch: 317, train_obj: 0.044736844482280866\n",
            "--- Epoch: 318, train_obj: 0.04182135079412289\n",
            "--- Epoch: 319, train_obj: 0.04407055615002221\n",
            "--- Epoch: 320, train_obj: 0.043200626607822035\n",
            "--- Epoch: 321, train_obj: 0.04765331126351106\n",
            "--- Epoch: 322, train_obj: 0.044065241867809135\n",
            "--- Epoch: 323, train_obj: 0.04140190135391425\n",
            "--- Epoch: 324, train_obj: 0.050877123398255834\n",
            "--- Epoch: 325, train_obj: 0.041568307994238725\n",
            "--- Epoch: 326, train_obj: 0.04133560882701575\n",
            "--- Epoch: 327, train_obj: 0.04309982575063156\n",
            "--- Epoch: 328, train_obj: 0.0425150899475083\n",
            "--- Epoch: 329, train_obj: 0.04161908748312511\n",
            "--- Epoch: 330, train_obj: 0.04480209086933379\n",
            "--- Epoch: 331, train_obj: 0.03999136743512957\n",
            "--- Epoch: 332, train_obj: 0.04012999314120314\n",
            "--- Epoch: 333, train_obj: 0.04199904410441311\n",
            "--- Epoch: 334, train_obj: 0.04080487299890891\n",
            "--- Epoch: 335, train_obj: 0.03925534416537795\n",
            "--- Epoch: 336, train_obj: 0.0425839492323427\n",
            "--- Epoch: 337, train_obj: 0.03901884073140821\n",
            "--- Epoch: 338, train_obj: 0.041005245400366416\n",
            "--- Epoch: 339, train_obj: 0.04059579954395223\n",
            "--- Epoch: 340, train_obj: 0.037013250514320174\n",
            "--- Epoch: 341, train_obj: 0.03688739906651901\n",
            "--- Epoch: 342, train_obj: 0.03806694559444301\n",
            "--- Epoch: 343, train_obj: 0.03859318284630135\n",
            "--- Epoch: 344, train_obj: 0.038838631792174755\n",
            "--- Epoch: 345, train_obj: 0.03789909812010924\n",
            "--- Epoch: 346, train_obj: 0.042310941594163126\n",
            "--- Epoch: 347, train_obj: 0.03720213628451513\n",
            "--- Epoch: 348, train_obj: 0.03715658191023015\n",
            "--- Epoch: 349, train_obj: 0.03619766769287177\n",
            "--- Epoch: 350, train_obj: 0.03683789638595737\n",
            "--- Epoch: 351, train_obj: 0.035739185809224076\n",
            "--- Epoch: 352, train_obj: 0.03751585953339885\n",
            "--- Epoch: 353, train_obj: 0.03676167664351362\n",
            "--- Epoch: 354, train_obj: 0.03668349185413858\n",
            "--- Epoch: 355, train_obj: 0.036798912760300465\n",
            "--- Epoch: 356, train_obj: 0.0344231639915806\n",
            "--- Epoch: 357, train_obj: 0.037068183118319796\n",
            "--- Epoch: 358, train_obj: 0.03315536611975223\n",
            "--- Epoch: 359, train_obj: 0.03553393585327441\n",
            "--- Epoch: 360, train_obj: 0.03364935200234339\n",
            "--- Epoch: 361, train_obj: 0.0354454910914566\n",
            "--- Epoch: 362, train_obj: 0.03253292296896394\n",
            "--- Epoch: 363, train_obj: 0.03357571626814809\n",
            "--- Epoch: 364, train_obj: 0.03242354569607579\n",
            "--- Epoch: 365, train_obj: 0.03389543199410768\n",
            "--- Epoch: 366, train_obj: 0.0336329137544185\n",
            "--- Epoch: 367, train_obj: 0.034544623603129175\n",
            "--- Epoch: 368, train_obj: 0.03369486737017681\n",
            "--- Epoch: 369, train_obj: 0.03381714241131735\n",
            "--- Epoch: 370, train_obj: 0.03382242525459258\n",
            "--- Epoch: 371, train_obj: 0.035488845028108805\n",
            "--- Epoch: 372, train_obj: 0.0426988160284385\n",
            "--- Epoch: 373, train_obj: 0.034046374254102785\n",
            "--- Epoch: 374, train_obj: 0.032387428083051284\n",
            "--- Epoch: 375, train_obj: 0.03195612126333844\n",
            "--- Epoch: 376, train_obj: 0.02990851080843503\n",
            "--- Epoch: 377, train_obj: 0.031555388770021515\n",
            "--- Epoch: 378, train_obj: 0.030092349750208882\n",
            "--- Epoch: 379, train_obj: 0.03458922526034113\n",
            "--- Epoch: 380, train_obj: 0.029581716771803806\n",
            "--- Epoch: 381, train_obj: 0.02810788968029103\n",
            "--- Epoch: 382, train_obj: 0.03017866431187573\n",
            "--- Epoch: 383, train_obj: 0.02831857101052332\n",
            "--- Epoch: 384, train_obj: 0.03054789602896439\n",
            "--- Epoch: 385, train_obj: 0.028700529888257113\n",
            "--- Epoch: 386, train_obj: 0.029068918935786524\n",
            "--- Epoch: 387, train_obj: 0.0346478143525783\n",
            "--- Epoch: 388, train_obj: 0.030033797244775787\n",
            "--- Epoch: 389, train_obj: 0.030058521196730192\n",
            "--- Epoch: 390, train_obj: 0.028652345468307205\n",
            "--- Epoch: 391, train_obj: 0.027839744384651272\n",
            "--- Epoch: 392, train_obj: 0.026907969072508687\n",
            "--- Epoch: 393, train_obj: 0.027778497689461185\n",
            "--- Epoch: 394, train_obj: 0.027447912206263296\n",
            "--- Epoch: 395, train_obj: 0.029188719747796455\n",
            "--- Epoch: 396, train_obj: 0.028408198624068282\n",
            "--- Epoch: 397, train_obj: 0.029080304437548592\n",
            "--- Epoch: 398, train_obj: 0.03260372692693706\n",
            "--- Epoch: 399, train_obj: 0.028376940513099403\n",
            "--- Epoch: 400, train_obj: 0.035724509818784854\n",
            "--- Epoch: 401, train_obj: 0.02842387628775903\n",
            "--- Epoch: 402, train_obj: 0.025362062209084303\n",
            "--- Epoch: 403, train_obj: 0.025811436513857478\n",
            "--- Epoch: 404, train_obj: 0.02479445914665301\n",
            "--- Epoch: 405, train_obj: 0.025084641006825996\n",
            "--- Epoch: 406, train_obj: 0.024769561751269306\n",
            "--- Epoch: 407, train_obj: 0.02779175981986818\n",
            "--- Epoch: 408, train_obj: 0.026273620590295376\n",
            "--- Epoch: 409, train_obj: 0.026151784468828144\n",
            "--- Epoch: 410, train_obj: 0.024994062951051547\n",
            "--- Epoch: 411, train_obj: 0.03830954908722104\n",
            "--- Epoch: 412, train_obj: 0.025372615365106575\n",
            "--- Epoch: 413, train_obj: 0.024382132279652798\n",
            "--- Epoch: 414, train_obj: 0.025034939123460453\n",
            "--- Epoch: 415, train_obj: 0.024418301009504446\n",
            "--- Epoch: 416, train_obj: 0.024875752640218803\n",
            "--- Epoch: 417, train_obj: 0.02598886385616229\n",
            "--- Epoch: 418, train_obj: 0.023035135361717085\n",
            "--- Epoch: 419, train_obj: 0.026723463330762788\n",
            "--- Epoch: 420, train_obj: 0.0228707529128361\n",
            "--- Epoch: 421, train_obj: 0.02328915157252728\n",
            "--- Epoch: 422, train_obj: 0.028465443455814164\n",
            "--- Epoch: 423, train_obj: 0.0238248428633171\n",
            "--- Epoch: 424, train_obj: 0.025176468490891647\n",
            "--- Epoch: 425, train_obj: 0.021650903011596907\n",
            "--- Epoch: 426, train_obj: 0.024411107366039995\n",
            "--- Epoch: 427, train_obj: 0.02229317446167617\n",
            "--- Epoch: 428, train_obj: 0.02141502034365772\n",
            "--- Epoch: 429, train_obj: 0.022089945353305333\n",
            "--- Epoch: 430, train_obj: 0.022964357696525047\n",
            "--- Epoch: 431, train_obj: 0.022725824530376908\n",
            "--- Epoch: 432, train_obj: 0.023078272231505142\n",
            "--- Epoch: 433, train_obj: 0.020979406797586173\n",
            "--- Epoch: 434, train_obj: 0.022564113750443027\n",
            "--- Epoch: 435, train_obj: 0.025673520785565566\n",
            "--- Epoch: 436, train_obj: 0.021505059790148485\n",
            "--- Epoch: 437, train_obj: 0.02140395001290003\n",
            "--- Epoch: 438, train_obj: 0.02195489389024889\n",
            "--- Epoch: 439, train_obj: 0.023301775780784403\n",
            "--- Epoch: 440, train_obj: 0.020267734380804706\n",
            "--- Epoch: 441, train_obj: 0.020599965547681738\n",
            "--- Epoch: 442, train_obj: 0.020756449884760615\n",
            "--- Epoch: 443, train_obj: 0.02200895364481654\n",
            "--- Epoch: 444, train_obj: 0.024236892410995275\n",
            "--- Epoch: 445, train_obj: 0.0216976791914142\n",
            "--- Epoch: 446, train_obj: 0.020172482024893804\n",
            "--- Epoch: 447, train_obj: 0.019515674609491745\n",
            "--- Epoch: 448, train_obj: 0.020770386184476356\n",
            "--- Epoch: 449, train_obj: 0.01936313360177771\n",
            "--- Epoch: 450, train_obj: 0.02029997975253896\n",
            "--- Epoch: 451, train_obj: 0.019715293356133187\n",
            "--- Epoch: 452, train_obj: 0.02067992492275113\n",
            "--- Epoch: 453, train_obj: 0.020613241164131095\n",
            "--- Epoch: 454, train_obj: 0.019585427684582376\n",
            "--- Epoch: 455, train_obj: 0.020152018378888515\n",
            "--- Epoch: 456, train_obj: 0.01856808364722693\n",
            "--- Epoch: 457, train_obj: 0.01941880789193571\n",
            "--- Epoch: 458, train_obj: 0.018968412421716284\n",
            "--- Epoch: 459, train_obj: 0.020485716773753382\n",
            "--- Epoch: 460, train_obj: 0.021351610181962778\n",
            "--- Epoch: 461, train_obj: 0.019670331244355493\n",
            "--- Epoch: 462, train_obj: 0.01946060282823708\n",
            "--- Epoch: 463, train_obj: 0.018220307606084903\n",
            "--- Epoch: 464, train_obj: 0.01813400688886726\n",
            "--- Epoch: 465, train_obj: 0.018181144410326557\n",
            "--- Epoch: 466, train_obj: 0.019774590686340347\n",
            "--- Epoch: 467, train_obj: 0.018385899415383454\n",
            "--- Epoch: 468, train_obj: 0.017528843369744085\n",
            "--- Epoch: 469, train_obj: 0.017547552679034385\n",
            "--- Epoch: 470, train_obj: 0.017778550629913223\n",
            "--- Epoch: 471, train_obj: 0.01688287494575359\n",
            "--- Epoch: 472, train_obj: 0.019102225202991027\n",
            "--- Epoch: 473, train_obj: 0.019988327142602555\n",
            "--- Epoch: 474, train_obj: 0.018682198151606873\n",
            "--- Epoch: 475, train_obj: 0.01920476412610057\n",
            "--- Epoch: 476, train_obj: 0.016755374995086246\n",
            "--- Epoch: 477, train_obj: 0.021366169238631105\n",
            "--- Epoch: 478, train_obj: 0.018263016159093974\n",
            "--- Epoch: 479, train_obj: 0.01844354999201992\n",
            "--- Epoch: 480, train_obj: 0.01598480451944878\n",
            "--- Epoch: 481, train_obj: 0.018139332584946474\n",
            "--- Epoch: 482, train_obj: 0.017381325744031773\n",
            "--- Epoch: 483, train_obj: 0.017525844964658758\n",
            "--- Epoch: 484, train_obj: 0.018997512442030816\n",
            "--- Epoch: 485, train_obj: 0.01677526406676439\n",
            "--- Epoch: 486, train_obj: 0.01939286346340377\n",
            "--- Epoch: 487, train_obj: 0.01743351116173523\n",
            "--- Epoch: 488, train_obj: 0.015663107652673862\n",
            "--- Epoch: 489, train_obj: 0.01714089572448898\n",
            "--- Epoch: 490, train_obj: 0.01804319592460637\n",
            "--- Epoch: 491, train_obj: 0.01730008386182032\n",
            "--- Epoch: 492, train_obj: 0.015644234092473067\n",
            "--- Epoch: 493, train_obj: 0.015405320047217296\n",
            "--- Epoch: 494, train_obj: 0.015531973004022058\n",
            "--- Epoch: 495, train_obj: 0.017601840561787777\n",
            "--- Epoch: 496, train_obj: 0.015097533602074074\n",
            "--- Epoch: 497, train_obj: 0.015999298944787264\n",
            "--- Epoch: 498, train_obj: 0.01638709926848289\n",
            "--- Epoch: 499, train_obj: 0.018498730498767272\n",
            "--- Epoch: 500, train_obj: 0.015550298276641478\n",
            "--- Epoch: 501, train_obj: 0.017871951978289096\n",
            "--- Epoch: 502, train_obj: 0.016386100052956234\n",
            "--- Epoch: 503, train_obj: 0.015162633502391757\n",
            "--- Epoch: 504, train_obj: 0.016745448855619315\n",
            "--- Epoch: 505, train_obj: 0.015853687163629438\n",
            "--- Epoch: 506, train_obj: 0.015531546634563619\n",
            "--- Epoch: 507, train_obj: 0.015163754822415542\n",
            "--- Epoch: 508, train_obj: 0.014541642257328043\n",
            "--- Epoch: 509, train_obj: 0.0172644625521732\n",
            "--- Epoch: 510, train_obj: 0.014299666208596023\n",
            "--- Epoch: 511, train_obj: 0.014710610679253595\n",
            "--- Epoch: 512, train_obj: 0.01522894908982756\n",
            "--- Epoch: 513, train_obj: 0.015595430095097807\n",
            "--- Epoch: 514, train_obj: 0.014174749789708273\n",
            "--- Epoch: 515, train_obj: 0.014249382541495166\n",
            "--- Epoch: 516, train_obj: 0.015760126065680436\n",
            "--- Epoch: 517, train_obj: 0.01623116026426429\n",
            "--- Epoch: 518, train_obj: 0.014122060034272221\n",
            "--- Epoch: 519, train_obj: 0.014580495658014402\n",
            "--- Epoch: 520, train_obj: 0.014256458593866903\n",
            "--- Epoch: 521, train_obj: 0.014185414467535705\n",
            "--- Epoch: 522, train_obj: 0.015883691054980238\n",
            "--- Epoch: 523, train_obj: 0.013612440131844589\n",
            "--- Epoch: 524, train_obj: 0.01399355486815454\n",
            "--- Epoch: 525, train_obj: 0.015317486601239926\n",
            "--- Epoch: 526, train_obj: 0.014451306058870668\n",
            "--- Epoch: 527, train_obj: 0.014798732751831167\n",
            "--- Epoch: 528, train_obj: 0.013507603214036207\n",
            "--- Epoch: 529, train_obj: 0.013819973226829221\n",
            "--- Epoch: 530, train_obj: 0.013423601093795024\n",
            "--- Epoch: 531, train_obj: 0.014008697620709913\n",
            "--- Epoch: 532, train_obj: 0.013519764512569471\n",
            "--- Epoch: 533, train_obj: 0.015292423329389153\n",
            "--- Epoch: 534, train_obj: 0.015080594911139004\n",
            "--- Epoch: 535, train_obj: 0.012958124610156718\n",
            "--- Epoch: 536, train_obj: 0.012420270316621455\n",
            "--- Epoch: 537, train_obj: 0.015313746151244952\n",
            "--- Epoch: 538, train_obj: 0.01380351578484361\n",
            "--- Epoch: 539, train_obj: 0.014203174870874075\n",
            "--- Epoch: 540, train_obj: 0.014060608198408785\n",
            "--- Epoch: 541, train_obj: 0.012846158048855649\n",
            "--- Epoch: 542, train_obj: 0.012878453860195397\n",
            "--- Epoch: 543, train_obj: 0.019155788273794515\n",
            "--- Epoch: 544, train_obj: 0.012845336102560452\n",
            "--- Epoch: 545, train_obj: 0.012456743855164348\n",
            "--- Epoch: 546, train_obj: 0.012934047113889121\n",
            "--- Epoch: 547, train_obj: 0.012850965362356326\n",
            "--- Epoch: 548, train_obj: 0.013078429687762468\n",
            "--- Epoch: 549, train_obj: 0.013081487173360513\n",
            "--- Epoch: 550, train_obj: 0.014732797940997012\n",
            "--- Epoch: 551, train_obj: 0.012840179191816346\n",
            "--- Epoch: 552, train_obj: 0.01378311663561785\n",
            "--- Epoch: 553, train_obj: 0.011353501304308898\n",
            "--- Epoch: 554, train_obj: 0.012503354197191518\n",
            "--- Epoch: 555, train_obj: 0.01175506270502646\n",
            "--- Epoch: 556, train_obj: 0.01349716227534496\n",
            "--- Epoch: 557, train_obj: 0.012557454953495466\n",
            "--- Epoch: 558, train_obj: 0.011642088377443358\n",
            "--- Epoch: 559, train_obj: 0.013650108384799447\n",
            "--- Epoch: 560, train_obj: 0.01183326791710319\n",
            "--- Epoch: 561, train_obj: 0.01198060747978924\n",
            "--- Epoch: 562, train_obj: 0.012427775765436456\n",
            "--- Epoch: 563, train_obj: 0.011527644631953488\n",
            "--- Epoch: 564, train_obj: 0.013812407297505656\n",
            "--- Epoch: 565, train_obj: 0.012684368334327767\n",
            "--- Epoch: 566, train_obj: 0.011049126749624973\n",
            "--- Epoch: 567, train_obj: 0.012997032101567257\n",
            "--- Epoch: 568, train_obj: 0.010373986257065104\n",
            "--- Epoch: 569, train_obj: 0.011151847711578832\n",
            "--- Epoch: 570, train_obj: 0.01110290657514136\n",
            "--- Epoch: 571, train_obj: 0.012199823088790947\n",
            "--- Epoch: 572, train_obj: 0.013666095212578737\n",
            "--- Epoch: 573, train_obj: 0.011953805246420328\n",
            "--- Epoch: 574, train_obj: 0.010975708526987002\n",
            "--- Epoch: 575, train_obj: 0.012903435194482575\n",
            "--- Epoch: 576, train_obj: 0.011026591414463539\n",
            "--- Epoch: 577, train_obj: 0.011492412940067137\n",
            "--- Epoch: 578, train_obj: 0.011603030222664993\n",
            "--- Epoch: 579, train_obj: 0.01178348360226537\n",
            "--- Epoch: 580, train_obj: 0.010984924358585973\n",
            "--- Epoch: 581, train_obj: 0.012314612769805137\n",
            "--- Epoch: 582, train_obj: 0.011375570523569724\n",
            "--- Epoch: 583, train_obj: 0.010934602898217979\n",
            "--- Epoch: 584, train_obj: 0.011063073740457667\n",
            "--- Epoch: 585, train_obj: 0.010812903749108135\n",
            "--- Epoch: 586, train_obj: 0.010486225731378666\n",
            "--- Epoch: 587, train_obj: 0.01045432912671969\n",
            "--- Epoch: 588, train_obj: 0.012311394455967265\n",
            "--- Epoch: 589, train_obj: 0.011528005453521412\n",
            "--- Epoch: 590, train_obj: 0.010750098554186377\n",
            "--- Epoch: 591, train_obj: 0.012754412721205591\n",
            "--- Epoch: 592, train_obj: 0.011422806511975091\n",
            "--- Epoch: 593, train_obj: 0.012207483853924181\n",
            "--- Epoch: 594, train_obj: 0.010448216892445246\n",
            "--- Epoch: 595, train_obj: 0.01302745853096086\n",
            "--- Epoch: 596, train_obj: 0.011163216988779968\n",
            "--- Epoch: 597, train_obj: 0.010873283148938869\n",
            "--- Epoch: 598, train_obj: 0.009860459296718424\n",
            "--- Epoch: 599, train_obj: 0.011713374769476063\n",
            "train_err: 0.000021, val_err: 0.097167\n",
            "\n",
            "--- Epoch: 0, train_obj: 0.6632170864922736\n",
            "--- Epoch: 1, train_obj: 0.5682512782491698\n",
            "--- Epoch: 2, train_obj: 0.5263617740714372\n",
            "--- Epoch: 3, train_obj: 0.49726573315832434\n",
            "--- Epoch: 4, train_obj: 0.4705185652870205\n",
            "--- Epoch: 5, train_obj: 0.4504425349843963\n",
            "--- Epoch: 6, train_obj: 0.4389115097570075\n",
            "--- Epoch: 7, train_obj: 0.42695330054803593\n",
            "--- Epoch: 8, train_obj: 0.41768065702962215\n",
            "--- Epoch: 9, train_obj: 0.40644326223974325\n",
            "--- Epoch: 10, train_obj: 0.4001678791215051\n",
            "--- Epoch: 11, train_obj: 0.3928133345439756\n",
            "--- Epoch: 12, train_obj: 0.38399741223998174\n",
            "--- Epoch: 13, train_obj: 0.3754331029265583\n",
            "--- Epoch: 14, train_obj: 0.3714088647367891\n",
            "--- Epoch: 15, train_obj: 0.365497714363962\n",
            "--- Epoch: 16, train_obj: 0.3573735089624641\n",
            "--- Epoch: 17, train_obj: 0.35553349386086636\n",
            "--- Epoch: 18, train_obj: 0.34887250747701576\n",
            "--- Epoch: 19, train_obj: 0.3422617430721036\n",
            "--- Epoch: 20, train_obj: 0.34098444358100705\n",
            "--- Epoch: 21, train_obj: 0.3349644918898605\n",
            "--- Epoch: 22, train_obj: 0.33308450246611476\n",
            "--- Epoch: 23, train_obj: 0.32612845868105306\n",
            "--- Epoch: 24, train_obj: 0.3251601873404541\n",
            "--- Epoch: 25, train_obj: 0.32033639165781186\n",
            "--- Epoch: 26, train_obj: 0.31562722548779526\n",
            "--- Epoch: 27, train_obj: 0.31105787648104993\n",
            "--- Epoch: 28, train_obj: 0.3087722285884659\n",
            "--- Epoch: 29, train_obj: 0.3053362028987867\n",
            "--- Epoch: 30, train_obj: 0.30355052383801495\n",
            "--- Epoch: 31, train_obj: 0.29688526379617014\n",
            "--- Epoch: 32, train_obj: 0.29753756133504755\n",
            "--- Epoch: 33, train_obj: 0.294485352116756\n",
            "--- Epoch: 34, train_obj: 0.290204405137611\n",
            "--- Epoch: 35, train_obj: 0.2884206413321189\n",
            "--- Epoch: 36, train_obj: 0.2854199137231933\n",
            "--- Epoch: 37, train_obj: 0.28470483429373805\n",
            "--- Epoch: 38, train_obj: 0.2798346886704868\n",
            "--- Epoch: 39, train_obj: 0.2769250494212278\n",
            "--- Epoch: 40, train_obj: 0.2757184304522885\n",
            "--- Epoch: 41, train_obj: 0.2735189831518609\n",
            "--- Epoch: 42, train_obj: 0.2714697423580874\n",
            "--- Epoch: 43, train_obj: 0.2684209074727216\n",
            "--- Epoch: 44, train_obj: 0.2621587382320642\n",
            "--- Epoch: 45, train_obj: 0.2625855261489872\n",
            "--- Epoch: 46, train_obj: 0.2581806145012019\n",
            "--- Epoch: 47, train_obj: 0.25632021667821636\n",
            "--- Epoch: 48, train_obj: 0.2547527166317141\n",
            "--- Epoch: 49, train_obj: 0.25192367371684476\n",
            "--- Epoch: 50, train_obj: 0.2478094093889949\n",
            "--- Epoch: 51, train_obj: 0.24815500854199976\n",
            "--- Epoch: 52, train_obj: 0.2451995726271447\n",
            "--- Epoch: 53, train_obj: 0.24734779933261794\n",
            "--- Epoch: 54, train_obj: 0.24005946383666973\n",
            "--- Epoch: 55, train_obj: 0.24040266303958133\n",
            "--- Epoch: 56, train_obj: 0.23601351448346783\n",
            "--- Epoch: 57, train_obj: 0.23853877653550604\n",
            "--- Epoch: 58, train_obj: 0.23517303929127728\n",
            "--- Epoch: 59, train_obj: 0.2304335307331782\n",
            "--- Epoch: 60, train_obj: 0.2301704029589829\n",
            "--- Epoch: 61, train_obj: 0.22924640717339553\n",
            "--- Epoch: 62, train_obj: 0.22616502510015418\n",
            "--- Epoch: 63, train_obj: 0.2226665779456104\n",
            "--- Epoch: 64, train_obj: 0.2206596763984562\n",
            "--- Epoch: 65, train_obj: 0.2182661494438736\n",
            "--- Epoch: 66, train_obj: 0.22005861106679916\n",
            "--- Epoch: 67, train_obj: 0.21575166132280754\n",
            "--- Epoch: 68, train_obj: 0.21548062509221644\n",
            "--- Epoch: 69, train_obj: 0.21004549814779852\n",
            "--- Epoch: 70, train_obj: 0.20860589720614006\n",
            "--- Epoch: 71, train_obj: 0.21138310010308026\n",
            "--- Epoch: 72, train_obj: 0.20848954637483014\n",
            "--- Epoch: 73, train_obj: 0.20468036391210986\n",
            "--- Epoch: 74, train_obj: 0.20870243113219733\n",
            "--- Epoch: 75, train_obj: 0.20190217032764476\n",
            "--- Epoch: 76, train_obj: 0.19658333539413722\n",
            "--- Epoch: 77, train_obj: 0.19566698742227803\n",
            "--- Epoch: 78, train_obj: 0.19822215352489486\n",
            "--- Epoch: 79, train_obj: 0.19483388564265894\n",
            "--- Epoch: 80, train_obj: 0.19077448039222536\n",
            "--- Epoch: 81, train_obj: 0.19494121457934552\n",
            "--- Epoch: 82, train_obj: 0.18712636258378826\n",
            "--- Epoch: 83, train_obj: 0.18660744677812838\n",
            "--- Epoch: 84, train_obj: 0.18422669991614862\n",
            "--- Epoch: 85, train_obj: 0.18164076331065734\n",
            "--- Epoch: 86, train_obj: 0.18336099864389094\n",
            "--- Epoch: 87, train_obj: 0.17667232367655772\n",
            "--- Epoch: 88, train_obj: 0.17660382846288086\n",
            "--- Epoch: 89, train_obj: 0.17398169003670821\n",
            "--- Epoch: 90, train_obj: 0.17539243037810592\n",
            "--- Epoch: 91, train_obj: 0.172391504990187\n",
            "--- Epoch: 92, train_obj: 0.169195366162754\n",
            "--- Epoch: 93, train_obj: 0.17062942099742212\n",
            "--- Epoch: 94, train_obj: 0.16685719957880688\n",
            "--- Epoch: 95, train_obj: 0.1685335744227199\n",
            "--- Epoch: 96, train_obj: 0.16232984279279036\n",
            "--- Epoch: 97, train_obj: 0.16126007373658413\n",
            "--- Epoch: 98, train_obj: 0.15759192644420766\n",
            "--- Epoch: 99, train_obj: 0.1665614417526223\n",
            "--- Epoch: 100, train_obj: 0.16712431256579108\n",
            "--- Epoch: 101, train_obj: 0.15547550115350806\n",
            "--- Epoch: 102, train_obj: 0.1569962736420985\n",
            "--- Epoch: 103, train_obj: 0.15231848621235874\n",
            "--- Epoch: 104, train_obj: 0.15061660357281173\n",
            "--- Epoch: 105, train_obj: 0.14742850107084887\n",
            "--- Epoch: 106, train_obj: 0.14931502067608632\n",
            "--- Epoch: 107, train_obj: 0.1489415898698392\n",
            "--- Epoch: 108, train_obj: 0.14727544346514546\n",
            "--- Epoch: 109, train_obj: 0.1437614345404845\n",
            "--- Epoch: 110, train_obj: 0.13982737304108397\n",
            "--- Epoch: 111, train_obj: 0.1438445167764995\n",
            "--- Epoch: 112, train_obj: 0.13873572365108933\n",
            "--- Epoch: 113, train_obj: 0.13737351337394435\n",
            "--- Epoch: 114, train_obj: 0.1397968878873849\n",
            "--- Epoch: 115, train_obj: 0.14316731429210383\n",
            "--- Epoch: 116, train_obj: 0.13515223895891923\n",
            "--- Epoch: 117, train_obj: 0.13571411392450083\n",
            "--- Epoch: 118, train_obj: 0.13218250280800412\n",
            "--- Epoch: 119, train_obj: 0.12933251943964952\n",
            "--- Epoch: 120, train_obj: 0.13033348248848908\n",
            "--- Epoch: 121, train_obj: 0.14005711630804069\n",
            "--- Epoch: 122, train_obj: 0.1255720217555308\n",
            "--- Epoch: 123, train_obj: 0.12371670104373711\n",
            "--- Epoch: 124, train_obj: 0.1247019324075169\n",
            "--- Epoch: 125, train_obj: 0.12098096674644895\n",
            "--- Epoch: 126, train_obj: 0.11978442922265557\n",
            "--- Epoch: 127, train_obj: 0.11870095402255194\n",
            "--- Epoch: 128, train_obj: 0.12117660972168798\n",
            "--- Epoch: 129, train_obj: 0.11944973762160138\n",
            "--- Epoch: 130, train_obj: 0.11429395738364412\n",
            "--- Epoch: 131, train_obj: 0.11201511615562032\n",
            "--- Epoch: 132, train_obj: 0.11363670872495472\n",
            "--- Epoch: 133, train_obj: 0.1154796792777229\n",
            "--- Epoch: 134, train_obj: 0.12184900031709543\n",
            "--- Epoch: 135, train_obj: 0.11034842539975408\n",
            "--- Epoch: 136, train_obj: 0.10940844659321296\n",
            "--- Epoch: 137, train_obj: 0.10624216728168265\n",
            "--- Epoch: 138, train_obj: 0.10386643035569057\n",
            "--- Epoch: 139, train_obj: 0.10268023853298856\n",
            "--- Epoch: 140, train_obj: 0.10858769662166562\n",
            "--- Epoch: 141, train_obj: 0.10178024144480131\n",
            "--- Epoch: 142, train_obj: 0.10187931136646887\n",
            "--- Epoch: 143, train_obj: 0.10204773075536527\n",
            "--- Epoch: 144, train_obj: 0.0987815311391238\n",
            "--- Epoch: 145, train_obj: 0.10286812138375535\n",
            "--- Epoch: 146, train_obj: 0.09903261046158446\n",
            "--- Epoch: 147, train_obj: 0.1023714834539734\n",
            "--- Epoch: 148, train_obj: 0.0913869540152964\n",
            "--- Epoch: 149, train_obj: 0.09662175843095737\n",
            "--- Epoch: 150, train_obj: 0.08989578059199245\n",
            "--- Epoch: 151, train_obj: 0.09082009877258716\n",
            "--- Epoch: 152, train_obj: 0.08958714695658836\n",
            "--- Epoch: 153, train_obj: 0.08969987704331048\n",
            "--- Epoch: 154, train_obj: 0.08923482916463515\n",
            "--- Epoch: 155, train_obj: 0.08787554092093372\n",
            "--- Epoch: 156, train_obj: 0.09035241678218585\n",
            "--- Epoch: 157, train_obj: 0.08743706756141165\n",
            "--- Epoch: 158, train_obj: 0.08993894646180299\n",
            "--- Epoch: 159, train_obj: 0.08563475678745108\n",
            "--- Epoch: 160, train_obj: 0.08044607183847863\n",
            "--- Epoch: 161, train_obj: 0.08252849111746614\n",
            "--- Epoch: 162, train_obj: 0.08217704061199407\n",
            "--- Epoch: 163, train_obj: 0.08728157324895028\n",
            "--- Epoch: 164, train_obj: 0.07851236380912702\n",
            "--- Epoch: 165, train_obj: 0.08167592205860166\n",
            "--- Epoch: 166, train_obj: 0.07959139003698082\n",
            "--- Epoch: 167, train_obj: 0.0756918358751573\n",
            "--- Epoch: 168, train_obj: 0.0776121955652954\n",
            "--- Epoch: 169, train_obj: 0.07813537215637469\n",
            "--- Epoch: 170, train_obj: 0.07346524950534294\n",
            "--- Epoch: 171, train_obj: 0.07303816633973381\n",
            "--- Epoch: 172, train_obj: 0.07792465757191928\n",
            "--- Epoch: 173, train_obj: 0.07192665271560376\n",
            "--- Epoch: 174, train_obj: 0.06901233423768137\n",
            "--- Epoch: 175, train_obj: 0.06867936591022114\n",
            "--- Epoch: 176, train_obj: 0.06895211889607797\n",
            "--- Epoch: 177, train_obj: 0.06783839247812383\n",
            "--- Epoch: 178, train_obj: 0.06887269041536155\n",
            "--- Epoch: 179, train_obj: 0.06746686542422893\n",
            "--- Epoch: 180, train_obj: 0.066886455185875\n",
            "--- Epoch: 181, train_obj: 0.06842088856689572\n",
            "--- Epoch: 182, train_obj: 0.064971620220163\n",
            "--- Epoch: 183, train_obj: 0.062107905972494326\n",
            "--- Epoch: 184, train_obj: 0.06153268784557561\n",
            "--- Epoch: 185, train_obj: 0.0644116339266292\n",
            "--- Epoch: 186, train_obj: 0.06180561855179423\n",
            "--- Epoch: 187, train_obj: 0.0601102899899574\n",
            "--- Epoch: 188, train_obj: 0.05914061255383482\n",
            "--- Epoch: 189, train_obj: 0.05718993864869242\n",
            "--- Epoch: 190, train_obj: 0.05941846420555894\n",
            "--- Epoch: 191, train_obj: 0.060086609547175554\n",
            "--- Epoch: 192, train_obj: 0.05871175571054371\n",
            "--- Epoch: 193, train_obj: 0.05370288049882672\n",
            "--- Epoch: 194, train_obj: 0.059685252313773425\n",
            "--- Epoch: 195, train_obj: 0.056956206039439704\n",
            "--- Epoch: 196, train_obj: 0.05484224582107269\n",
            "--- Epoch: 197, train_obj: 0.05393631889811862\n",
            "--- Epoch: 198, train_obj: 0.05587172094222551\n",
            "--- Epoch: 199, train_obj: 0.055607818170429986\n",
            "--- Epoch: 200, train_obj: 0.055694586281649854\n",
            "--- Epoch: 201, train_obj: 0.05374233001257944\n",
            "--- Epoch: 202, train_obj: 0.05147408576993406\n",
            "--- Epoch: 203, train_obj: 0.05023701593244502\n",
            "--- Epoch: 204, train_obj: 0.05063328119290506\n",
            "--- Epoch: 205, train_obj: 0.04813349139295016\n",
            "--- Epoch: 206, train_obj: 0.0564025938540606\n",
            "--- Epoch: 207, train_obj: 0.04954307897276148\n",
            "--- Epoch: 208, train_obj: 0.052481470012417174\n",
            "--- Epoch: 209, train_obj: 0.0468848677870051\n",
            "--- Epoch: 210, train_obj: 0.053792224310903934\n",
            "--- Epoch: 211, train_obj: 0.05614514370245132\n",
            "--- Epoch: 212, train_obj: 0.04759862557587537\n",
            "--- Epoch: 213, train_obj: 0.046038143270880454\n",
            "--- Epoch: 214, train_obj: 0.047433319178603967\n",
            "--- Epoch: 215, train_obj: 0.043947257109035784\n",
            "--- Epoch: 216, train_obj: 0.045487424559072544\n",
            "--- Epoch: 217, train_obj: 0.04539828563488969\n",
            "--- Epoch: 218, train_obj: 0.04496461444587725\n",
            "--- Epoch: 219, train_obj: 0.04189954930609654\n",
            "--- Epoch: 220, train_obj: 0.04265094376078377\n",
            "--- Epoch: 221, train_obj: 0.04625263510531142\n",
            "--- Epoch: 222, train_obj: 0.041617099070731314\n",
            "--- Epoch: 223, train_obj: 0.04605428646026096\n",
            "--- Epoch: 224, train_obj: 0.04176604206868782\n",
            "--- Epoch: 225, train_obj: 0.03949548399828868\n",
            "--- Epoch: 226, train_obj: 0.04079623763570016\n",
            "--- Epoch: 227, train_obj: 0.042332054710030806\n",
            "--- Epoch: 228, train_obj: 0.04107535863715738\n",
            "--- Epoch: 229, train_obj: 0.04217285517855926\n",
            "--- Epoch: 230, train_obj: 0.03692928018646193\n",
            "--- Epoch: 231, train_obj: 0.03611855333289121\n",
            "--- Epoch: 232, train_obj: 0.037817957204497724\n",
            "--- Epoch: 233, train_obj: 0.038002335657213976\n",
            "--- Epoch: 234, train_obj: 0.03651782138563936\n",
            "--- Epoch: 235, train_obj: 0.03886269467928363\n",
            "--- Epoch: 236, train_obj: 0.03661541750575629\n",
            "--- Epoch: 237, train_obj: 0.03495459388747797\n",
            "--- Epoch: 238, train_obj: 0.03352444903857958\n",
            "--- Epoch: 239, train_obj: 0.03422509606888633\n",
            "--- Epoch: 240, train_obj: 0.03770300833650669\n",
            "--- Epoch: 241, train_obj: 0.0343785172598045\n",
            "--- Epoch: 242, train_obj: 0.03581200216004799\n",
            "--- Epoch: 243, train_obj: 0.03368724844290156\n",
            "--- Epoch: 244, train_obj: 0.03417971199511158\n",
            "--- Epoch: 245, train_obj: 0.031674793663201084\n",
            "--- Epoch: 246, train_obj: 0.030263616389109762\n",
            "--- Epoch: 247, train_obj: 0.03173242162026757\n",
            "--- Epoch: 248, train_obj: 0.03274005160013658\n",
            "--- Epoch: 249, train_obj: 0.033858692051827934\n",
            "--- Epoch: 250, train_obj: 0.03247866500792181\n",
            "--- Epoch: 251, train_obj: 0.031626765882453875\n",
            "--- Epoch: 252, train_obj: 0.028589900954502312\n",
            "--- Epoch: 253, train_obj: 0.033863320992984135\n",
            "--- Epoch: 254, train_obj: 0.031018091033746066\n",
            "--- Epoch: 255, train_obj: 0.028135601091530234\n",
            "--- Epoch: 256, train_obj: 0.031686095385656636\n",
            "--- Epoch: 257, train_obj: 0.029304834257937857\n",
            "--- Epoch: 258, train_obj: 0.029121630368259224\n",
            "--- Epoch: 259, train_obj: 0.03213332416942808\n",
            "--- Epoch: 260, train_obj: 0.028834939740127102\n",
            "--- Epoch: 261, train_obj: 0.02666419465592356\n",
            "--- Epoch: 262, train_obj: 0.034339725883282636\n",
            "--- Epoch: 263, train_obj: 0.03893027403220495\n",
            "--- Epoch: 264, train_obj: 0.029743696514471334\n",
            "--- Epoch: 265, train_obj: 0.026352888308156793\n",
            "--- Epoch: 266, train_obj: 0.02783427726513798\n",
            "--- Epoch: 267, train_obj: 0.025531275130888226\n",
            "--- Epoch: 268, train_obj: 0.02661439370199267\n",
            "--- Epoch: 269, train_obj: 0.026105106126760896\n",
            "--- Epoch: 270, train_obj: 0.024828820792869313\n",
            "--- Epoch: 271, train_obj: 0.027304143302812132\n",
            "--- Epoch: 272, train_obj: 0.03152264790632132\n",
            "--- Epoch: 273, train_obj: 0.025073937841960887\n",
            "--- Epoch: 274, train_obj: 0.023822771207502123\n",
            "--- Epoch: 275, train_obj: 0.03167894728423146\n",
            "--- Epoch: 276, train_obj: 0.022874685862066816\n",
            "--- Epoch: 277, train_obj: 0.02393978354375723\n",
            "--- Epoch: 278, train_obj: 0.023443078867243127\n",
            "--- Epoch: 279, train_obj: 0.024322920746211764\n",
            "--- Epoch: 280, train_obj: 0.02166452628351072\n",
            "--- Epoch: 281, train_obj: 0.027372985678737763\n",
            "--- Epoch: 282, train_obj: 0.022719003002588996\n",
            "--- Epoch: 283, train_obj: 0.029380788001573316\n",
            "--- Epoch: 284, train_obj: 0.021591247809652894\n",
            "--- Epoch: 285, train_obj: 0.021118993240389846\n",
            "--- Epoch: 286, train_obj: 0.026177820654592912\n",
            "--- Epoch: 287, train_obj: 0.02430938904718184\n",
            "--- Epoch: 288, train_obj: 0.02379939629177264\n",
            "--- Epoch: 289, train_obj: 0.02124873626504639\n",
            "--- Epoch: 290, train_obj: 0.02354734452608906\n",
            "--- Epoch: 291, train_obj: 0.023067370979373093\n",
            "--- Epoch: 292, train_obj: 0.025674829756194485\n",
            "--- Epoch: 293, train_obj: 0.024172719896901507\n",
            "--- Epoch: 294, train_obj: 0.022903548781282975\n",
            "--- Epoch: 295, train_obj: 0.018879539553235118\n",
            "--- Epoch: 296, train_obj: 0.020986436670091944\n",
            "--- Epoch: 297, train_obj: 0.020810018593609286\n",
            "--- Epoch: 298, train_obj: 0.0211558290534637\n",
            "--- Epoch: 299, train_obj: 0.020402078441892076\n",
            "--- Epoch: 300, train_obj: 0.020205501267446597\n",
            "--- Epoch: 301, train_obj: 0.02001579008012833\n",
            "--- Epoch: 302, train_obj: 0.01696931827318566\n",
            "--- Epoch: 303, train_obj: 0.021790341731802096\n",
            "--- Epoch: 304, train_obj: 0.020770225982315232\n",
            "--- Epoch: 305, train_obj: 0.017855830102471\n",
            "--- Epoch: 306, train_obj: 0.018948287295466035\n",
            "--- Epoch: 307, train_obj: 0.017130276789867182\n",
            "--- Epoch: 308, train_obj: 0.023528157858962298\n",
            "--- Epoch: 309, train_obj: 0.01962524842983687\n",
            "--- Epoch: 310, train_obj: 0.016160524352448063\n",
            "--- Epoch: 311, train_obj: 0.018614553508792105\n",
            "--- Epoch: 312, train_obj: 0.01896241882492794\n",
            "--- Epoch: 313, train_obj: 0.01855625577070111\n",
            "--- Epoch: 314, train_obj: 0.0183074424858393\n",
            "--- Epoch: 315, train_obj: 0.017018274799626035\n",
            "--- Epoch: 316, train_obj: 0.01816296791472437\n",
            "--- Epoch: 317, train_obj: 0.016157615374386106\n",
            "--- Epoch: 318, train_obj: 0.021356363263082486\n",
            "--- Epoch: 319, train_obj: 0.02022530573519181\n",
            "--- Epoch: 320, train_obj: 0.018485720602066165\n",
            "--- Epoch: 321, train_obj: 0.015187227612759131\n",
            "--- Epoch: 322, train_obj: 0.016719790334095062\n",
            "--- Epoch: 323, train_obj: 0.017659942545043546\n",
            "--- Epoch: 324, train_obj: 0.01487424752943136\n",
            "--- Epoch: 325, train_obj: 0.01807165916336819\n",
            "--- Epoch: 326, train_obj: 0.017613320579070845\n",
            "--- Epoch: 327, train_obj: 0.01708650664785794\n",
            "--- Epoch: 328, train_obj: 0.015713159896492163\n",
            "--- Epoch: 329, train_obj: 0.015239267422093136\n",
            "--- Epoch: 330, train_obj: 0.015479357586874349\n",
            "--- Epoch: 331, train_obj: 0.015287000164966586\n",
            "--- Epoch: 332, train_obj: 0.01786205986578716\n",
            "--- Epoch: 333, train_obj: 0.01619878287684854\n",
            "--- Epoch: 334, train_obj: 0.015571622609853283\n",
            "--- Epoch: 335, train_obj: 0.013696794810100069\n",
            "--- Epoch: 336, train_obj: 0.015105868864133148\n",
            "--- Epoch: 337, train_obj: 0.01453613976044609\n",
            "--- Epoch: 338, train_obj: 0.013457770703956765\n",
            "--- Epoch: 339, train_obj: 0.017024601681388366\n",
            "--- Epoch: 340, train_obj: 0.014937861246450024\n",
            "--- Epoch: 341, train_obj: 0.014627796221027028\n",
            "--- Epoch: 342, train_obj: 0.013485850952111382\n",
            "--- Epoch: 343, train_obj: 0.014982506318544102\n",
            "--- Epoch: 344, train_obj: 0.017767452848456947\n",
            "--- Epoch: 345, train_obj: 0.015052896955509624\n",
            "--- Epoch: 346, train_obj: 0.015111720216767845\n",
            "--- Epoch: 347, train_obj: 0.01410093905451011\n",
            "--- Epoch: 348, train_obj: 0.013779417944219996\n",
            "--- Epoch: 349, train_obj: 0.017597011206365466\n",
            "--- Epoch: 350, train_obj: 0.015798671650660453\n",
            "--- Epoch: 351, train_obj: 0.012172889064848627\n",
            "--- Epoch: 352, train_obj: 0.013378338124799544\n",
            "--- Epoch: 353, train_obj: 0.016258692245107485\n",
            "--- Epoch: 354, train_obj: 0.0134004919801419\n",
            "--- Epoch: 355, train_obj: 0.01407009268895222\n",
            "--- Epoch: 356, train_obj: 0.01478200384646489\n",
            "--- Epoch: 357, train_obj: 0.01393300953223154\n",
            "--- Epoch: 358, train_obj: 0.012434272487173506\n",
            "--- Epoch: 359, train_obj: 0.011709654562504663\n",
            "--- Epoch: 360, train_obj: 0.013370792337146926\n",
            "--- Epoch: 361, train_obj: 0.014710915495350712\n",
            "--- Epoch: 362, train_obj: 0.014186777075362316\n",
            "--- Epoch: 363, train_obj: 0.01300919121813464\n",
            "--- Epoch: 364, train_obj: 0.011406999462833028\n",
            "--- Epoch: 365, train_obj: 0.012163257355728168\n",
            "--- Epoch: 366, train_obj: 0.016019426251048494\n",
            "--- Epoch: 367, train_obj: 0.011557492448761812\n",
            "--- Epoch: 368, train_obj: 0.01107786705238046\n",
            "--- Epoch: 369, train_obj: 0.012405963387473577\n",
            "--- Epoch: 370, train_obj: 0.013039268118826297\n",
            "--- Epoch: 371, train_obj: 0.011972097371936191\n",
            "--- Epoch: 372, train_obj: 0.015825387128347246\n",
            "--- Epoch: 373, train_obj: 0.01384058023891437\n",
            "--- Epoch: 374, train_obj: 0.0120511085052227\n",
            "--- Epoch: 375, train_obj: 0.010796952071981895\n",
            "--- Epoch: 376, train_obj: 0.012170200410750188\n",
            "--- Epoch: 377, train_obj: 0.012164176433172462\n",
            "--- Epoch: 378, train_obj: 0.012171150632603676\n",
            "--- Epoch: 379, train_obj: 0.012065751008818194\n",
            "--- Epoch: 380, train_obj: 0.015405356961536281\n",
            "--- Epoch: 381, train_obj: 0.01577478422853618\n",
            "--- Epoch: 382, train_obj: 0.011428282799063723\n",
            "--- Epoch: 383, train_obj: 0.01205486867068203\n",
            "--- Epoch: 384, train_obj: 0.010371957799014496\n",
            "--- Epoch: 385, train_obj: 0.01981336893217801\n",
            "--- Epoch: 386, train_obj: 0.01749147803026406\n",
            "--- Epoch: 387, train_obj: 0.011781791025398329\n",
            "--- Epoch: 388, train_obj: 0.009244585870865555\n",
            "--- Epoch: 389, train_obj: 0.010867698325614088\n",
            "--- Epoch: 390, train_obj: 0.009833044421059483\n",
            "--- Epoch: 391, train_obj: 0.010768529035493276\n",
            "--- Epoch: 392, train_obj: 0.009841465129909325\n",
            "--- Epoch: 393, train_obj: 0.009167529621225955\n",
            "--- Epoch: 394, train_obj: 0.010196085909047484\n",
            "--- Epoch: 395, train_obj: 0.009254668594044212\n",
            "--- Epoch: 396, train_obj: 0.01033504198756374\n",
            "--- Epoch: 397, train_obj: 0.009634174420216685\n",
            "--- Epoch: 398, train_obj: 0.010247243158985678\n",
            "--- Epoch: 399, train_obj: 0.011729971988986183\n",
            "--- Epoch: 400, train_obj: 0.010803183893100131\n",
            "--- Epoch: 401, train_obj: 0.00969604792605707\n",
            "--- Epoch: 402, train_obj: 0.01154213080950769\n",
            "--- Epoch: 403, train_obj: 0.008648428867034172\n",
            "--- Epoch: 404, train_obj: 0.009470206366207417\n",
            "--- Epoch: 405, train_obj: 0.009987891470700738\n",
            "--- Epoch: 406, train_obj: 0.011337315662869541\n",
            "--- Epoch: 407, train_obj: 0.009352571812527936\n",
            "--- Epoch: 408, train_obj: 0.010655374441026795\n",
            "--- Epoch: 409, train_obj: 0.00857524074938252\n",
            "--- Epoch: 410, train_obj: 0.00879688968380545\n",
            "--- Epoch: 411, train_obj: 0.00911681878989287\n",
            "--- Epoch: 412, train_obj: 0.009125683060887173\n",
            "--- Epoch: 413, train_obj: 0.0084247325710823\n",
            "--- Epoch: 414, train_obj: 0.009863045307321874\n",
            "--- Epoch: 415, train_obj: 0.009650732673397274\n",
            "--- Epoch: 416, train_obj: 0.009169256256838421\n",
            "--- Epoch: 417, train_obj: 0.008177027648495419\n",
            "--- Epoch: 418, train_obj: 0.010355586797105463\n",
            "--- Epoch: 419, train_obj: 0.008977859970996873\n",
            "--- Epoch: 420, train_obj: 0.009132976938384435\n",
            "--- Epoch: 421, train_obj: 0.009549694006934978\n",
            "--- Epoch: 422, train_obj: 0.008762680651192355\n",
            "--- Epoch: 423, train_obj: 0.007933660760861555\n",
            "--- Epoch: 424, train_obj: 0.008843550866665162\n",
            "--- Epoch: 425, train_obj: 0.009447657744003624\n",
            "--- Epoch: 426, train_obj: 0.009920395737880347\n",
            "--- Epoch: 427, train_obj: 0.009986987167168056\n",
            "--- Epoch: 428, train_obj: 0.009640506176577853\n",
            "--- Epoch: 429, train_obj: 0.00816340532567482\n",
            "--- Epoch: 430, train_obj: 0.009935801996881597\n",
            "--- Epoch: 431, train_obj: 0.008199912502481255\n",
            "--- Epoch: 432, train_obj: 0.0074667075543839185\n",
            "--- Epoch: 433, train_obj: 0.009349118699402074\n",
            "--- Epoch: 434, train_obj: 0.0071169266441048675\n",
            "--- Epoch: 435, train_obj: 0.007813390301223673\n",
            "--- Epoch: 436, train_obj: 0.008269360722757324\n",
            "--- Epoch: 437, train_obj: 0.009490645986842414\n",
            "--- Epoch: 438, train_obj: 0.007296353014398437\n",
            "--- Epoch: 439, train_obj: 0.00812324556593388\n",
            "--- Epoch: 440, train_obj: 0.00948521925600154\n",
            "--- Epoch: 441, train_obj: 0.007515316079608963\n",
            "--- Epoch: 442, train_obj: 0.00787777664891273\n",
            "--- Epoch: 443, train_obj: 0.007957137413735587\n",
            "--- Epoch: 444, train_obj: 0.008033006611748406\n",
            "--- Epoch: 445, train_obj: 0.008028058384365834\n",
            "--- Epoch: 446, train_obj: 0.0076234150807052\n",
            "--- Epoch: 447, train_obj: 0.007357373517040397\n",
            "--- Epoch: 448, train_obj: 0.008037881233658315\n",
            "--- Epoch: 449, train_obj: 0.007630812523236604\n",
            "--- Epoch: 450, train_obj: 0.0064187716127743774\n",
            "--- Epoch: 451, train_obj: 0.011208378233633185\n",
            "--- Epoch: 452, train_obj: 0.008107782918625501\n",
            "--- Epoch: 453, train_obj: 0.008185879639789605\n",
            "--- Epoch: 454, train_obj: 0.009163078875321156\n",
            "--- Epoch: 455, train_obj: 0.0066424917654385445\n",
            "--- Epoch: 456, train_obj: 0.008081615092215527\n",
            "--- Epoch: 457, train_obj: 0.008536172813178112\n",
            "--- Epoch: 458, train_obj: 0.006160385046895653\n",
            "--- Epoch: 459, train_obj: 0.0069146057396696926\n",
            "--- Epoch: 460, train_obj: 0.007670344135840227\n",
            "--- Epoch: 461, train_obj: 0.0076428547006556265\n",
            "--- Epoch: 462, train_obj: 0.008066251379559766\n",
            "--- Epoch: 463, train_obj: 0.007707514068145019\n",
            "--- Epoch: 464, train_obj: 0.008127683454686367\n",
            "--- Epoch: 465, train_obj: 0.006809514590596896\n",
            "--- Epoch: 466, train_obj: 0.00997964692493683\n",
            "--- Epoch: 467, train_obj: 0.008469426658869609\n",
            "--- Epoch: 468, train_obj: 0.00799283862672944\n",
            "--- Epoch: 469, train_obj: 0.00829106083304447\n",
            "--- Epoch: 470, train_obj: 0.005884874341479512\n",
            "--- Epoch: 471, train_obj: 0.006260659145012146\n",
            "--- Epoch: 472, train_obj: 0.00998621393600774\n",
            "--- Epoch: 473, train_obj: 0.006345901419565709\n",
            "--- Epoch: 474, train_obj: 0.007497427744113321\n",
            "--- Epoch: 475, train_obj: 0.006060500636239605\n",
            "--- Epoch: 476, train_obj: 0.005757390299766192\n",
            "--- Epoch: 477, train_obj: 0.007102402277858625\n",
            "--- Epoch: 478, train_obj: 0.00687354940761542\n",
            "--- Epoch: 479, train_obj: 0.007259007338182729\n",
            "--- Epoch: 480, train_obj: 0.0074695002140805575\n",
            "--- Epoch: 481, train_obj: 0.007117377970312496\n",
            "--- Epoch: 482, train_obj: 0.005450788046822155\n",
            "--- Epoch: 483, train_obj: 0.006148167251604669\n",
            "--- Epoch: 484, train_obj: 0.006668004539908207\n",
            "--- Epoch: 485, train_obj: 0.007177552614918117\n",
            "--- Epoch: 486, train_obj: 0.0061740958150919185\n",
            "--- Epoch: 487, train_obj: 0.005935705704784534\n",
            "--- Epoch: 488, train_obj: 0.0064187578273798875\n",
            "--- Epoch: 489, train_obj: 0.005366268471689005\n",
            "--- Epoch: 490, train_obj: 0.00723561497515196\n",
            "--- Epoch: 491, train_obj: 0.006533238179104399\n",
            "--- Epoch: 492, train_obj: 0.005410782494601095\n",
            "--- Epoch: 493, train_obj: 0.0064108339915302645\n",
            "--- Epoch: 494, train_obj: 0.0064612259813184355\n",
            "--- Epoch: 495, train_obj: 0.007100018704135578\n",
            "--- Epoch: 496, train_obj: 0.005140145799487429\n",
            "--- Epoch: 497, train_obj: 0.00652544940265064\n",
            "--- Epoch: 498, train_obj: 0.007484577938665933\n",
            "--- Epoch: 499, train_obj: 0.0060237628239812185\n",
            "--- Epoch: 500, train_obj: 0.0061016978085751\n",
            "--- Epoch: 501, train_obj: 0.006429746883587908\n",
            "--- Epoch: 502, train_obj: 0.005566768597355542\n",
            "--- Epoch: 503, train_obj: 0.005917389112345196\n",
            "--- Epoch: 504, train_obj: 0.005353004479645537\n",
            "--- Epoch: 505, train_obj: 0.005684248657869011\n",
            "--- Epoch: 506, train_obj: 0.005547262591317301\n",
            "--- Epoch: 507, train_obj: 0.006566373559258536\n",
            "--- Epoch: 508, train_obj: 0.006769808044020815\n",
            "--- Epoch: 509, train_obj: 0.006261220552315375\n",
            "--- Epoch: 510, train_obj: 0.004938744946779131\n",
            "--- Epoch: 511, train_obj: 0.009358810908718448\n",
            "--- Epoch: 512, train_obj: 0.0074720712977657915\n",
            "--- Epoch: 513, train_obj: 0.005599533236266839\n",
            "--- Epoch: 514, train_obj: 0.005731647214343353\n",
            "--- Epoch: 515, train_obj: 0.005435253343342944\n",
            "--- Epoch: 516, train_obj: 0.005648941126454093\n",
            "--- Epoch: 517, train_obj: 0.005737429869606452\n",
            "--- Epoch: 518, train_obj: 0.0058987112842025765\n",
            "--- Epoch: 519, train_obj: 0.00603000662139904\n",
            "--- Epoch: 520, train_obj: 0.006610980494778552\n",
            "--- Epoch: 521, train_obj: 0.005942357470899665\n",
            "--- Epoch: 522, train_obj: 0.004892923997024246\n",
            "--- Epoch: 523, train_obj: 0.005340922222546882\n",
            "--- Epoch: 524, train_obj: 0.006457792410039318\n",
            "--- Epoch: 525, train_obj: 0.0065826475095515265\n",
            "--- Epoch: 526, train_obj: 0.005189612752528312\n",
            "--- Epoch: 527, train_obj: 0.007185249949075647\n",
            "--- Epoch: 528, train_obj: 0.005738794652680319\n",
            "--- Epoch: 529, train_obj: 0.005563549825315131\n",
            "--- Epoch: 530, train_obj: 0.005089796923890216\n",
            "--- Epoch: 531, train_obj: 0.005178795792263775\n",
            "--- Epoch: 532, train_obj: 0.005370665062599946\n",
            "--- Epoch: 533, train_obj: 0.005711103670755404\n",
            "--- Epoch: 534, train_obj: 0.005135042684886385\n",
            "--- Epoch: 535, train_obj: 0.006285229661784937\n",
            "--- Epoch: 536, train_obj: 0.0064842186942133985\n",
            "--- Epoch: 537, train_obj: 0.006468679462555809\n",
            "--- Epoch: 538, train_obj: 0.0056160209048413245\n",
            "--- Epoch: 539, train_obj: 0.005681887643369464\n",
            "--- Epoch: 540, train_obj: 0.005147867356055054\n",
            "--- Epoch: 541, train_obj: 0.004758405230717485\n",
            "--- Epoch: 542, train_obj: 0.005490185491720127\n",
            "--- Epoch: 543, train_obj: 0.005921562979205685\n",
            "--- Epoch: 544, train_obj: 0.00568250271945796\n",
            "--- Epoch: 545, train_obj: 0.004890757122890912\n",
            "--- Epoch: 546, train_obj: 0.004986463507665046\n",
            "--- Epoch: 547, train_obj: 0.005853073649220583\n",
            "--- Epoch: 548, train_obj: 0.004846159114333944\n",
            "--- Epoch: 549, train_obj: 0.004930774235392352\n",
            "--- Epoch: 550, train_obj: 0.004548968397805516\n",
            "--- Epoch: 551, train_obj: 0.005070276561170804\n",
            "--- Epoch: 552, train_obj: 0.005412578839069873\n",
            "--- Epoch: 553, train_obj: 0.004575894247348194\n",
            "--- Epoch: 554, train_obj: 0.004433710815701973\n",
            "--- Epoch: 555, train_obj: 0.004484562978833138\n",
            "--- Epoch: 556, train_obj: 0.004389764665336479\n",
            "--- Epoch: 557, train_obj: 0.005377765191398814\n",
            "--- Epoch: 558, train_obj: 0.004709760708298561\n",
            "--- Epoch: 559, train_obj: 0.004731731468411636\n",
            "--- Epoch: 560, train_obj: 0.00452449957465333\n",
            "--- Epoch: 561, train_obj: 0.005103195997081233\n",
            "--- Epoch: 562, train_obj: 0.005062063506208368\n",
            "--- Epoch: 563, train_obj: 0.006776839444208759\n",
            "--- Epoch: 564, train_obj: 0.004194422568751117\n",
            "--- Epoch: 565, train_obj: 0.004488619800955074\n",
            "--- Epoch: 566, train_obj: 0.003860293299067468\n",
            "--- Epoch: 567, train_obj: 0.004253046213277485\n",
            "--- Epoch: 568, train_obj: 0.005448986397488171\n",
            "--- Epoch: 569, train_obj: 0.004332726410857643\n",
            "--- Epoch: 570, train_obj: 0.005309795524825184\n",
            "--- Epoch: 571, train_obj: 0.009462937827382781\n",
            "--- Epoch: 572, train_obj: 0.005963447753429658\n",
            "--- Epoch: 573, train_obj: 0.005576182410268295\n",
            "--- Epoch: 574, train_obj: 0.007490694923401738\n",
            "--- Epoch: 575, train_obj: 0.004244287391369919\n",
            "--- Epoch: 576, train_obj: 0.004690856994487488\n",
            "--- Epoch: 577, train_obj: 0.005075646046033461\n",
            "--- Epoch: 578, train_obj: 0.005247205632551671\n",
            "--- Epoch: 579, train_obj: 0.004762777846298972\n",
            "--- Epoch: 580, train_obj: 0.006715823642194599\n",
            "--- Epoch: 581, train_obj: 0.005079541269119341\n",
            "--- Epoch: 582, train_obj: 0.004207138757629452\n",
            "--- Epoch: 583, train_obj: 0.0043455852248476\n",
            "--- Epoch: 584, train_obj: 0.004697254967646279\n",
            "--- Epoch: 585, train_obj: 0.005133550080547163\n",
            "--- Epoch: 586, train_obj: 0.004888394442034039\n",
            "--- Epoch: 587, train_obj: 0.005090325680171317\n",
            "--- Epoch: 588, train_obj: 0.0046259684213008296\n",
            "--- Epoch: 589, train_obj: 0.0061952794727615085\n",
            "--- Epoch: 590, train_obj: 0.0046613274443773455\n",
            "--- Epoch: 591, train_obj: 0.0039978687327304\n",
            "--- Epoch: 592, train_obj: 0.004167161730927089\n",
            "--- Epoch: 593, train_obj: 0.00464704859111308\n",
            "--- Epoch: 594, train_obj: 0.005161874190780943\n",
            "--- Epoch: 595, train_obj: 0.004208103226391434\n",
            "--- Epoch: 596, train_obj: 0.0035006227152648193\n",
            "--- Epoch: 597, train_obj: 0.00517203022003154\n",
            "--- Epoch: 598, train_obj: 0.005433969423083367\n",
            "--- Epoch: 599, train_obj: 0.004682765454133873\n",
            "train_err: 0.000000, val_err: 0.095917\n"
          ]
        }
      ],
      "source": [
        "train_data = np.load('fmnist_train.npy', allow_pickle=True).item()\n",
        "test_data = np.load('fmnist_test.npy', allow_pickle=True).item()\n",
        "\n",
        "X = train_data['data']\n",
        "y = train_data['labels']\n",
        "X_test = test_data['data']\n",
        "\n",
        "# Preprocessing X\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "if X.max() > 1: X = X / 255.\n",
        "\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "if X_test.max() > 1: X_test = X_test / 255.\n",
        "\n",
        "# Split into Xfm_train, yfm_train, Xfm_val, yfm_val\n",
        "Xfm_train, yfm_train, Xfm_val, yfm_val = utils.create_split(X, y, 0.8)\n",
        "\n",
        "models = [\n",
        "    Sequential([\n",
        "        Linear(Xfm_train.shape[1], 400),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(400, 10)\n",
        "    ]),\n",
        "    Sequential([\n",
        "        Linear(Xfm_train.shape[1], 2000),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(2000, 10)\n",
        "    ]),\n",
        "    Sequential([\n",
        "        Linear(Xfm_train.shape[1], 1000),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(1000, 800),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(800, 10)\n",
        "    ]),\n",
        "    Sequential([\n",
        "        Linear(Xfm_train.shape[1], 1000),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(1000, 800),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(800, 400),\n",
        "        ReLU(),\n",
        "        Dropout(0.1),\n",
        "        Linear(400, 10)\n",
        "    ])\n",
        "]\n",
        "best_clf = None\n",
        "best_val_err = np.inf\n",
        "for model in models:\n",
        "    loss = MultiLogisticLoss(k=10)\n",
        "    # An example of a sequential network\n",
        "    clf = ERMNeuralNetClassifier(model, loss)\n",
        "\n",
        "    sgd_kwargs = {\n",
        "        'batch_size': 128,\n",
        "        'n_epochs': 600,\n",
        "        'eta': 0.01,\n",
        "        'verbose': True, # Enable printing INSIDE SGD\n",
        "        'verbose_epoch_interval': 1,\n",
        "    }\n",
        "\n",
        "    clf.fit(Xfm_train, yfm_train, **sgd_kwargs)\n",
        "\n",
        "    yfm_train_pred = clf.predict(Xfm_train)\n",
        "    train_err = utils.empirical_err(yfm_train, yfm_train_pred)\n",
        "\n",
        "    yfm_val_pred = clf.predict(Xfm_val)\n",
        "    val_err = utils.empirical_err(yfm_val, yfm_val_pred)\n",
        "\n",
        "    print(f'train_err: {train_err:5f}, val_err: {val_err:5f}')\n",
        "    if val_err < best_val_err:\n",
        "        best_clf = clf\n",
        "\n",
        "#### TASK 6 CODE\n",
        "y_test_preds = best_clf.predict(X_test)\n",
        "#### TASK 6 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d74c953",
      "metadata": {
        "id": "0d74c953"
      },
      "source": [
        "#### Package output for uploading to Kaggle\n",
        "\n",
        "In order to upload your results to Kaggle, load the test data, use your model to predict outputs, and then write the outputs to a csv file. You will upload the csv file stored at `fname` to the Kaggle competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "269d57de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.Sequential object at 0x12cf7d6a0>\n"
          ]
        }
      ],
      "source": [
        "print(best_clf.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "87c64ee6",
      "metadata": {
        "id": "87c64ee6"
      },
      "outputs": [],
      "source": [
        "# Save the CSV file of labels. UPLOAD THIS FILE\n",
        "fname = 'fmnist_test_pred.csv'\n",
        "output = np.vstack((np.arange(y_test_preds.shape[0]), y_test_preds)).T\n",
        "np.savetxt(fname, output, fmt=\"%d\", delimiter=',', comments='', header='id,label')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c977b9a",
      "metadata": {
        "id": "9c977b9a"
      },
      "source": [
        "## [OPTIONAL] Extension: Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9382f645",
      "metadata": {
        "id": "9382f645"
      },
      "source": [
        "In practice, fully-connected neural networks are hard to scale to images -- the number of units required grows significantly. However, in images there is a lot of local structure that we can exploit, in particular by focusing on local features and sharing weights for features derived from different parts of the image. Another way to think about this is as passing a convolutional filter over the whole image. This is similar to what we did in the Viola-Jones face detector, except that in this case, we will learn the parameter values (and therefore the filter itself) rather than pre-setting them. We did not cover convolutional neural networks in a lot of detail this quarter, but you can look at [this course](https://cs231n.github.io/) and in particular [this set of notes](https://cs231n.github.io/convolutional-networks/) for a good introduction to the topic.\n",
        "\n",
        "To implement Neural Networks, we could take the approach that we did in the previous part of the homework, and build them ourselves from ground up. However, the implementation above is not very optimized, and we cannot run it on special hardware. To address this, there are several available libraries for building, training, and even deploying deep learning models. Of these, [Pytorch](https://pytorch.org/) is a very commonly used one. We'll give an example of how to load data from a pre-loaded dataset, how to set up a convolutional neural network, and how to train it. Feel free to experiment with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01201d88-4477-41ef-b820-da90a0e82dd8",
      "metadata": {
        "id": "01201d88-4477-41ef-b820-da90a0e82dd8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c274eff2-e141-4b48-94b0-97f1140523fc",
      "metadata": {
        "id": "c274eff2-e141-4b48-94b0-97f1140523fc"
      },
      "outputs": [],
      "source": [
        "# Import pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import pytorch vision modules (preprocessing transforms, datasets)\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d1bd84-3365-43d7-9871-cef7fc9c9225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14d1bd84-3365-43d7-9871-cef7fc9c9225",
        "outputId": "9c27bd4f-4a22-480a-cb67-452303ab2324"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab02112",
      "metadata": {
        "id": "aab02112"
      },
      "source": [
        "#### Load Data, set up `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0cad182",
      "metadata": {
        "id": "d0cad182"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "fmnist = FashionMNIST(\"/content/\", train=True, transform=transform, download=True)\n",
        "train_size = int(0.8*fmnist.targets.shape[0])\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(fmnist, [train_size, fmnist.targets.shape[0]-train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f72899f",
      "metadata": {
        "id": "4f72899f"
      },
      "outputs": [],
      "source": [
        "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "validation_data = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4575edf",
      "metadata": {
        "id": "e4575edf"
      },
      "source": [
        "#### Implement ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e45e60c",
      "metadata": {
        "id": "6e45e60c"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, n_filters=6, start_fc_units=100, activation=F.relu):\n",
        "        super(ConvNet, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e3507",
      "metadata": {
        "id": "d19e3507"
      },
      "source": [
        "#### Loops for Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5933d59f",
      "metadata": {
        "id": "5933d59f"
      },
      "outputs": [],
      "source": [
        "# adapted from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "def train_loop(\n",
        "    dataloader,\n",
        "    model,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    epochs=10,\n",
        "    log_epoch_interval: int = 1\n",
        "):\n",
        "    size = len(dataloader.dataset)\n",
        "    for ep in range(epochs):\n",
        "        loss_val = 0.\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Compute prediction and loss\n",
        "\n",
        "            # Backpropagation\n",
        "\n",
        "            continue\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35b8c80",
      "metadata": {
        "id": "e35b8c80"
      },
      "source": [
        "#### Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d88d5ba",
      "metadata": {
        "id": "1d88d5ba"
      },
      "outputs": [],
      "source": [
        "cn1 = ConvNet(n_filters=2)\n",
        "optimizer = torch.optim.SGD(cn1.parameters(), lr=0.1)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1e741d",
      "metadata": {
        "id": "0c1e741d"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b720e5",
      "metadata": {
        "id": "01b720e5"
      },
      "outputs": [],
      "source": [
        "train_loop(train_data, cn1, loss_fn, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkRfuGTAeH4u",
      "metadata": {
        "id": "xkRfuGTAeH4u"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b495108",
      "metadata": {
        "id": "3b495108"
      },
      "outputs": [],
      "source": [
        "test_loop(validation_data, cn1, loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
