{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba83e56c",
      "metadata": {
        "id": "ba83e56c"
      },
      "source": [
        "# Homework 8\n",
        "In this homework, you will explore neural networks, writing code to complete forward and backward passes for a fully-connected network. Having done this, you will train these networks with the goal of classifying images of apparel into the type (from the Fashion MNIST dataset). You will finally upload your predictions on the test set we give you to [this kaggle competition](https://www.kaggle.com/t/e7ec8dd1f8e64aa2997d82edd5e8f6eb) (more details later in the homework).\n",
        "\n",
        "In the (optional) extension section at the end, you will see how to use [PyTorch](https://pytorch.org/) to implement, train, and evaluate a convolutional neural network.\n",
        "\n",
        "There are a number of programming **tasks** and **quiz questions** in this homework.\n",
        "- For **tasks**, you will need to either **add code between comments \"`#### TASK N CODE`\"** to complete them or **modify code between those comments**. **DO NOT delete the comments \"#### TASK N CODE\". This is for graders' reference and you might not get full points if you tamper with these comments.**\n",
        "- For **quiz questions**, you will need to answer in a few sentences between the given lines.\n",
        "- For **optional tasks**, you are **NOT required to turn them in**. However, we encourage you to complete them as they are good practice.\n",
        "- For **challenge-optional tasks**, you are **NOT required to turn them in**. However, you will receive extra credit for completing the challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32d1c89",
      "metadata": {
        "id": "c32d1c89"
      },
      "source": [
        "-----\n",
        "\n",
        "We will first import libraries.\n",
        "- We provide you with a few relevant functions in the `utils` module. The `linclass` module has a bunch of linear classifiers that we also provided you in the previous homework.\n",
        "- In this assignment, we will program neural networks with different network operations represented as classes. Thus, we will need a common \"abstract\" class and several abstract methods; `abc` module provides these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xYvNiVguuYNP",
      "metadata": {
        "id": "xYvNiVguuYNP"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/utils.py https://www.dropbox.com/scl/fi/h2i6hzhxdz3bvyrcifr6k/utils.py?rlkey=ytpqbrd2l1xprmqyuacu84lod&dl=1\n",
        "!wget -O /content/linclass.py https://www.dropbox.com/scl/fi/l9zxfj6hw48f2klslp9e4/linclass.py?rlkey=7acwguylzhicwel5f0tydfm7u&dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2471ad09-513f-4c81-adb2-edfa7e722178",
      "metadata": {
        "id": "2471ad09-513f-4c81-adb2-edfa7e722178"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import linclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a59c5d9-a81b-4db8-bb2b-141529fe3eb2",
      "metadata": {
        "id": "5a59c5d9-a81b-4db8-bb2b-141529fe3eb2"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9ac85b",
      "metadata": {
        "id": "4e9ac85b"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "As discussed in lecture, neural networks are Directed Acyclic Graphs (DAGs) with input and output nodes, where each node is associated with an \"activation\" function, and where the edges have weights. Neural networks can be represented as DAGs in code using a \"computational graph\". In fact, many libraries such as PyTorch and Tensorflow choose to do so.\n",
        "\n",
        "We will follow a code structure similar to PyTorch, though we will limit the scope of this homework to a path in a DAG. A path in a DAG corresponds to a Sequential neural network, called as such because it comprises of a sequence of layers (collection of units of nodes) where the output of one layer is the input of the next layer in the sequence.\n",
        "\n",
        "We start by defining a `Module` that defines a node or a collection of nodes. A `Module` is an abstract view of the fundamental element of a neural network. It defines several attributes and methods, some of which must be implemented by the derived classes.\n",
        "- `train` is a flag to denote whether the module is in training or evaluation mode. We will see how this is important later. This flag is controlled by `train_mode()` and `eval_mode()` methods.\n",
        "- `forward()` and `backward()` methods do forward and backward passes of the module (recall the discussion from recitation).\n",
        "    - Inside `forward()`, attributes `_input` and `_output` must be assigned. These correspond to the input of the module (`x`) and the output (after whatever operations the module does in the forward pass).\n",
        "    - Inside `backward()`, attributes `_grad_output` and `grad_input` must be assigned. These correspond to the gradient of the loss w.r.t. module's outputs and module's inputs respectively. To do the backward pass, attributes `_input` and `_output` containing the values of the forward pass might come handy.\n",
        "- `get_params()` and `set_params()` are getter and setter functions for _all the **trainable** parameters_ of the module.\n",
        "- `get_grad_params()` method returns the gradient of the loss w.r.t. the **trainable** parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1358e0f",
      "metadata": {
        "id": "e1358e0f"
      },
      "outputs": [],
      "source": [
        "class Module(abc.ABC):\n",
        "    '''\n",
        "    A module defines a sub-graph G' = (V, E) in the DAG G that represents a\n",
        "    neural network. Therefore G' is a subset of G. An instantiation of a module\n",
        "    thus represents a realization of a subgraph, comprising of a set of nodes and\n",
        "    their connections.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        # Train mode or eval mode\n",
        "        self.train = True\n",
        "\n",
        "        # Forward pass cache\n",
        "        self._input = None\n",
        "        self._output = None\n",
        "\n",
        "        # Backward pass cache\n",
        "        self._grad_output = None\n",
        "        self._grad_input = None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Computes the forward pass z = f(x) where f is the function represented by\n",
        "        the module, x is the input, and z is the output of the forward pass.\n",
        "\n",
        "        Assigns to attributes self._input and self._output.\n",
        "\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim) where m is the number of data\n",
        "                points and in_dim is the number of input features.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_forward_attrs(self):\n",
        "        '''Sanity check after a forward pass.'''\n",
        "        assert self._input is not None\n",
        "        assert self._output is not None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        '''\n",
        "        Computes the gradient of loss w.r.t. cached input and trainable parameters.\n",
        "\n",
        "        Assigns to attributes self._grad_output and self._grad_input.\n",
        "        The gradients w.r.t. trainable parameters must also be cached so that\n",
        "        they can be returned by self.get_grad_params().\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of loss w.r.t. output z of the module, dL/dz.\n",
        "                shape (m, out_dim) where m is the number of data points and\n",
        "                out_dim is the number of output features.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        '''Sanity check after a backward pass.'''\n",
        "        assert self._grad_output is not None\n",
        "        assert self._grad_input is not None\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        '''\n",
        "        Returns the trainable parameters of the module. If there are no trainable\n",
        "        parameters, returns None.\n",
        "\n",
        "        Returns:\n",
        "            arr: (jagged) array of trainable parameters, where the entries are\n",
        "                differently-sized numpy arrays themselves.\n",
        "        '''\n",
        "        return None\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        '''\n",
        "        Sets the trainable parameters to params. If there are no trainable parameters\n",
        "        to set, raises a RuntimeError.\n",
        "\n",
        "        Args:\n",
        "            params: (jagged) array of trainable parameters, in the same order\n",
        "                as obtained from self.get_params(). The identity operation is satisfied:\n",
        "                ```\n",
        "                x = self.get_params()\n",
        "                self.set_params(x)\n",
        "                x == self.get_params()\n",
        "                ```\n",
        "        '''\n",
        "        raise RuntimeError('No trainable parameters to set!')\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        '''\n",
        "        Returns the gradients of the loss w.r.t. trainable parameters of the module.\n",
        "        If there are no trainable parameters, returns None.\n",
        "\n",
        "        Returns:\n",
        "            arr: (jagged) array of gradients of trainable parameters,\n",
        "                where the entries are differently-sized numpy arrays themselves.\n",
        "        '''\n",
        "        return None\n",
        "\n",
        "    def train_mode(self):\n",
        "        '''\n",
        "        Switches on the training mode. Useful e.g. in Dropout, where the nodes must be\n",
        "        dropped only during training, not evaluation.\n",
        "        '''\n",
        "        self.train = True\n",
        "\n",
        "    def eval_mode(self):\n",
        "        '''\n",
        "        Switches on the evaluation mode. Useful e.g. in Dropout, where the nodes must be\n",
        "        dropped only during training, not evaluation.\n",
        "        '''\n",
        "        self.train = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c80c47-da9d-4fee-aa1e-ecaa0e77ff9d",
      "metadata": {
        "id": "66c80c47-da9d-4fee-aa1e-ecaa0e77ff9d"
      },
      "source": [
        "### [Task 1] Complete Forward-Backward Prop for a Sequential Network\n",
        "\n",
        "Next, we will define a `Sequential` architecture, called as such because it allows for implementing a sequence of modules. Each module is called a layer, so that the preceding layer connects to the next layer. Sequential architectures are also called \"feed-forward\" networks.\n",
        "\n",
        "In this task, you will finish the `backward()` method. Remember to cache `_grad_output` attribute in the method. Also, store the results in `_grad_input`.\n",
        "\n",
        "We give you the forward pass as reference: iterate through the layers, doing a forward pass on the output of the preceding layer (accessed using `_output`) to get the output of each layer. The output of the sequential network is the final output.\n",
        "\n",
        "Similarly, in the backward pass, you should iterate through the layers in a reverse manner, doing a backward pass on the gradients w.r.t. inputs of the succeeding layer (accessed using `_grad_input`) to get the gradients w.r.t. inputs of each layer. The gradients w.r.t. inputs of the sequential network should be the final gradients you calculate in this reverse iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a43e012-e0d3-4666-8692-276b4c834953",
      "metadata": {
        "id": "2a43e012-e0d3-4666-8692-276b4c834953"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    '''\n",
        "    A sequence of modules, representing a DAG path.\n",
        "    '''\n",
        "    def __init__(self, layers: List[Module]):\n",
        "        '''\n",
        "        Args:\n",
        "            layers: A list of modules to initialize the sequential network.\n",
        "        '''\n",
        "        super(Sequential, self).__init__()\n",
        "        self.layers = layers\n",
        "\n",
        "    def add(self, layer: Module):\n",
        "        '''\n",
        "        Adds the layer at the end of the sequential network.\n",
        "        '''\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Computes a forward pass sequentially on the network layers.\n",
        "        '''\n",
        "        self._input = x\n",
        "        n_layers = len(self.layers)\n",
        "        for i in range(n_layers):\n",
        "            _output_prev = self._input if i == 0 else self.layers[i-1]._output\n",
        "            self.layers[i].forward(_output_prev)\n",
        "        self._output = self._input if n_layers == 0 else self.layers[-1]._output\n",
        "\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        '''\n",
        "        Backpropagates the gradient w.r.t. output of the sequential network,\n",
        "        computing gradients w.r.t. input and trainable parameters of the network.\n",
        "        '''\n",
        "        #### TASK 1 CODE\n",
        "        #### TASK 1 CODE\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            p = layer.get_params()\n",
        "            if p is not None:\n",
        "                params.append(p)\n",
        "\n",
        "        # Wrap parameters in an array. Just np.array(params) won't work due to broadcasting\n",
        "        # conflicts: https://stackoverflow.com/a/49119983. So initialize array and then fill.\n",
        "        arr = np.empty(len(params), dtype=np.ndarray)\n",
        "        arr[:] = params\n",
        "        return arr\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        # Since params has trainable parameters listed in the same order as\n",
        "        # get_params() would have returned, follow the same iteration, and call\n",
        "        # layer.set_params() on params[i] where i is the ith layer with any trainable\n",
        "        # parameters\n",
        "        i = 0\n",
        "        for layer in self.layers:\n",
        "            p = layer.get_params()\n",
        "            if p is not None:\n",
        "                layer.set_params(params[i])\n",
        "                i += 1\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        grad_params = []\n",
        "        for layer in self.layers:\n",
        "            g = layer.get_grad_params()\n",
        "            if g is not None:\n",
        "                grad_params.append(g)\n",
        "        arr = np.empty(len(grad_params), dtype=np.ndarray)\n",
        "        arr[:] = grad_params\n",
        "        return arr\n",
        "\n",
        "    def train_mode(self):\n",
        "        # Switch on training in all layers\n",
        "        super().train_mode()\n",
        "        for layer in self.layers:\n",
        "            layer.train_mode()\n",
        "\n",
        "    def eval_mode(self):\n",
        "        # Switch on eval in all layers\n",
        "        super().eval_mode()\n",
        "        for layer in self.layers:\n",
        "            layer.eval_mode()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6a88ae",
      "metadata": {
        "id": "6e6a88ae"
      },
      "source": [
        "### [Task 2] Complete Forward-Backward Prop for a Linear Layer\n",
        "\n",
        "A linear layer defines an affine transformation on inputs $x \\in \\mathbb{R}^{m \\times d_{in}}$, that is $z = x W + b$ where $W \\in \\mathbb{R}^{d_{in} \\times d_{out}}, b \\in \\mathbb{R}^{d_{out}}$ are trainable parameters. This affine transformation defines a \"fully-connected module\" where each output feature $z[:, j]$ is connected to input feature $x[:, i]$ with an edge-weight $W[i, j]$.\n",
        "\n",
        "We provide you with code that initializes the weight and bias parameters, and their corresponding attributes for storing gradients.\n",
        "\n",
        "You will complete the `backward()` method. We provide you with the forward pass as reference.\n",
        "\n",
        "The discussion from the recitation might be helpful here.\n",
        "\n",
        "**Note on Initialization**: We initialize i.i.d. $W[i, j] \\sim \\mathcal{N}\\left( 0, \\sqrt{\\frac{2}{d_{in}}} \\right)$ and $b_j = 0$ for all $i, j$. The scaled variance for the weights is important for faster convergence, but this is outside the scope of the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2285d4c",
      "metadata": {
        "id": "a2285d4c"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    '''\n",
        "    Linear transformation on the inputs, z = xW + b.\n",
        "\n",
        "    Corresponds to all nodes in the preceding layer connected to all nodes in the\n",
        "    current layer.\n",
        "    '''\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        '''\n",
        "        Args:\n",
        "            in_dim: Number of input dimensions (number of incoming connections\n",
        "                in the network).\n",
        "            out_dim: Number of output dimensions (number of outgoing connections\n",
        "                in the network).\n",
        "        '''\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # Initialize trainable parameters\n",
        "        self.weight = np.random.normal(0, np.sqrt(2/in_dim), (in_dim, out_dim))\n",
        "        self.bias =  np.zeros(out_dim)\n",
        "\n",
        "        # Initialize gradients w.r.t. trainable parameters\n",
        "        self._grad_weight = None\n",
        "        self._grad_bias = None\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim)\n",
        "        '''\n",
        "        assert x.shape[1] == self.weight.shape[0]\n",
        "\n",
        "        self._input = x\n",
        "        self._output = x @ self.weight + self.bias\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Computes gradient w.r.t. trainable parameters and input and returns the gradient\n",
        "        w.r.t. input.\n",
        "\n",
        "        Important: gradients are accumulated for trainable parameters, i.e. added to the existing\n",
        "        values.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient w.r.t. output dL/dz. shape (m, out_dim)\n",
        "\n",
        "        Returns:\n",
        "            grad_input: shape (m, in_dim)\n",
        "        '''\n",
        "        assert grad_output.shape[1] == self.weight.shape[1]\n",
        "\n",
        "        #### TASK 2 CODE\n",
        "        self._grad_output =\n",
        "        self._grad_weight =\n",
        "        self._grad_bias =\n",
        "        self._grad_input =\n",
        "        #### TASK 2 CODE\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        super()._check_backward_attrs()\n",
        "        assert self._grad_weight is not None\n",
        "        assert self._grad_bias is not None\n",
        "\n",
        "    def get_params(self) -> Optional[np.ndarray]:\n",
        "        params = np.empty(2, dtype=np.ndarray)\n",
        "        params[0] = self.weight\n",
        "        params[1] = self.bias\n",
        "        return params\n",
        "\n",
        "    def set_params(self, params: np.ndarray):\n",
        "        assert len(params) == 2\n",
        "        self.weight = params[0]\n",
        "        self.bias = params[1]\n",
        "\n",
        "    def get_grad_params(self) -> Optional[np.ndarray]:\n",
        "        grad_params = np.empty(2, dtype=np.ndarray)\n",
        "        grad_params[0] = self._grad_weight\n",
        "        grad_params[1] = self._grad_bias\n",
        "        return grad_params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6478f7d4",
      "metadata": {
        "id": "6478f7d4"
      },
      "source": [
        "### [Task 3] Complete Forward-Backward Prop for ReLU Activation\n",
        "\n",
        "The `Module` interface allows us to use every element of in the network as a node in the graph, even the activation function applied to a node. The `ReLU` activation function, $z = \\max(0, x)$ for inputs $x$, does not have any trainable parameters, however.\n",
        "\n",
        "You will complete the `backward()` method below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f30ff0",
      "metadata": {
        "id": "77f30ff0"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    '''\n",
        "    ReLU activation, not trainable. z = max(x, 0) for each input value x.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Data features. shape (m, in_dim)\n",
        "        '''\n",
        "        self._input = x\n",
        "        self._output = np.maximum(0., self._input)\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Since there are no trainable parameters, only the gradient w.r.t. input is computed.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient w.r.t. output dL/dz. Any shape\n",
        "        '''\n",
        "        assert grad_output.shape == self._input.shape\n",
        "\n",
        "        #### TASK 3 CODE\n",
        "        #### TASK 3 CODE\n",
        "        self._check_backward_attrs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2db3c0",
      "metadata": {
        "id": "be2db3c0"
      },
      "source": [
        "### Regularization using Dropout\n",
        "\n",
        "We define the final module we will use in this homework: Dropout. Dropout is a commonly used technique for regularization to try to \"limit dependence\" of the network's prediction capabilities on any specific trainable parameters.\n",
        "\n",
        "Dropout accomplishes by switching off nodes, i.e. \"dropping\" them, with some probability during training. Nodes are only dropped during training and not evaluation as Dropout is a regularization technique. Using Dropout during evaluation would cause us to not use the full trained network. This is where the `train` attribute defined in the `Module` abstract class comes in handy. Before training the model, we will call `model.train_mode()` to set `model.train = True`. The implementation `train_mode()` in `Sequential` ensures that the `train` attribute of each layer is set to `True`. Before using the model for evaluation, we will call `model.eval_mode()` to set `model.train = False` and all `train` attributes of the comprising layers to `False`. This way Dropout is not active during evaluation.\n",
        "\n",
        "The non-dropped values are rescaled so that the expectation of the non-dropped node is the same as when dropout wasn't applied.\n",
        "\n",
        "Read through the code so you understand how Dropout is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136cee5b",
      "metadata": {
        "id": "136cee5b"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    '''\n",
        "    A dropout layer.\n",
        "    '''\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            p: (default 0.5) Probability of dropping each node (prob. of setting each value to 0).\n",
        "                If p is 0, then no nodes are dropped, i.e. we get the identity layer.\n",
        "        '''\n",
        "        assert 0 <= p <= 1\n",
        "        super(Dropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        self._input = x\n",
        "        self._output = self._input\n",
        "        if self.train and (not np.isclose(self.p, 0)):\n",
        "            # In training mode and drop probability is positive\n",
        "\n",
        "            # Create a mask to apply to the input using Bernoulli(1-p) RV\n",
        "            self.mask = np.random.binomial(1, 1 - self.p, x.shape).astype(float)\n",
        "\n",
        "            # Scale the mask so that the expected value of prediction during\n",
        "            # testing is same as x, and not (1-p)x\n",
        "            self.mask /= 1 - self.p\n",
        "\n",
        "            self._output *= self.mask\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray):\n",
        "        self._grad_output = grad_output\n",
        "        self._grad_input = self._grad_output\n",
        "        if self.train and (not np.isclose(self.p, 0)):\n",
        "            # In training mode and drop probability is positive, _grad_input is masked\n",
        "            self._grad_input *= self.mask\n",
        "        self._check_backward_attrs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3067360-4d42-4516-b1a8-806fbe18727f",
      "metadata": {
        "id": "e3067360-4d42-4516-b1a8-806fbe18727f"
      },
      "source": [
        "### [Task 4] Complete MultiLogisticLoss\n",
        "\n",
        "A loss function is applied on the predicted probabilities of each label $r \\in \\mathbb{R}^{m \\times k}$ and the true labels $y \\in \\{0, \\dots, k\\}^{m}$. Following the code pattern of `Module`, we define the abstract class `Loss`.\n",
        "- The `forward()` method computes the loss value using responses $r$ and true labels $y$. It assigns to attributes `_input = r`, `_input_target = y`, and `_output`.\n",
        "- The `backward()` method computes the gradient of the loss w.r.t. the inputs, i.e. the responses `r`. It assigns to attribute `_grad_input`.\n",
        "\n",
        "In this task, you will complete `MultiLogisticLoss`. The implementation is almost the same as you did in the previous homework.\n",
        "\n",
        "We provide you an implementation of softmax in `utils.softmax`---you might find this useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a48ee89-c655-4c70-87fd-810a261107cc",
      "metadata": {
        "id": "1a48ee89-c655-4c70-87fd-810a261107cc"
      },
      "outputs": [],
      "source": [
        "class Loss(abc.ABC):\n",
        "    '''Defines a loss function.'''\n",
        "    def __init__(self, k: int):\n",
        "        '''\n",
        "        Args:\n",
        "            k: Number of labels.\n",
        "        '''\n",
        "        self.k = k\n",
        "        self._input = None\n",
        "        self._input_target = None\n",
        "        self._output = None\n",
        "        self._grad_input = None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, r: np.ndarray, y: np.ndarray):\n",
        "        '''\n",
        "        Computes the loss value using responses r and true labels y.\n",
        "\n",
        "        Sets the attributes self._input, self._input_target, and self._output.\n",
        "\n",
        "        Args:\n",
        "            r: Responses of a classifier. shape (m, k) where m is the number of data\n",
        "                points.\n",
        "            y: True labels. shape (m). For all i, 0 <= y_i < k\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_forward_attrs(self):\n",
        "        assert self._input is not None\n",
        "        assert self._input_target is not None\n",
        "        assert self._output is not None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self):\n",
        "        '''\n",
        "        Computes the gradient of the loss value w.r.t. cached responses.\n",
        "\n",
        "        Sets the attribute self._grad_input.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def _check_backward_attrs(self):\n",
        "        assert self._grad_input is not None\n",
        "\n",
        "\n",
        "class MultiLogisticLoss(Loss):\n",
        "    def __init__(self, k: int):\n",
        "        super(MultiLogisticLoss, self).__init__(k)\n",
        "\n",
        "    def forward(self, r: np.ndarray, y: np.ndarray):\n",
        "        '''\n",
        "        Computes the multiclass logistic loss, using the softmax operation to\n",
        "        convert responses r to normalized probabilities.\n",
        "        '''\n",
        "        assert r.shape[0] == y.shape[0]\n",
        "        assert r.shape[1] == self.k\n",
        "\n",
        "        self._input = r\n",
        "        self._input_target = y\n",
        "\n",
        "        stable_r = self._input - np.max(self._input, axis=1)[:, np.newaxis]\n",
        "        nll = np.log(np.sum(np.exp(stable_r), axis=1)) - \\\n",
        "            np.take_along_axis(stable_r, self._input_target[:, np.newaxis], axis=1).flatten()\n",
        "        self._output = np.mean(nll)\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self):\n",
        "        #### TASK 4 CODE\n",
        "        #### TASK 4 CODE\n",
        "        self._check_backward_attrs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4ff6b0",
      "metadata": {
        "id": "fc4ff6b0",
        "tags": []
      },
      "source": [
        "### Testing your gradient calculation\n",
        "\n",
        "To help you ensure your code calculates the forward and backward passes correctly, we have provided a few tests below. These tests match your implementation against a numerical approximation of the gradient. You should get relative errors on the order of $10^{-8}$ or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf16719-87ec-409f-97c1-ed0b414cff8a",
      "metadata": {
        "id": "1cf16719-87ec-409f-97c1-ed0b414cff8a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def test_log_grad():\n",
        "    '''\n",
        "    Tests MultiLogisticLoss Gradient implementation. The relative error should be small.\n",
        "    '''\n",
        "    print('Multi Logistic Loss Grad Test')\n",
        "    loss = MultiLogisticLoss(k=10)\n",
        "    x = np.random.rand(3, 10)\n",
        "    y = np.array([4, 5, 6])\n",
        "    def test_f(x):\n",
        "        loss.forward(x, y)\n",
        "        return loss._output\n",
        "\n",
        "    # Backprop manually\n",
        "    test_f(x)\n",
        "    loss.backward()\n",
        "    grad_input = loss._grad_input\n",
        "\n",
        "    # Gradient approximation\n",
        "    grad_input_num = utils.numeric_grad(test_f, x, 1, 1e-6)\n",
        "\n",
        "    rel_err = utils.relative_error(grad_input, grad_input_num, 1e-8)\n",
        "    print(f'Relative err between your computation and numerical gradient: {rel_err:e}')\n",
        "\n",
        "test_log_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371e4fde-7723-48cb-b3af-41ee5fac0e01",
      "metadata": {
        "id": "371e4fde-7723-48cb-b3af-41ee5fac0e01",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LossForUnitTesting(Loss):\n",
        "    def __init__(self, k):\n",
        "        super(LossForUnitTesting, self).__init__(k)\n",
        "\n",
        "    def forward(self, x: np.ndarray, y: np.ndarray):\n",
        "        self._input = x\n",
        "        self._input_target = y\n",
        "        self._output = np.mean(np.sum(np.abs(self._input), axis=1))\n",
        "        self._check_forward_attrs()\n",
        "\n",
        "    def backward(self):\n",
        "        self._grad_input = np.sign(self._input) / self._input.shape[0]\n",
        "        self._check_backward_attrs()\n",
        "\n",
        "\n",
        "def test_module(model: Module):\n",
        "    '''\n",
        "    Tests Module Gradient implementation. The relative error should be small.\n",
        "    '''\n",
        "    print('Module Grad Test')\n",
        "\n",
        "    model.eval_mode()\n",
        "\n",
        "    loss = LossForUnitTesting(k=10)\n",
        "    x = np.random.rand(3, 10)\n",
        "    y = np.array([4, 5, 6])\n",
        "    def test_f(x):\n",
        "        model.forward(x)\n",
        "        loss.forward(model._output, y)\n",
        "        return loss._output\n",
        "\n",
        "    # Backprop manually\n",
        "    test_f(x)\n",
        "    loss.backward()\n",
        "    model.backward(loss._grad_input)\n",
        "    grad_input = model._grad_input\n",
        "\n",
        "    # Gradient approximation\n",
        "    grad_input_num = utils.numeric_grad(test_f, x, 1, 1e-6)\n",
        "\n",
        "    rel_err = utils.relative_error(grad_input, grad_input_num, 1e-8)\n",
        "    print(f'Relative err between your computation and numerical gradient: {rel_err:e}')\n",
        "\n",
        "print('Testing Linear...')\n",
        "model = Linear(10, 20)\n",
        "test_module(model)\n",
        "\n",
        "print('Testing ReLU...')\n",
        "model = ReLU()\n",
        "test_module(model)\n",
        "\n",
        "print('Testing Dropout...')\n",
        "model = Dropout()\n",
        "test_module(model)\n",
        "\n",
        "print('Testing 2-layer model')\n",
        "model = Sequential([\n",
        "    Linear(10, 20),\n",
        "    ReLU(),\n",
        "    Linear(20, 10)\n",
        "])\n",
        "test_module(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2b53f5-757e-4222-893d-6a97feb31176",
      "metadata": {
        "id": "3e2b53f5-757e-4222-893d-6a97feb31176"
      },
      "source": [
        "### [Task 5] Building a Neural Network Classifier\n",
        "\n",
        "We can now define a classifier using the code structure we have followed in the previous homeworks. A neural network classifier consists of an initialized model and a loss, specified when instantiating `ERMNeuralNetClassifier`.\n",
        "\n",
        "We will be using SGD to train the classifier, using the same implementation from the previous homework. We have included the SGD implementation in `utils`.\n",
        "\n",
        "In this task, you will complete the `predict` and `train_grad` methods. The first is for logging purposes, and the second is for calculating the gradients w.r.t. trainable parameters. Like `train_obj`, `train_grad` takes in and sets the parameters of the model and does a forward pass to calculate the loss on data `X, y`  indexed by the batch indices. `train_grad` finally does a backward propagation and returns the gradients w.r.t. trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afcdba5-c019-496d-9861-e025bd532d39",
      "metadata": {
        "id": "6afcdba5-c019-496d-9861-e025bd532d39"
      },
      "outputs": [],
      "source": [
        "class ERMNeuralNetClassifier(linclass.Classifier):\n",
        "    '''\n",
        "    Neural network trained by minimizing the empirical risk with SGD,\n",
        "    w.r.t. some loss function.\n",
        "    '''\n",
        "    def __init__(self, model: Module, loss: Loss, **kwargs):\n",
        "        '''\n",
        "        Args:\n",
        "            model: A neural network object with initialized parameters.\n",
        "            loss: A loss function.\n",
        "        '''\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.params0 = self.model.get_params()\n",
        "        self.params = None\n",
        "        self.loss = loss\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Returns predicted labels for data.\n",
        "\n",
        "        Args:\n",
        "            X: Data features. shape (m, d_in) where d_in is the number of\n",
        "                input features of self.model.\n",
        "\n",
        "        Returns:\n",
        "            shape (m)\n",
        "        '''\n",
        "        assert self.params is not None, \"Classifier hasn't been fit!\"\n",
        "\n",
        "        # Switch to evaluation mode to disable dropout\n",
        "        self.model.eval_mode()\n",
        "\n",
        "        #### TASK 5 CODE\n",
        "        #### TASK 5 CODE\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, **sgd_kwargs):\n",
        "        '''\n",
        "        Fits the classifier on dataset.\n",
        "\n",
        "        Args:\n",
        "            X: Data features. shape (m, d_in) where d_in is the number of input\n",
        "                features of self.model.\n",
        "            y: Data labels, 0 <= y_i < k. shape (m)\n",
        "        '''\n",
        "        assert X.shape[0] == y.shape[0]\n",
        "\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Define training objective\n",
        "        def train_obj(params: np.ndarray, batch: Optional[np.ndarray] = None) -> float:\n",
        "            '''\n",
        "            Calculates the training objective with parameters on a batch of training samples.\n",
        "\n",
        "            Args:\n",
        "                params: Trainable parameters, in the same format as self.model.get_params().\n",
        "                batch: (default None) Indices of samples to calculate objective on. If None,\n",
        "                    calculate objective on all samples.\n",
        "            '''\n",
        "            if batch is None:\n",
        "                # All data is in a batch\n",
        "                batch = slice(None)\n",
        "\n",
        "            self.model.set_params(params)\n",
        "\n",
        "            # Forward pass\n",
        "            self.model.forward(X[batch])\n",
        "            self.loss.forward(self.model._output, y[batch])\n",
        "\n",
        "            loss_val = self.loss._output\n",
        "            return loss_val\n",
        "\n",
        "        # Define training gradient\n",
        "        def train_grad(params: np.ndarray, batch: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "            '''\n",
        "            Returns the gradient of the training objective w.r.t. parameters,\n",
        "            calculated on a batch of training samples.\n",
        "\n",
        "            Args:\n",
        "                params: Trainable parameters, in the same format as self.model.get_params().\n",
        "                batch: (default None) Indices of samples to calculate objective on. If None,\n",
        "                    calculate objective on all samples.\n",
        "            '''\n",
        "            if batch is None:\n",
        "                # All data is in a batch\n",
        "                batch = slice(None)\n",
        "\n",
        "            self.model.set_params(params)\n",
        "            #### TASK 5 CODE\n",
        "            # Forward pass\n",
        "\n",
        "            # Backward pass\n",
        "\n",
        "            grad_params = self.model.get_grad_params()\n",
        "            #### TASK 5 CODE\n",
        "            return grad_params\n",
        "\n",
        "        self.sgd_loggers = [\n",
        "            utils.SGDLogger('train_obj', train_obj, can_display=True, per_epoch=True),\n",
        "        ] + sgd_kwargs.pop('loggers', [])\n",
        "\n",
        "        # Switch to training mode to enable dropout, if present in the model\n",
        "        self.model.train_mode()\n",
        "\n",
        "        # Optimize using SGD\n",
        "        self.params = utils.SGD(\n",
        "            self.params0,\n",
        "            train_grad,\n",
        "            m,\n",
        "            loggers=self.sgd_loggers,\n",
        "            **sgd_kwargs\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20b3f74",
      "metadata": {
        "id": "e20b3f74"
      },
      "source": [
        "### Training the Neural Network Classifier\n",
        "\n",
        "We will now train a classifier using your implementation. We will use a toy example first. You should check that your training objective decreases with training, and that you get better than random training error (think what the error of random prediction is when the number of labels is 5).\n",
        "\n",
        "After this, we will train a classifier and evaluate it on a real-world dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaee175b-38cf-41d3-a75f-a0db557328ad",
      "metadata": {
        "id": "eaee175b-38cf-41d3-a75f-a0db557328ad"
      },
      "outputs": [],
      "source": [
        "n_train = 50\n",
        "k = 5\n",
        "X_train = np.random.rand(n_train, 10)\n",
        "y_train = np.random.choice(k, n_train)\n",
        "\n",
        "model = Sequential([\n",
        "    Linear(10, 20),\n",
        "    ReLU(),\n",
        "    Linear(20, k)\n",
        "])\n",
        "loss = MultiLogisticLoss(k=k)\n",
        "clf = ERMNeuralNetClassifier(model, loss)\n",
        "\n",
        "clf.fit(X_train, y_train, eta=0.01, n_epochs=1000,\n",
        "        verbose=True, verbose_epoch_interval=100)\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "train_err = utils.empirical_err(y_train, y_train_pred)\n",
        "\n",
        "print(f'train_err: {train_err:5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d119e56",
      "metadata": {
        "id": "9d119e56"
      },
      "source": [
        "### [Task 6] Fashion MNIST dataset\n",
        "\n",
        "As mentioned, for this assigment, we will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. It is a slightly harder 10-class classification problem than the MNIST dataset you worked with last week but is set up in the same way (10 classes, 28x28 images).\n",
        "\n",
        "We provide you with `.npy` files of the data to use. The code below loads the training data and labels. Your task is to submit labels for the test set.\n",
        "\n",
        "We provide an example network for inspiration. You should build a better model, perhaps using deeper models or wider layers (just examples).\n",
        "\n",
        "#### **Endgame**\n",
        "\n",
        "Finally, upload the predicted test labels to [this kaggle competition](https://www.kaggle.com/t/e7ec8dd1f8e64aa2997d82edd5e8f6eb), where we will first evaluate it on a slice of the test set to show your standings. Kaggle allows resubmissions, but beware of submitting relentlessly to climb up in the public leaderboard---you'd be overfitting to the test set. After the deadline, we will test it on secret data, which the public leaderboard does not show you, to see how well your predictor generalizes. We recommend using validation or cross-validation for developing your model and then submitting to Kaggle.\n",
        "\n",
        "**Your grade will not depend on your performance relative to others in the class**. We simply want you to try your best on getting good performance. Any concepts you learnt in this class or code you developed in this class are fair game. You are also welcome to use methods not directly covered in this class, but **the work must be your own** -- i.e., you must implement and train the models yourself, even if the architecture is inspired by something you saw elsewhere.\n",
        "\n",
        "**Report your Kaggle username here so we can know where you are on the leaderboard!**\n",
        "#### **Your User Name:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a4ddJDwk8d",
      "metadata": {
        "id": "58a4ddJDwk8d"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/fmnist_train.npy https://www.dropbox.com/scl/fi/zi2yfdcnkuj6n5set9f1r/fmnist_train.npy?rlkey=4a58rlxci1nk1q4ti535n7anc&dl=1\n",
        "!wget -O /content/fmnist_test.npy https://www.dropbox.com/scl/fi/t0o66rz2uu3955t4rdqga/fmnist_test.npy?rlkey=ja8asqgwp99vf6sxblqqt89jo&dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2fc8c3",
      "metadata": {
        "id": "9a2fc8c3"
      },
      "outputs": [],
      "source": [
        "train_data = np.load('/content/fmnist_train.npy', allow_pickle=True).item()\n",
        "test_data = np.load('/content/fmnist_test.npy', allow_pickle=True).item()\n",
        "\n",
        "X = train_data['data']\n",
        "y = train_data['labels']\n",
        "X_test = test_data['data']\n",
        "\n",
        "# Preprocessing X\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "if X.max() > 1: X = X / 255.\n",
        "\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "if X_test.max() > 1: X_test = X_test / 255.\n",
        "\n",
        "# Split into Xfm_train, yfm_train, Xfm_val, yfm_val\n",
        "Xfm_train, yfm_train, Xfm_val, yfm_val = utils.create_split(X, y, 0.8)\n",
        "\n",
        "# An example of a sequential network\n",
        "model = Sequential([\n",
        "    Linear(Xfm_train.shape[1], 400),\n",
        "    ReLU(),\n",
        "    Dropout(0.1),\n",
        "    Linear(400, 10)\n",
        "])\n",
        "loss = MultiLogisticLoss(k=10)\n",
        "clf = ERMNeuralNetClassifier(model, loss)\n",
        "\n",
        "sgd_kwargs = {\n",
        "    'batch_size': 128,\n",
        "    'n_epochs': 5,\n",
        "    'eta': 0.01,\n",
        "    'verbose': True, # Enable printing INSIDE SGD\n",
        "    'verbose_epoch_interval': 1,\n",
        "}\n",
        "\n",
        "clf.fit(Xfm_train, yfm_train, **sgd_kwargs)\n",
        "\n",
        "yfm_train_pred = clf.predict(Xfm_train)\n",
        "train_err = utils.empirical_err(yfm_train, yfm_train_pred)\n",
        "\n",
        "yfm_val_pred = clf.predict(Xfm_val)\n",
        "val_err = utils.empirical_err(yfm_val, yfm_val_pred)\n",
        "\n",
        "print(f'train_err: {train_err:5f}, val_err: {val_err:5f}')\n",
        "\n",
        "#### TASK 6 CODE\n",
        "#### TASK 6 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d74c953",
      "metadata": {
        "id": "0d74c953"
      },
      "source": [
        "#### Package output for uploading to Kaggle\n",
        "\n",
        "In order to upload your results to Kaggle, load the test data, use your model to predict outputs, and then write the outputs to a csv file. You will upload the csv file stored at `fname` to the Kaggle competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c64ee6",
      "metadata": {
        "id": "87c64ee6"
      },
      "outputs": [],
      "source": [
        "# Save the CSV file of labels. UPLOAD THIS FILE\n",
        "fname = '/content/fmnist_test_pred.csv'\n",
        "output = np.vstack((np.arange(y_test_preds.shape[0]), y_test_preds)).T\n",
        "np.savetxt(fname, output, fmt=\"%d\", delimiter=',', comments='', header='id,label')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c977b9a",
      "metadata": {
        "id": "9c977b9a"
      },
      "source": [
        "## [OPTIONAL] Extension: Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9382f645",
      "metadata": {
        "id": "9382f645"
      },
      "source": [
        "In practice, fully-connected neural networks are hard to scale to images -- the number of units required grows significantly. However, in images there is a lot of local structure that we can exploit, in particular by focusing on local features and sharing weights for features derived from different parts of the image. Another way to think about this is as passing a convolutional filter over the whole image. This is similar to what we did in the Viola-Jones face detector, except that in this case, we will learn the parameter values (and therefore the filter itself) rather than pre-setting them. We did not cover convolutional neural networks in a lot of detail this quarter, but you can look at [this course](https://cs231n.github.io/) and in particular [this set of notes](https://cs231n.github.io/convolutional-networks/) for a good introduction to the topic.\n",
        "\n",
        "To implement Neural Networks, we could take the approach that we did in the previous part of the homework, and build them ourselves from ground up. However, the implementation above is not very optimized, and we cannot run it on special hardware. To address this, there are several available libraries for building, training, and even deploying deep learning models. Of these, [Pytorch](https://pytorch.org/) is a very commonly used one. We'll give an example of how to load data from a pre-loaded dataset, how to set up a convolutional neural network, and how to train it. Feel free to experiment with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01201d88-4477-41ef-b820-da90a0e82dd8",
      "metadata": {
        "id": "01201d88-4477-41ef-b820-da90a0e82dd8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c274eff2-e141-4b48-94b0-97f1140523fc",
      "metadata": {
        "id": "c274eff2-e141-4b48-94b0-97f1140523fc"
      },
      "outputs": [],
      "source": [
        "# Import pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import pytorch vision modules (preprocessing transforms, datasets)\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d1bd84-3365-43d7-9871-cef7fc9c9225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14d1bd84-3365-43d7-9871-cef7fc9c9225",
        "outputId": "9c27bd4f-4a22-480a-cb67-452303ab2324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7afa8438a7d0>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab02112",
      "metadata": {
        "id": "aab02112"
      },
      "source": [
        "#### Load Data, set up `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0cad182",
      "metadata": {
        "id": "d0cad182"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "fmnist = FashionMNIST(\"/content/\", train=True, transform=transform, download=True)\n",
        "train_size = int(0.8*fmnist.targets.shape[0])\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(fmnist, [train_size, fmnist.targets.shape[0]-train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f72899f",
      "metadata": {
        "id": "4f72899f"
      },
      "outputs": [],
      "source": [
        "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "validation_data = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4575edf",
      "metadata": {
        "id": "e4575edf"
      },
      "source": [
        "#### Implement ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e45e60c",
      "metadata": {
        "id": "6e45e60c"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, n_filters=6, start_fc_units=100, activation=F.relu):\n",
        "        super(ConvNet, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e3507",
      "metadata": {
        "id": "d19e3507"
      },
      "source": [
        "#### Loops for Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5933d59f",
      "metadata": {
        "id": "5933d59f"
      },
      "outputs": [],
      "source": [
        "# adapted from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "def train_loop(\n",
        "    dataloader,\n",
        "    model,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    epochs=10,\n",
        "    log_epoch_interval: int = 1\n",
        "):\n",
        "    size = len(dataloader.dataset)\n",
        "    for ep in range(epochs):\n",
        "        loss_val = 0.\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Compute prediction and loss\n",
        "\n",
        "            # Backpropagation\n",
        "\n",
        "            continue\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35b8c80",
      "metadata": {
        "id": "e35b8c80"
      },
      "source": [
        "#### Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d88d5ba",
      "metadata": {
        "id": "1d88d5ba"
      },
      "outputs": [],
      "source": [
        "cn1 = ConvNet(n_filters=2)\n",
        "optimizer = torch.optim.SGD(cn1.parameters(), lr=0.1)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1e741d",
      "metadata": {
        "id": "0c1e741d"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b720e5",
      "metadata": {
        "id": "01b720e5"
      },
      "outputs": [],
      "source": [
        "train_loop(train_data, cn1, loss_fn, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkRfuGTAeH4u",
      "metadata": {
        "id": "xkRfuGTAeH4u"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b495108",
      "metadata": {
        "id": "3b495108"
      },
      "outputs": [],
      "source": [
        "test_loop(validation_data, cn1, loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
