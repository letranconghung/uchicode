{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "daf09323",
      "metadata": {
        "id": "daf09323"
      },
      "source": [
        "# Homework 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ff58c9",
      "metadata": {
        "id": "14ff58c9"
      },
      "source": [
        "In this notebook, you will explore kernel ridge regression and kernel SVM. We first present kernel ridge regression on a housing dataset to showcase the ideas in Question 4 on the theoretical portion of the homework. Next, we start our exploration into kernel SVM with a two-dimensional example on the spiral data and then build a simple but powerful sentiment classifier on tweets to airlines, a topic we may have more sympathy for as inclement weather hits us here in Chicago..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7785d9",
      "metadata": {
        "id": "0e7785d9"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O $PWD/utils.py https://www.dropbox.com/scl/fi/9od7hx1y2q0jf7sncsk1w/utils.py?rlkey=c3gy7aiphk8ycfb9qjgsm5a06&dl=1"
      ],
      "metadata": {
        "id": "vm2ZbHdk8MNU"
      },
      "id": "vm2ZbHdk8MNU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6b4192",
      "metadata": {
        "id": "ec6b4192"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7197a8",
      "metadata": {
        "id": "4d7197a8"
      },
      "source": [
        "## Kernel Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411e03d8",
      "metadata": {
        "id": "411e03d8"
      },
      "source": [
        "In the Question 3 of the theoretical homework, we studied kernel ridge regression. In this part, we will address the practical considerations for solving the kernel ridge regression problem.\n",
        "\n",
        "We consider the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset), which consists of 8 numeric features, including house age, number of bedrooms, and location, and has target median house value. It contains over 20 thousand samples. We will use 3000 for now, but feel free to use more to see what happens! First, let us load it from `sklearn`'s set of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9358488",
      "metadata": {
        "id": "b9358488"
      },
      "outputs": [],
      "source": [
        "cal  = sklearn.datasets.fetch_california_housing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3612113",
      "metadata": {
        "id": "e3612113"
      },
      "outputs": [],
      "source": [
        "from utils import TrainAndTestData\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "\n",
        "m = 3000\n",
        "perm = np.random.permutation(len(cal.target))\n",
        "train_i = perm[:m]\n",
        "test_i = perm[m:]\n",
        "train_X = cal.data[train_i,:]\n",
        "train_y = cal.target[train_i]\n",
        "test_X = cal.data[test_i,:]\n",
        "test_y = cal.target[test_i]\n",
        "\n",
        "housing_data = TrainAndTestData(train_X, train_y, test_X, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3ebbce",
      "metadata": {
        "id": "3d3ebbce"
      },
      "source": [
        "Let use a Gaussian (RBF) kernel. Recall that the definition of this kernel is:\n",
        "\n",
        "$$\n",
        "    K_{RBF}(x_i, x_j) = \\exp \\left( -\\beta \\left(  \\langle x_i, x_i \\rangle + \\langle x_j, x_j \\rangle - 2\\langle x_i, x_j \\rangle  \\right)    \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0629ab22",
      "metadata": {
        "id": "0629ab22"
      },
      "outputs": [],
      "source": [
        "def RBF_kernel(beta = 1):\n",
        "    def RBF_kernel_beta(x1,x2):\n",
        "        return np.exp(- beta*(np.sum(x1*x1, 1)[:,np.newaxis] + np.sum(x2*x2, 1)-2*x1@x2.T ))\n",
        "    return RBF_kernel_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d4029c",
      "metadata": {
        "id": "70d4029c"
      },
      "source": [
        "*A quick note on closure: the above kernel construction function employs a Python concept known as `closure` which provides us the following functionality. Every kernel function should have the same signature: given two data points as input, output a real number. However, the function may be dependent on some parameter that we cannot hard code. Thus, the outer function constructs a kernel function with the desired signature while fixing a value for the parameter. This becomes very useful when we may need to pass around a kernel function but always for the same parameter value*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10908c5",
      "metadata": {
        "id": "e10908c5"
      },
      "source": [
        "To use this function, we first pick some value of `beta` and instantiate: `RBF_kernel(beta=myvalue)`. This is itself a function and can now take in matrices `x1, x1`. That is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb8a42f",
      "metadata": {
        "id": "deb8a42f"
      },
      "outputs": [],
      "source": [
        "RBF_kernel(beta = 1)(housing_data.X_train[:10], housing_data.X_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411cdf0f",
      "metadata": {
        "id": "411cdf0f"
      },
      "source": [
        "### [Task 1] Looking for a great $\\beta$ value\n",
        "Look at the values of the kernel for that setting of $\\beta$. What do you notice about the values? Generally, what values is the Gaussian kernel distributed between (consider limiting cases: large distance / exponent, small distance exponent). What part of that range does the above lie in? How can you improve this? Find a good setting for $\\beta$ by considering these questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec788ec",
      "metadata": {
        "id": "fec788ec"
      },
      "outputs": [],
      "source": [
        "#### TASK 1 CODE\n",
        "RBF_kernel(beta = ???)(housing_data.X_train[:10], housing_data.X_train[:10])\n",
        "#### TASK 1 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b660b84d",
      "metadata": {
        "id": "b660b84d"
      },
      "source": [
        "Armed with a good kernel to represent our features well, we move on to the learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Task 2] Implementing Kernel Ridge Regression\n",
        "Now, let us implement the kernel ridge regression solution. In 3(b), you formulated the solution to the kernel ridge regression as computing the least squares solution to some expression. Finish the function below using proper numeric python syntax. Also compute the prediction given the kernel, the validation points, the predictor, and the training points (`predict_kernel_ridge`)."
      ],
      "metadata": {
        "id": "4aR6PPNVmuz6"
      },
      "id": "4aR6PPNVmuz6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13baf8e",
      "metadata": {
        "id": "a13baf8e"
      },
      "outputs": [],
      "source": [
        "def train_kernel_ridge(kernel, lmbd, x, y):\n",
        "    from numpy.linalg import lstsq\n",
        "    K = kernel(x,x)\n",
        "    #### TASK 2 CODE\n",
        "    least_squares_soln =\n",
        "    #### TASK 2 CODE\n",
        "    return least_squares_soln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16429bf0",
      "metadata": {
        "id": "16429bf0"
      },
      "outputs": [],
      "source": [
        "def predict_kernel_ridge(kernel, x, alpha, train_x):\n",
        "    #### TASK 2 CODE\n",
        "    #### TASK 2 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710a9661",
      "metadata": {
        "id": "710a9661"
      },
      "source": [
        "Given this, load and process the data into the Gram matrix, compute the KRR solution for this data for some fixed regularization parameter $\\lambda$, and predict the answers on a validation set.\n",
        "\n",
        "**Check yourself**: How long does the `train_kernel_ridge` function take to run? Are you inverting a matrix?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a55207e",
      "metadata": {
        "id": "0a55207e"
      },
      "source": [
        "Great. You now are able to compute the KRR solution for data. Let's compute some baselines to know what we're trying to beat. Compute the mean squared error for the following two (extremely simple) predictors:\n",
        "* **null predictor**: output 0 for every data point\n",
        "* **mean predictor**: output the mean of the training `y` for every data point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a6076a",
      "metadata": {
        "id": "87a6076a"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(pred, y):\n",
        "    return np.mean((pred-y)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5519a8de",
      "metadata": {
        "id": "5519a8de"
      },
      "outputs": [],
      "source": [
        "#### TASK 2 CODE\n",
        "## BASELINES\n",
        "\n",
        "#### TASK 2 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aa4602",
      "metadata": {
        "id": "d3aa4602"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 1]</h4>\n",
        "\n",
        "Use cross-validation or validation to choose $\\lambda$. Make sure you beat the baselines. What values of $\\beta, \\lambda$ give good performance? <br>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer:\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc77cb76",
      "metadata": {
        "id": "dc77cb76"
      },
      "outputs": [],
      "source": [
        "#### CODE\n",
        "#### CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a1d719",
      "metadata": {
        "id": "25a1d719"
      },
      "source": [
        "## 2D kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c74bc9e",
      "metadata": {
        "id": "7c74bc9e"
      },
      "source": [
        "In this part, we revisit the spiral dataset. We will walk through solving this problem using the radial basis function kernels. Your job will be to understand how well the predictor works. After this, we encourage you to explore trying different kernels and parameter settings to see how they perform. We will use the [implementation of SVM in `scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "LABELS = [-1, 1]\n",
        "SP_THETA_SIGMA = 0.3\n",
        "SP_R_SIGMA = 0.05\n",
        "NOISE_LEVEL = 0.2\n",
        "\n",
        "m = 1000\n",
        "Xsp, ysp = utils.generate_spiral_data(m, noise_level=NOISE_LEVEL, theta_sigma=SP_THETA_SIGMA, r_sigma=SP_R_SIGMA)\n",
        "\n",
        "train_test_ratio = 0.8\n",
        "Xsp_train, ysp_train, Xsp_test, ysp_test = utils.create_split(Xsp, ysp, train_test_ratio)\n",
        "\n",
        "spirals = TrainAndTestData(Xsp_train, ysp_train, Xsp_test, ysp_test)"
      ],
      "metadata": {
        "id": "mXSAPW9C_to3"
      },
      "id": "mXSAPW9C_to3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6c559e2e",
      "metadata": {
        "id": "6c559e2e"
      },
      "source": [
        "See how RBF kernel (sigma) compares to the $k$-Nearest Neighbor predictor and decision tree predictors from HW3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b7fee0",
      "metadata": {
        "id": "d7b7fee0"
      },
      "outputs": [],
      "source": [
        "# for now, kernel = one of ['linear', 'poly', 'rbf']\n",
        "# the relevant parameters are degree, gamma -- see documentation for details of how to use them\n",
        "svm_clf = svm.SVC(kernel='rbf', gamma=10, C=5)\n",
        "svm_clf = svm_clf.fit(Xsp_train, ysp_train)\n",
        "utils.plot_decision_boundary(svm_clf, Xsp_train, ysp_train, Xsp_test, ysp_test)\n",
        "\n",
        "spirals.print_errors(svm_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83d5235",
      "metadata": {
        "id": "b83d5235"
      },
      "source": [
        "### [Task 3] Applying SVM classifiers to the 2D spiral data\n",
        "For the classifier you trained, find and write down the value of the SVM object, training loss, and training error, margin violations, and the number of support vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce33d559",
      "metadata": {
        "id": "ce33d559"
      },
      "outputs": [],
      "source": [
        "#### TASK 3 CODE\n",
        "#### TASK 3 CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4749feb1",
      "metadata": {
        "id": "4749feb1"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer:\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ac7fcc",
      "metadata": {
        "id": "56ac7fcc"
      },
      "source": [
        "Explore different kernels, parameter values, and regularization parameters to see how these factors affect the learned classifier. Spend some time exploring here to understand the influence of these factors, since in the next part, you will be using the same techniques, but this time on data that lies in much higher than 2D, so plotting to analyze what you get is not really an option."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7272f9eb",
      "metadata": {
        "id": "7272f9eb"
      },
      "source": [
        "## Sentiment Analysis Using Kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376d040",
      "metadata": {
        "id": "3376d040"
      },
      "source": [
        "Next, we will develop a sentiment classifier using kernel SVM. We will work with a real-world dataset of [tweets to airlines](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). Datasets that have been scraped from the internet (such as this one) are prone to many issues, whether we use them directly or with some filtering. See if you can think of a few potential issues, and feel free to discuss with TAs at office hours. Despite these issues, the dataset provides value in giving us short statements with strong sentiment that we will build a classifier over.\n",
        "\n",
        "**Data pre-processing** The raw data, which you can access and study at the above link, contains 15 attributes, including `tweet_id` , `airline_sentiment`, `negative_reason`, `airline`, `text.` Of these, we are most interested in `airline_sentiment` and `text`. To that end, we have extracted these for you in the files `cleaned_tweets_train.tsv` and `cleaned_tweets_test.tsv`. A `tsv` file is a file where the different attributes are separated by tabs. The dataset identifies three different sentiments: `positive`, `neutral`, and `negative`. After extracting just the tweets and the sentiments, we shuffled all the tweets and saved the first 3/4 of them to the training file and the remaining 1/4 of them to the test file.\n",
        "\n",
        "**Data loading** The `load_data` function we provide in `utils.py` allows filtering neutral (i.e., removing them from the data) by setting `filter_neutrals` flag to `True` or including them but counting them as \"not positive\" examples by setting `filter_neutrals` flag to `False`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $PWD/data/\n",
        "!wget -O $PWD/data/cleaned_tweets_train.tsv https://www.dropbox.com/scl/fi/tuh6nuhr0uqqj6tjxyd4p/cleaned_tweets_train.tsv?rlkey=xe71ynusj9fzbm1i7vrcq9gpp&dl=1\n",
        "!wget -O $PWD/data/cleaned_tweets_test.tsv https://www.dropbox.com/scl/fi/q0e356xajsxjh40x7xwcr/cleaned_tweets_test.tsv?rlkey=hfenusk7tcvvi0xu85181dbcy&dl=1"
      ],
      "metadata": {
        "id": "DAZ9_cceEgfk"
      },
      "id": "DAZ9_cceEgfk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "629010c8",
      "metadata": {
        "id": "629010c8"
      },
      "source": [
        "### Kernel Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a5c310",
      "metadata": {
        "id": "31a5c310"
      },
      "source": [
        "Recall that a kernel can be defined as $K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle\\, \\in \\mathbb{R}\\,.$ However, the $x_i$ do not have to be real-valued, or numeric at all. Indeed, in this case, they are strings of length $k$ (tweets, in particular) in $\\mathcal{D}^k$, where $\\mathcal{D}$ is the dictionary of words. Then, we can decompose the kernel as:\n",
        "\n",
        "\n",
        "$$\n",
        "K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\langle \\tilde{\\phi}(v(x_i)), \\tilde{\\phi}(v(x_j))\\rangle\\\n",
        "$$\n",
        "\n",
        "\n",
        "where $\\phi = \\tilde{\\phi} \\circ v\\,,$ $\\tilde{\\phi} \\, : \\mathbb{R}^{d_1} \\mapsto \\mathbb{R}^{d_2}$ and $v \\, : \\mathcal{D}^k \\mapsto \\mathbb{R}^{d_1}\\,.$\n",
        "\n",
        "This decomposition allows us to separate the transformation into two parts. Now, we can choose both independently. Here are some suggestions for each:\n",
        "\n",
        "* For $v(x_i)\\,:$\n",
        "    * **Bag-of-words**: for each word $w$ in the corpus, the corresponding component of the bag-of-words representation of $x_i$ is defined as the number of occurences of $w$ in $x_i$.\n",
        "    * **Bi-gram**: for each pair of words that occur contiguously in the corpus, the corresponding component of the bi-gram representation of $x_i$ is the number of times that the bi-gram (two-word pattern) appears in $x_i$.\n",
        "    * **Subsequence counts**: for each subsequence (of fixed size) in the corpus, the corresponding component of this representation of $x_i$ is the nubmer of times that the subsequence has appeared in the document $x_i.$ (A subsequence allows for skipping characters, whereas a substring is all continguous characters.)\n",
        "    * ...others?\n",
        "\n",
        "\n",
        "\n",
        "* For $\\tilde{\\phi}\\,:$\n",
        "    * Linear\n",
        "    * Polynomial\n",
        "    * Radial basis function\n",
        "    * Weighted cosine similarity (word kernel in the pdf)\n",
        "    * ...others?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we implement several kernels with which you may experiment. **You should also try at least one combination that we have not implemented for you.**"
      ],
      "metadata": {
        "id": "UzqBVamlph12"
      },
      "id": "UzqBVamlph12"
    },
    {
      "cell_type": "markdown",
      "id": "e88a1e1a",
      "metadata": {
        "id": "e88a1e1a"
      },
      "source": [
        "**Check your understanding**:\n",
        "* what is the dimension of a bag-of-words representation of a sentence? what about bigram?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b40782c3",
      "metadata": {
        "id": "b40782c3"
      },
      "outputs": [],
      "source": [
        "def BoW_inner(s1,s2):\n",
        "    \"returns inner product between bag-of-word feature vectors of the two input strings\"\n",
        "    from collections import Counter\n",
        "    d1 = Counter(s1.split())\n",
        "    return sum(d1[w] for w in s2.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c077d57",
      "metadata": {
        "id": "0c077d57"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 2]</h4>\n",
        "\n",
        "No coding in this one: Consider the BoW kernel constructed in `BoW_inner`. Suppose there are $D$ words in the corpus, and each sentence (document) has at most $k$ words.\n",
        "* What is the time and space complexity of naively constructing the bag-of-words vector for each sentence and computing their inner product?\n",
        "* What is the time and space complexity of the implementation in the code?\n",
        "* What accounts for the difference? <br>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer:\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d4c5b6",
      "metadata": {
        "id": "87d4c5b6"
      },
      "source": [
        "### [Task 4] Implementing your own feature mapping function for text data\n",
        "\n",
        "Think about your own understanding of sentences. What features do you use to understand them? **Hand design a feature mapping from sentence to numeric values that might help a kernel learn to classify sentiment.** To do this, you may wish to load the training data and inspect what positive and negative samples look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a5cbf7",
      "metadata": {
        "id": "14a5cbf7"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = utils.load_data(os.path.join(os.getcwd(),\"data/cleaned_tweets_train.tsv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8126bf",
      "metadata": {
        "id": "0d8126bf"
      },
      "outputs": [],
      "source": [
        "print(\"---- positive samples ----\")\n",
        "for i in range(100):\n",
        "    if y_train[i] == 1:\n",
        "        print(X_train[i])\n",
        "print(\"--------------------------\")\n",
        "print(\"---- negative samples ----\")\n",
        "for i in range(100):\n",
        "    if y_train[i] == -1:\n",
        "        print(X_train[i])\n",
        "print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d64e391",
      "metadata": {
        "id": "1d64e391"
      },
      "outputs": [],
      "source": [
        "def my_feature_map(x1): ### You should rename this to be more descriptive about the feature mapping that you are using\n",
        "    #### TASK 4 CODE\n",
        "    #### TASK 4 CODE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "382c06e9",
      "metadata": {
        "id": "382c06e9"
      },
      "outputs": [],
      "source": [
        "def my_inner_product(x1, x2):\n",
        "    '''\n",
        "    this function computes the inner product phi(x1)*phi(x2) for phi,\n",
        "        the feature transform defined in the previous cell, i.e., my_feature_map\n",
        "    implementing this as np.dot(my_feature_map(x1), my_feature_map(x2)) is not\n",
        "        super-useful, as the runtime will be at least linear in the dimension of\n",
        "        the feature map. Instead, implement this without ever using my_feature_map.\n",
        "    '''\n",
        "    #### TASK 4 CODE\n",
        "    #### TASK 4 CODE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65b65ac",
      "metadata": {
        "id": "b65b65ac"
      },
      "source": [
        "Next, we compute the Gram matrix from any kernel we have implemented (e.g., the one you just implemented or the bag-of-words example given in function `BoW_inner`. This is useful in computing $\\tilde{\\phi}$ for $\\tilde{\\phi}$ that can be vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f599ca4",
      "metadata": {
        "id": "3f599ca4"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(K):\n",
        "    def gram_matrix_K(xs_1, xs_2):\n",
        "        return np.array([[K(x1, x2) for x2 in xs_2] for x1 in xs_1])\n",
        "    return gram_matrix_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772b295f",
      "metadata": {
        "id": "772b295f"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel_gram(inner, beta=1):\n",
        "    \"\"\"Gaussian RBF kernel.\n",
        "\n",
        "    Returns a functoin gram(xs_1,xs_2) that calculate the (cross) gram matrix G[i,j]=K(xs_1[i,xs_2[j]])\n",
        "    where K is the Gaussian RBF on the features phi, specified through the inner product in phi space.\"\"\"\n",
        "    def rbf_kernel_sigma_inner(xs_1,xs_2):\n",
        "        return np.exp(-beta*(np.array([inner(x1, x1) for x1 in xs_1])[:, np.newaxis]\n",
        "                             + np.array([inner(x2, x2) for x2 in xs_2])\n",
        "                             - 2*gram_matrix(inner)(xs_1, xs_2)))\n",
        "    return rbf_kernel_sigma_inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4db925",
      "metadata": {
        "id": "ca4db925"
      },
      "outputs": [],
      "source": [
        "def poly_kernel_gram(inner, deg, alpha=1.0):\n",
        "    def poly_kernel_deg_alpha(xs_1, xs_2):\n",
        "        return (alpha + gram_matrix(inner)(xs_1, xs_2))**deg\n",
        "    return poly_kernel_deg_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b862ec",
      "metadata": {
        "id": "b7b862ec"
      },
      "source": [
        "*A quick note on closure: the above kernel construction functions employ a Python concept known as `closure` which provides us the following functionality. Every kernel function should have the same signature: given two data points as input, output a real number. However, the function may be dependent on some parameter that we cannot hard code. Thus, the outer function constructs a kernel function with the desired signature while fixing a value for the parameter. This becomes very useful when we later need to compute kernel values for all pairs of input for a fixed parameter value.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cac6cd2",
      "metadata": {
        "id": "5cac6cd2"
      },
      "source": [
        "Let us see how to generate the RBF kernel matrix using `BoW_inner` and `my_inner_product` as $\\langle v(x_i), v(x_j)\\rangle$ for some value of the parameter $\\beta\\,.$ Note that `rbf_kernel_gram(inner, beta)` returns a function, and we pass it the datasets as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac763e16",
      "metadata": {
        "id": "ac763e16"
      },
      "outputs": [],
      "source": [
        "rbf_kernel_gram(BoW_inner,0.2)(X_train[:10],X_train[5:12])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rbf_kernel_gram(my_inner_product,0.2)(X_train[:10],X_train[5:12])"
      ],
      "metadata": {
        "id": "bNKwtXZRrLOI"
      },
      "id": "bNKwtXZRrLOI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d0411bae",
      "metadata": {
        "id": "d0411bae"
      },
      "source": [
        "### Train SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ddad657",
      "metadata": {
        "id": "7ddad657"
      },
      "source": [
        "Now that we know how to extract and kernelize our data, let us train SVM on it. Note that $C = 1/\\lambda\\,,$ where $\\lambda$ is the regularization parameter we discussed in class. Use a validation set to evaluate the performance of the classifiers you train. As an example, here is how you might use `sklearn`'s SVM implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6160b3a5",
      "metadata": {
        "id": "6160b3a5"
      },
      "outputs": [],
      "source": [
        "svm_clf = svm.SVC(kernel=poly_kernel_gram(BoW_inner, 2))\n",
        "svm_clf= svm_clf.fit(X_train[:100], y_train[:100])\n",
        "preds = svm_clf.predict(X_train[200:220])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ecc25d1",
      "metadata": {
        "id": "3ecc25d1"
      },
      "source": [
        "### [Task 5] Optimizing your SVM classifier\n",
        "We give you a set of questions below to explore and some direction regarding how to explore them. As your \"answer\" for this section, submit a write-up in the notebook with 2-3 plots about the answers to these questions. Also submit your code as applicable.\n",
        "\n",
        "* For differing kernels, train SVM with different $\\lambda$ spanning a good range. Use cross validation to determine a good value of $\\lambda$. What are the resulted (1) 0-1, Hinge training loss? (2) Margin loss? (3) Test error? (4) Support Vectors?\n",
        "\n",
        "* Identify examples where the classifier fails for different kernels. Speculate on what the various kernels might be more suited to.\n",
        "\n",
        "* You implemented your own kernel: how did that do? did the performance match what you were expecting? if not, what factors might have influenced that?\n",
        "\n",
        "* Consider the various attributes of a machine learning algorithm we may be interested in practically: generalization, runtime, memory usage, ease of implementation, understandability. How does kernel SVM for the kernels you tried perform on each of these attributes?\n",
        "\n",
        "\n",
        "After evaluating the different classifiers you developed based on various design choices, evaluate the performance of the one you've chosen to \"ship\". In particular, BEFORE LOADING THE TEST DATA, write your final prediction function, which accepts as input a list of strings to predict on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f842a7",
      "metadata": {
        "id": "63f842a7"
      },
      "outputs": [],
      "source": [
        "def my_final_predictor(x_data_to_predict_on):\n",
        "    #### TASK 5 CODE\n",
        "    my_predictions =      # an array of +1/-1 the same length as x_data_to_predict on\n",
        "    #### TASK 5 CODE\n",
        "    return my_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b927d0",
      "metadata": {
        "id": "a3b927d0"
      },
      "source": [
        "<span style=\"color: red\">\n",
        "<h4 style=\"font-weight: bold\">[Answer Question 3]</h4>\n",
        "\n",
        "BEFORE LOADING THE TEST DATA, what is your estimate, based on whatever validation you would like to do using your available training data, for the generalization (ie population) error of my_final_predictor ? <br>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "Answer: my_estimated_error_rate = ?????\n",
        "</span>\n",
        "\n",
        "<h4 style=\"font-weight: bold\">---------------------</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are now ready to ship your predictor!!  Hurray!!  \n",
        "Let's use it, and see how well it does."
      ],
      "metadata": {
        "id": "lQWJQobhsO7r"
      },
      "id": "lQWJQobhsO7r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a33754",
      "metadata": {
        "id": "51a33754"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = utils.load_data(os.path.join(os.getcwd(), \"data/cleaned_tweets_test.tsv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e761a988",
      "metadata": {
        "id": "e761a988"
      },
      "outputs": [],
      "source": [
        "test_predictions = my_final_predictor(X_test)\n",
        "print(\"My test error:\", np.mean(y_test != test_predictions))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py36",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}